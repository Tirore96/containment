
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}
%This is a template for producing LIPIcs articles. 
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Computationally interpreting regular expression containment proofs to effecient procedures } %TODO Please add

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Fritz Henglein}{University of Copenhagen, Denmark}{johnqpublic@dummyuni.org}{https://orcid.org/0000-0002-1825-0097}{(Optional) author-specific funding acknowledgements}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional. Use additional curly braces to indicate the correct name splitting when the last name consists of multiple name parts.

\author{Dawit Tirore\footnote{corresponding author}}{IT University of Copenhagen, Dummy College, [optional: Address], Country}{dati@itu.dk}{[orcid]}{[funding]}

\author{Nobuko Yohsida}{University of Oxford}{mail}{[orcid]}{}

\authorrunning{J. Open Access and J.\,R. Public} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Jane Open Access and Joan R. Public} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  +http://creativecommons.org/licenses/by/3.0/

\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Dummy keyword} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{proof}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{stmaryrd}

%\newcommand\mycomment[1]{\textcolor{red}{#1}}
\newcommand\mycomment[1]{}

\begin{document}
\include{macros}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
% Regular expressions are ubiqutious. They are theoretically relevant to automata theory for being syntactic representations of finite state automata and essential in practice for text processing. Theory and practice share an extensional view of regular expressions, caring more about the binary property of whether a string matches an expression, and less about the more informative property of how it matches the expression as a parse tree. Regular expressions can be viewed as types that have parse trees as their terms and Henglein and Nielsen demonstrate how these parse trees can represent compressed text. In most typed programming languages one expects type checking to work up to subtyping. Incorporating \textit{regular expressions as types} into such a language requires coercive subtyping to preserve that the only terms of regular expression types are their parse trees. Henglein and Nielsen show how such coercions can be derived by a computational interpretation of intuitionistic proofs of regular expression containments. They prove their system sound and complete with respect to language containment. We extend on their work by proving that effecient coercions can be synthesized effeciently and mechanise this result, along with soundness and completeness, in the Coq proof assistant. As part of the solution to synthesis, we also introduce a technique for mechanising decidability of regular expression containment and equivalence in under 900 loc. Finally we extract the synthesis procedure and benchmark the performance of the coercions.
% \\\\
 Regular expressions are typically studied in automata and formal language theory as a formalism for denoting the class of regular languages, where the only observable property is whether a string belongs to the denoted set of strings or not, and any two regular expressions are observably equivalent if they satisfy the same membership tests.  In programming practice, however, regular expressions are typically used to extract more detailed information: not only whether but also how a string matches a regular expression, ranging from extracting a particular substring to constructing a full parse tree. Many automata theory techniques are not applicable in this setting; for example, a deterministic finite automaton constructed from a regular expression provides no way of constructing a parse tree for the given regular expression.  

It is known that proofs of membership of strings in (the language denoted by) a regular expression are in one-to-one correspondence to parse trees; and that a formal proof of a regular expression containment $A \leq B$ in an axiomatization of regular expression containment can be operationally interpreted as a coercion, a function that maps any proof of containment of a string in $A$ into a proof of containment of the same string in $B$.  Different containment proofs may yield extensionally different functions, however, since regular expressions, considered as grammars, can be ambiguous. Additionally, even if extensionally the same, one proof of containment may yield a slow implementation, another an efficient one.

In this paper we prove that efficient coercions can be synthesized efficiently and mechanize this result in Coq, along with soundness (only terminating coercions are inferred) and completeness (for each valid containment at least one coercion is derivable). We extract our efficient coercion synthesis procedure and benchmark the performance of the synthesized coercions. We also present a technique used in the synthesis procedure, that can prove regular expression equivalence in under 900 lines of code. This is to the best of our knowledge, the shortest mechanised proof in the litterature for such a decision procedure.
\end{abstract}

% \section{Introduction}
% Regular expressions is inductive syntax that represents finite state automaton. Regular expressions equivalence and inequalities are well understood and studied. Often these proof systems are considered in a proof-irrelevant way, not caring what the inference tree is, only that it exists. Brandt and Henglein [cite] show how regular expression containment proofs can be interpreted to functional programs on parse trees. Such interpretation has applications in string compression, which we shall explain more later. The rules of their containment axiomatization corresponds to an intrinscially typed domain specific language. They derive the constructs of this language by combining the rules of idempotent semirings, unfolding $A`* \leq 1 +  A \times A^*$, with a powerful fix rule that is a of coinductive nature. 

% Our goal is effecient coercions on parse trees with applications for string compression. 
% Existing work by Brandt and Henglein formalize this using a powerful fix rule, whose soundness they ensure by instantiating it with a proper side condition. This side condition makes the coercion proofs less compositional and thus harder (as well as more expensive) to synthesize. This side condition checks for contractiveness. We derive a similar coercion system with a much weaker (but compositional) fixpoint rule that both simplifies (and makes more effecient) the synthesis of coercions, and implies contractiveness. The system is inspired by parameterized coinduction and allows us simple intepretation (our definition is good for simple termination arguments) and synthesis. These simple approaches would not have worked in Brandt and Henglein's origina system [detail why].\\\\
% We derive this inductive coercion system in the following steps: 
% \begin{enumerate}
% \item The coercion system is an axiomatization of language containment, which can be obtained by adding a few rules to axiomatizations of language equivalence. Starting with Grabmeyer's coinductive characterization, we prove it equivalent to another coinductive characterization that is free of operational notions. 
% \item We then prove this equivalent to an inductive version of this (why is it important to go inductive? Depends on the performance between the two dsls). 
% \item The inductive system for language equivalence is the altered to create one for language containment, and we go from Prop to Type.
% \end{enumerate}
\section{Preliminaries}
\mycomment{double check it is 900 lines it takes to prove this}
Cardelli and Fresch introduce regular expressions as types.
\begin{definition}[Regular expression and semantics]
Regular expressions over a finite base set $a$ are given by the syntax:\\
$A ::= A + B~| A \times B ~|~A ^* ~|~ 1 ~|~ 0 ~|~ a$\\
Matching is defined as:\\
\begin{displaymath} 
\infer{\match{\epsilon}{1}}{}\qquad
\infer{\match{s s'}{A \times B}}{\match{s}{A} & \match{s'}{B}} \qquad
\infer{\match{s} {A + B}}{\match{s}{A}} \qquad
\infer{\match{s} {A + B}}{\match{s}{B}} \qquad
\infer{\match{\epsilon}{A^*}}{}\qquad
\infer{\match{s s'}{A^*}}{\match{s}{A} & \match{s'}{A^*}}\qquad
\end{displaymath}
Intrinsically typed parse trees:
\begin{displaymath}
\begin{array}{l}
\infer{\oft{()}{1}}{} \qquad 
\infer{\oft{a}{a}}{} \qquad \infer{\oft{\pair{t}{t'}}{A \times B}}{\oft{t}{A} & \oft{t'}{B}}
\qquad \infer{\oft{\inl {t}}{A + B}}{\oft{t}{A}} \qquad
\infer{\oft{\inr {t}}{A + B}}{\oft{t}{B}}  \qquad
\infer{\oft{\fold{t}{A^*}}}{\oft{t}{1 + A \times A^*}}
\end{array}
\end{displaymath}
\end{definition}
\begin{definition}[Equivalence] \noindent \\
Language equivalence: $\forall s, \match s\in A \iff s \in B$\\
Language containment: $\forall s, s \in A \implies s \in B$
\end{definition}
\begin{definition}[Derivative (standard/partial) and nullariness]
\begin{displaymath}
\begin{array}{l}
\mynu{a} = \false \qquad
\mynu{\epsilon} = \true \qquad
\mynu{0} = \false
\\\\
\mynu{A + B} = \mynu A  \lor \mynu B \qquad
\mynu{A \times B} = \mynu A  \land \mynu B
\end{array}
\end{displaymath}
\begin{displaymath}
\begin{array}{l}
\derive {a}{1} = \qquad
\derive{a}{0} = 0 \qquad
\derive{a}{b} = \epsilon if a = b\qquad
\derive{a}{b} = 0 if a \neq b\qquad
\derive {a}{(A + B)} = \derive{a}{A} +  \derive{a}{B}\qquad
\\\\
\derive {a}{(A \times B)} = \derive{a}{A} \times B +  \derive{a}{B} ~if \mynu{A} \qquad
\derive {a}{(A \times B)} = \derive{a}{A} \times B ~if not \nu{A} \qquad
\derive {a}{(A^*)} = \derive{a}{A} \times A^*
\end{array}
\end{displaymath}
\end{definition}j
\begin{definition}[Parameterized coinduction]
  fill out....
\end{definition}
% \section{Axiomatizations:  Prior ones and a new one}
% The first axiomatization of equivalence was given by Salomaa [cite]. We shall do as Brandt and Henglein, refering jointly to the laws of semiring, equality and the unfold rule $A^* = 1 + A \times A^*$ as \textit{weak equivalence}. Salomaa provec soundness and completeness of the system $F_1$ which consists of the rules of weak equivalence with the rules in Figure (\ref{fig:salomaa}).
% \begin{definition}[Laws of idempotent semi-ring]
% \label{definition:ring}
% \begin{displaymath}
% \begin{array}{lll}
% \myaxiom{A + B + C}{A + (B + C)} \qquad  
% \myaxiom{A + B}{B + A} \qquad 
% \myaxiom{A + 0}{A} \qquad
% \myaxiom{A + A}{A} \qquad
% \\\\
% \myaxiom{A \times B \times C}{A \times (B \times C)} \qquad
% \myaxiom{1 \times A}{A} \qquad
% \myaxiom{A \times 1}{A} \qquad 
% \myaxiom{0 \times A}{0} \qquad
% \\\\
% \myaxiom{A \times 0}{0} \qquad
% \myaxiom{A \times (B + C)} {A \times B + A \times C} \qquad
% \myaxiom{(A + B)\times C}{A \times C + B \times C}
% \end{array}
% \end{displaymath}
% \end{definition}

% \begin{definition}[Laws of equality]
% \label{definition:equality}
% \begin{displaymath}
% \begin{array}{lll}
% \infer{A = A}{}\qquad
% \infer{A = B}{B = A}\qquad
% \infer{A = C}{A = B & B = C}
% \end{array}
% \end{displaymath}
% \end{definition}
% \begin{figure}
% \caption{Rules of Salomaa}
% \label{fig:salomaa}
% \begin{displaymath}
% \infer[A_{11}]{\eqBodyE{A^*}{B^*}}
%   {\eqBodyE{A}{B}} \qquad
% \myaxiomN{(1 + A)^*}{A^*} \qquad \infer[\mynu{F}=\false]{\eqBodyE{E}{F^* \times G}}{\eqBodyE{E}{F \times E + G}}
% \end{displaymath}
% \end{figure}
% Salomaa proved that from the rules of weak equivalence with the unfold-rule and $A_{11}$, one can derive the decmosition:
% \begin{lemma} \label{lem:salomaa}
% \[A = \mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}\]
% \end{lemma}
% We will return to this decomposition again later.




% \subsection{Why axiomatizations matter}
% \mycomment{Mention regular expressions as types}
% Regular expressions denote an important class of automata deserving of its study on equivalences and containments. From an automata theoretic view, axiomatizations improve our understanding on the properties of finite state automaton. The shape of the proof within an axiomatization is of no importance, one only cares about soundness and completeness. Brand and Henglein showed why it is worth to consider not only \textit{what} an axiomatization lets you prove, but also \textit{how} the proof rule lets build your derivation. To see the benefit of taking a proof-relevant approach to regular expression axiomatizations we start by presenting the problem we investigate in this paper.
% [Example of simple coercion]
% [Explain why parse trees is equal to proof-relevance]
% [Hint at comleteteness here]
% [Containment is more essential than equivalence because it is more than bijection] 
% \begin{center}
% \textit{Given regular expressions $A$ and $B$ and language inclusion $A \subseteq  B$, how does one define a procedure that outputs an effecient function mapping parse trees of $A$ to parse trees of $B$}.\\
% \end{center}
% Such a function, $f : A \rightarrow B$, must exist, otherwise the language inclusion would not hold. This is especially the case in the dependent type theory of Coq which follows Curry-Howard proof as programs philosophy. Let us consider a definition of language containment in Coq:
% \begin{minted}{Coq}
% Definition Contains A B := forall s, Match s A -> Match s B
% \end{minted}
% \textsf{Contains} is a map on \textsf{Match} derivations, which are essentially parse trees. 
% They are defined as 
% \begin{minted}{Coq}
% Inductive Match : trace -> regex -> Prop :=
%   | MEps : Match [::]  Eps
%   | MEvent x : Match [::x] (Event x)
%   | MSeq s1 c1 s2 c2 : Match s1 c1 ->  Match s2 c2 -> Match (s1 ++ s2) (c1 _;_ c2)
%   | MPlusL s1 c1 c2:  Match s1 c1 -> Match s1 (c1 _+_ c2)
%   | MPlusR c1 s2 c2:  Match s2 c2 ->  Match s2 (c1 _+_ c2)
%   | MStar0 c  : Match [::] (Star c)
%   | MStarSeq c s1 s2:  Match s1 c -> Match s2 (Star c) -> Match (s1 ++ s2) (Star c).
% \end{minted}
% The definition of parse trees is
% \begin{minted}{Coq}
% Inductive pTree : @regex A -> Type := 
% | p_tt : pTree Eps 
% | p_singl a : pTree (Event a)
% | p_inl r0 r1 : pTree r0 -> pTree (r0 _+_ r1) 
% | p_inr r0 r1 : pTree r1 -> pTree (r0 _+_ r1) 
% | p_pair r0 r1 : pTree r0 -> pTree r1 -> pTree (r0 _;_ r1)
% | p_fold r : pTree (Eps _+_ (r _;_ (Star r))) -> pTree (Star r).
% \end{minted}
% One crucial difference between the two is \textsf{Match} derivations live in \prop, making the shape of the derivation inaccesible for case distinction during computatioin in \myset \footnote{To be precise pTree lives in Type because it is parameterized by A which lives in Type. This is due to parameterizing the entire development over the ssreflect finite type which lives in Type }. Though we can not use \textsf{Contains} directly in constructing $f : A \rightarrow B$, knowing that the containment \textsf{Contains A B} holds, can be used as a termination argument for proof search. A mapping $f : A \rightarrow B$ can be obtained by lifting a parse tree \textsf{t} of regular expression $A$, \textsf{t : pTree A}, to a Match derivation \textsf{Match (flatten t) A}, where \textsf{flatten} returns the underlying string of the parse tree. Translating to a match derivation takes linear time in the size of the parse tree. From here we can apply the \textsf{Contains A B} assumption as a function returning \textsf{Match (flatten t) B}, from which me must construct a \textsf{t' : pTree B}, preserving the underlying string, that is,\textsf{flatten t' = flatten t}. Building \textsf{t' : pTree B} is where need proof search. With \textsf{Match (flatten t) B} living in Prop, we may not case distinct on the derivation to learn the shape \textsf{t'} should take. But the presence of \textsf{Match (flatten t) B} means that there exists a natural number $n$, such that the set of parse trees of at most $n$ constructor applications, will contain our desired parse tree. We show this in file \textsf{Constructive.v}, following the technique of the Constructive Epsilon Coq libarary [cite].\\
% Now with a procedure in hand to transform a language containment \textsf{Contains A B} into a map on parse trees \textsf{f : pTree A -> pTree B} all we need is a decision procedure for language containment, which there exists several of in the litteratue[cite]. We now have a way to synthesize mapping on parse trees (which we from now on will call coercions)
.
\begin{definition}[Coercion]
A function $f :\type{E} \rightarrow \type{F}$ is a coercion iff $\forall \mathsf{t}. \flatten{f~\mathsf{t}} = \flatten{\mathsf{t}}$.\\
\end{definition}
 % This is of course too slow to be of any real use and only serves to show why we might wish to limit the function space of coercions to include only linear time programs that process the underlying string of the parse tree in a streaming fashion rather than brute forcing the problem. How should the function space be constrained? The coercion is not allowed to alter the underlying string (soundness). What ever constructs we do decide on must let us build any mapping that can be proved with \textsf{Contains} (completeness). As we shall see in a moment, Brandt and Henglein found that axiomatizing regular expression containment, is the same as defining the primitives and combinators of a set of rules for building coercions.
%Characterising (in)-equalties of regular expressions by a set of compositional rules is what we mean by an axiomatization. Many such axiomatizations exist. Brandt and Henglein presents an axiomatization of containment: $\Gamma \vdash A \leq B$, which is related to equivalence since $A = B \iff A \subseteq B \land B \subseteq A$. They take a proof-relevant view of containment proofs, defining the judgment $\Gamma \vdash c : A \leq B$, where $c$ is an inductive term that records the shape of the contaiment proof. This proof-relevant view allows an interpretation to be given to $c$ as $[c]$ that yields a functional program mapping parse trees of $A$ to parse trees of $B$. Proving the existance of such interpretation function $[\cdot]$ corresponds to a soundness proof the containment axiomatization. On the other hand, completeness means the language containment $A \subseteq B$ implies the existence of a derivation $\vdash c : A \leq  B$, that is, we must be able to synthesize $c$ from a language containment.
\section{A permissive coercion language}
Henglein and Nielsen present a derivation system for building a coercion $\dslcom{c}$ with the judgment $\containsG{c}{A}{B}$. The judgment is parameterized by a side condition $P$. 
This is used in the rule for $\fix~\dslcom{f. c}$ to ensure termination by checking that $P(\contains{\Gamma}{\fix~ \mathsf{f.c}}{A}{B})$ holds. This is necessary because unrestricted fixpoint definitions allows non-terminating coercions like $\fix~ \mathsf{f}.\dslcom{f}$ and it is unsound for these coercions to be derivable because they do not interpret to total functions. 

The rules for $\containsG{c}{A}{B}$ can be seen in Figure (\ref{fig:HN}) where we write $\Gamma \vdash_P \mathsf{c}: {A} = {B}$ for simultanously defining the two rules  $\containsG{c}{A}{B}$ and  $\containsG{c^{-1}}{A}{B}$. We omit the subscript $\vdash_P$ for axiom rules. 

The most permissive side condition that ensures termination is hereditary totality:
\begin{definition}[Hereditary totality]
$\containsG{c}{A}{B}$ is hereditarily total if $\mathsf{f}: A \leq B$ is total, $\forall \mathsf{f} : A \leq B \in \Gamma$.
\end{definition}
This property is the most permissive side condition and it is undecidable. For the coercion system instantiated with hereditary totality they prove soundness.
\[ \contains{}{c}{A}{B} \implies A \subseteq B\]
This holds because all constructs are string preserving transformations and all derivable coercions are terminating.
Parameterizing the coercion language by a side condition makes it possible to compare different axiomatizations. For each of three distinct axiomatizations of regular expression equivalence and containment they give side conditions that are suffecient to translate proofs in the axiomatization into coercion derivations.
\begin{definition}[Syntactic side conditions $S_i$]
 Define predicates $S_1$,$S_2$,$S_3$,$S_4$ on coercion judgments of the form $\containsG{\fix f. c}{E}{F}$ as follows:
 \begin{itemize}
 \item $S_1(\containsG{\fix f. c}{E}{F})$ if and only if each occurence of $f$ in $c$ is left-guarded by a $d$ where, $\contains{\Gamma ,...}{d}{E'}{F'}$ is the coercion judgment for $d$ occuring in the derivation of $\containsG{\fix f. c}{E}{F}$ and $\myo{(E')}=0$\\
 \item $S_2(\containsG{\fix f. c}{E}{F})$ if and only if each occurence of $f$ in $c$ is eft-guarded and for each subterm of the form $c_1;c_2$ in $c$ at least one of the following conditions is satisfied:
   \begin{itemize}
   \item $c_1$ is closed and $\dslcominv{proj}$ free
     \item $c_2$ is closed
   \end{itemize}
\item $S_3(\containsG{\fix f. c}{E}{F}) \textit{if c is of the form } \dslcominv{wrap};(\dslcom{id}+\dslcom{e} \times \dslcom{f});d \textit{ where d and e are closed}$ \footnote{In Henglein and Nielsen, the $S_3$ sidecondition has a minor mistake because it instead expects the shape $\dslcominv{wrap};(\dslcom{id}+\dslcom{id} \times \dslcom{f});d \textit{ where d is closed}$. This is insuffecient to derive the example above, used in their proof of completeness. It is however minor as replacing \dslcom{id} with \dslcom{e} does not affect the underlying termination measure}
\item $S_4 = S_1 \lor S_3$
   \end{itemize}
%\item  $S_3(\containsG{\fix f. c}{E}{F})$ if $c$ is of the form $\dslcominv{wrap};(\dslcom{id} + \dslcom{d} \times f) ; \dslcom{e}$ where $d$ and $e$ are closed \footnote{This is slightly different from Henglein and Nielsen because their version is not suffecient for their completeness proof via encoding to salomaa}
%\item 
\end{definition}
%Henglein and Nielsen show that multiple distinct axiomatizations of regular expression containment, such as the one proposed by GBrabmeyer for example, all can be captured coinductively motivated rule, which in the context of proof-relevance translates nicely to a fixpoint construction. Unrestricted fixpoint definitions allows non-termination, which is unsound, and to avoid this they apply a side condition to restrict it to safe use. The rules for declaring and calling fixpoints are seen below:
\begin{example}[Salomaa]\label{ex:map}
The Salomaa axiomatization of regular expression equivalence includes a context rule for kleene-star. We translate this into a coercion derivation. Expressed as a containment the context rule is
%\[ \infer{A^* = B^*}{A=B} \]
\[ \infer{A^* \leq B^*}{A\leq B} \]
% Henglein and Nielsen show that different predicates can be used to ensure soundness and that the particular way they restrict the use of recursive calls corresponds to distinct termination measures.
%The two rules in Figure (\ref{fig:salomaa}) are axioms in Salomaas axiomatization of equivalence and they are not present in Henglein and Nielsen's system.
%They can however be derived using the fixpoint rule. Reusing their example we show to derive the first rule.
We assume coercion derivation $\contains{}{c}{A}{B}$, and construct a coercion of $ A^* \leq B^*$.\\
Assume $\dslcom{f}: A^* \leq B^*$ as a hypothesis and derive
\begin{align}
  A^* &\leq (1 + A \times A^*) && \textsf{ by} ~\dslcominv{wrap}\\
&\leq  (1 + B \times B^*) &&\textsf{ by} ~\dslcom{id}+ \dslcom{c} \times \dslcom{f}\\
&\leq B^*
\end{align}
This derives:
$\contains{f : A^* \leq B^*}{\dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}$\\
By the fix-rule with %$S_3$ we get
we get $\contains{}{\fix~f. \dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}$ which is just the map function on a list 
\begin{minted}{Coq}
Definition map :=  fix f (a : Star) := let v := match fold a with 
                       | inl () => inl ()
                       | inr (a',b) => inr (c a',f b)
                      in fold v
\end{minted}
Which is terminating because of structural recursion. To use this termination argument for the coercion derivation we instantiate $P$ with $S_3$. We have now derived
\[\infer{\containsP{}{\fix~f. \dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}{S_3}}{\containsP{}{c}{A}{B}{S_3}}\]
\end{example}



% and is reminicset of Coq's fixpoint \textsf{fix} which must be defined by structural recursion.
%A syntactic side condition reflects the derivation shape of an axiomatization. 
%Multiple side conditions are defined to prove completeness  they each are suffecient for proving completeness of 
\begin{figure}\label{fig:HN}
\caption{Coercion system of Henglein and Nielsen}
\begin{displaymath}
\begin{array}{lll}
\myaxiomC{shuffle}{A + B + C}{A + (B + C)} \qquad  
\myaxiomC{retag}{A + B}{B + A} \qquad 
\myaxiomC{untagL}{0 + A}{A} 
\\\\
\myaxiomC{untag}{A + A}{A}   \qquad \myaxiomC{tagL}{A}{A + B} \qquad

\myaxiomC{assoc}{A \times B \times C}{A \times (B \times C)}
\\\\
\myaxiomC{swap}{A \times 1}{A} \qquad
\myaxiomC{proj}{1 \times A}{A} \qquad
\myaxiomC{abortR}{A \times 0}{0} \qquad
\myaxiomC{abortL}{0 \times A}{0} 
\\\\
\myaxiomC{distL}{A \times (B + C)} {A \times B + A \times C} \qquad
\myaxiomC{distR}{(A + B)\times C}{A \times C + B \times C} 
\\\\
\myaxiomC{wrap}{1 + A \times A^*}{A^* } \qquad \myaxiomC{id}{A}{A}
\\\\
\infer{\containsG{c;d}{A}{C}}{\containsG{c}{A}{B} & \containsG{d}{B}{C}} \qquad

\infer{\containsG{c + d}{A + B}{ C + D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}}  \qquad

\infer{\containsG{c \times d}{A \times B}{ C \times D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}} 
\\\\
\infer[P(\contains{\Gamma}{\fix~ \mathsf{f.c}}{A}{B})]{ \containsG{\fix~\mathsf{f.c}}{A}{B}}{\contains{\Gamma,f : A \leq B}{c}{A}{B}} \qquad
 \infer{\contains{\Gamma ,f : A \leq B, \Gamma'}{ f}{A}{B}}{}
\end{array}
\end{displaymath}
\end{figure}
%The syntactic side conditions all prevent deriving ill-fixpoints like $\contains{}{\fix ~ f.f}{A}{B}$. Multiple side conditions are defined to capture the derivation shape in different axtiomatizations. As an example, the first axiomatization of regular expression equivalence was Salomaa's $F_1$ where he included the rule
% \[\infer{A^* = B^*}{A = B}\]
% To derive such a rule, the $S_3$ conditions is suffecient.
% % Henglein and Nielsen show that different predicates can be used to ensure soundness and that the particular way they restrict the use of recursive calls corresponds to distinct termination measures.
% %The two rules in Figure (\ref{fig:salomaa}) are axioms in Salomaas axiomatization of equivalence and they are not present in Henglein and Nielsen's system.
% %They can however be derived using the fixpoint rule. Reusing their example we show to derive the first rule.
% \begin{example}[A coercion derivation with $S_3$]\label{ex:map}
% From $\contains{}{c}{A}{B}$, we can construct a coercion $d$, s.t $\contains{}{d}{A^*}{B^*}$.\\
% Assume $\dslcom{f}: A^* \leq B^*$ and derive
% \begin{align}
%   A^* &\leq (1 + A \times A^*) && \textsf{ by} ~\dslcominv{wrap}\\
% &\leq  (1 + B \times B^*) &&\textsf{ by} ~\dslcom{id}+ \dslcom{c} \times \dslcom{f}\\
% &\leq B^*
% \end{align}
% This derives:
% $\contains{f : A^* \leq B^*}{\dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}$\\
% By the fix-rule with $S_3$ we get
% $\contains{}{\fix~f. \dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}$\\
% \end{example}
% %The rules of Henglein and Nielsen can be thought of as an intrinsically typed domain specific language for defining coercions on parse trees.
% %The example above showed how to construct the coercion $\fix~f. \dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}$ of type $A^* \leq B^*$. 
% The interpretation of the coercion in Example (\ref{ex:map}) should yield us the following gallina code
% \begin{minted}{Coq}
% Definition map := 
%  fix f (a : Star) := let v := match fold a with 
%                        | inl () => inl ()
%                        | inr (a',b) => inr (c a',f b)
%                       in fold v
% \end{minted}
% This is just the map function on lists. The $S_3$ condition checks that $\dslcom{f}$ is only applied on the tail of the list. 
% %For the completeess proof by Henglein and Nielsen, $S_3$ is too strict by itself, and they present two distinct side conditions, that each yield a sound and complete characterization of containment. The other rule, $(1 + A)^* = A^*$ is in the right dirction interpreted as the filter function that removes \textsf{None}. Both rules apply a linear time operation.

%The minimal side condition necessary to derive all Salomaa proofs in the coercion system informs us what underlying termination measure that ensures soundness for rules that transform kleene-star. In Salomaa's $F_1$ those rules given as containments are
% \[ \infer{\eqBodyE{A^*}{B^*}}
%    {\eqBodyE{A}{B}} \qquad
%  \myaxiomN{(1 + A)^*}{A^*} \qquad \infer[\mynu{F}=\false]{\eqBodyE{E}{F^* \times G}}{\eqBodyE{E}{F \times E + G}} \]
% The first two rules are derivable with $S_3$ and the last by $S_1$, thus all Salomaa proofs can be coded as coercions using $S_4 = S_1 \lor S_3$.
% % in order to understand the computational interpretation of the proofs. Henglein and Nielsen compare Salooma, Kozen and Grabmeyer, showing that using one side condition, $S_4$, they can code the proofs of Salooma and Grabmeyer, and with another side condition $S_2$ they can code Kozen.
%  As all the axiomatizations are sound and complete, we only care which proof rules that admit proofs that interpret to effecient programs. The proof rules translates to the primitives of our coercion language, and the wrong choice leads to slow coercions

\begin{example}[Kozen]
Along with the rules of weak equivalence, and the unfold rule: $1 + (E^* \times E) \leq E$ Kozen axiomatizes containment with the rules
\begin{displaymath}
\infer{A^* \times B \leq B}{A \times B \leq B} \qquad \infer{A \times B^* \leq A}{A \times B \leq A}
\end{displaymath}
Assuming $\dslcom{d} : A \times B \leq B $ and $\dslcom{e} : A \times B \leq A $ these correspond to the coericons
\begin{align}
&\fix~f.(\dslcominv{wrap}\times \dslcom{id}); \dslcom{distR};(\dslcom{proj} + (\dslcominv{assoc};(\dslcom{id} \times \dslcom{f}));\dslcom{d});\dslcom{untag}\label{align:kozen1}\\
&\fix~f.( \dslcom{id} \times \dslcominv{wrap}); \dslcom{distL};((\dslcom{swap};\dslcom{proj}) + (\dslcom{assoc};(\dslcom{e} \times \dslcom{id}));\dslcom{f});\dslcom{untag}\label{align:kozen2}
\end{align}
These are the \textsf{foldright} (\ref{align:kozen1}) and \textsf{foldleft} (\ref{align:kozen2}) functions in functional programmming. Like the last example, termination is ensured by structural recursion. The syntactic shape of the coercions do not satisfy $S_3$. $S_3$ is a special case of $S_2$ which the coercions do satisfy. Kozen's rules corresponds to these coercion derivations
\[ \infer{\containsP{}{\mathsf{foldright}(\dslcom{d})}{A^* \times B}{B}{S_2}}{\containsP{}{d}{A \times B}{B}{S_2}} \qquad \text{and} \qquad  \infer{\containsP{}{\mathsf{foldleft}(\dslcom{e})}{A \times B^*}{A}{S_2}}{\containsP{}{e}{A \times B}{A}{S_2}} \]
% \begin{figure}\label{figure:kozen}
% \caption{Computational interpretation of Kozen rules}
%   \centering
%   \begin{minted}{Coq}
% Definition d : A \times B -> B := ...
% Definition fold_right : (Star A) * B -> B := fix f (a : Star A * B) := 
%                 match a with 
%                   | (fold (inl ()),b) => b 
%                   | (fold (inr (a',a_star)),b) => d (a',f (a_star,b))
%                 end

% Definition e : A \times B -> A := ...
% Definition fold_left A * Star B -> A := fix f (a : A * Star B) := 
%                 match a with 
%                   | (a,fold (inl ())) => a
%                   | (a,fold (inr (b,b_star))) => f (e (a,b),b_star)
%                 end
% \end{minted}
% \end{figure}
The computational ineffeciency of these rules can be observed by deriving the denesting property $a ^* \times (a^*)^* \leq a^*$. 
\[\infer{\containsP{}{\dslcom{foldright}(\dslcom{foldleft}(\dslcom{retag};\dslcom{tagL};\dslcom{wrap}))}{a^* \times (a^*)^*}{a^*}{S_2}}{\infer{\containsP{}{\dslcom{foldright}(\dslcom{retag};\dslcom{tagL};\dslcom{wrap})}{a^* \times a^*}{a^*}{S_2}}{\containsP{}{\dslcom{retag};\dslcom{tagL};\dslcom{wrap}}{a \times a^*}{a^*}{S_2}}} \]
% \begin{align}
% \contains{}{\dslcom{retag};\dslcom{tagL};\dslcom{wrap}}{a \times a^*}{a^*}
% \end{align}
% Now we apply $\mathsf{foldright}(\cdot)$
% \begin{align}
% \contains{}{\dslcom{foldright}(\dslcom{retag};\dslcom{tagL};\dslcom{wrap})}{a^* \times a^*}{a^*}
% \end{align}
% Now apply \textsf{foldleft}
% \begin{align}
% \contains{}{\dslcom{foldright}(\dslcom{foldleft}(\dslcom{retag};\dslcom{tagL};\dslcom{wrap}))}{a^* \times (a^*)^*}{a^*}
% \end{align}
We have created a nested loop, morever the rules presented are the only ways of manipulating kleene star so it appears that the most effecient derivaiton of denesting has a quadratic runtime.
\end{example}
\begin{example}[Grabmeyer]
%Brozowski showed that modulus rules of associativity, commutativity and transitivity, a regular expression has a finite set of unique derivatives. 
%Grabmeyer's axiomatization is influenced by the work of Brandt and Henglein (1997) who show that that a coinductive definition of recursive type equality (modulus unfolding) $\tau = \tau'$, can be characterized inductively using a fixpoint rule similar to the one seen for the coercion system. This is due to the regularity of recursive types $\tau$ containing only finitely many distinct subterms, allowing an inductive characterization. Grabmeyer applies this technique to characterize regular expression equivalence inductively with a similar fix rule. The technique is applicable in this context because Brozowski showed that modulus ACI, a regular expression has a finitely many distinct derivatives. 
A variation of Grabmeyer's axiomatizations of equivalence is presented in Henglein and Nielsen, containining rules of equational logic, context rules and ACI-rules along with COMP/FIX.
% The first one is due to Brandt and Henglein who show that 
% that the method of Brandt and Henglein could be used in this setting as well. He showed that modulus some ACI rewriting\footnote{To minimize the size of the generated relations he allowed more rules than ACI}, regular expression equivalence can be charaterized by a natural deduction system that emulates finitary coinduction via the COMPFix rule. One can see finitary coinduction in action by the accumulation of $(A,B)$ into the context of the second premise.
\[\infer[\mathsf{COMP/FIX}]{\Gamma \vdash A =  B}{\myo{A} =  \myo{B} & \Gamma, A = B \vdash \derive{a}{A} = \derive{a}{B},~\forall a \in \Sigma}\] 
%A remark about COMP/FIX is its operational nature, refering to derivatives $\derive{a}{A}$ and $\myo{\cdot}$. 
Translating this rule into coercion derivation, relies on the following equivalence proved by Salomaa
\[ A = \myo{A} + \decomp{A}\]
Which implies that there for all $A$ exists coercions
\[\contains{}{decomp_A}{A}{\myo{A} + \Sigma_{a \in \Sigma} \derive{a}{A}} \qquad \contains{}{recomp_A}{\myo{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A}\]
%We have seen that Kozen proofs leads to slow coercions and a natural question is then which proof rules gives rise to fast coercions? Henglein and Nielsen argue that Grabmeyer rules lead to fast coercions.
Allowing the inference rule to be translated into the coercion derivation
\[\infer{\containsP{}{\fix ~f.decomp_A;(id + \Sigma_{a \in \Sigma} id\times f_a); recomp_B}{A}{B}{S_1}}{\containsP{\dslcom{f} : A \leq B}{f_a}{\derive{a}{A}}{\derive{a}{B}}{S_1}, \forall a \in \Sigma} \]
Termination is ensured for this fixpoint because the hypothesis $\dslcom{f} : A \leq B$ is used to derive a coercion for a parse tree $ \mathsf{v} : \derive{a}{A}$ of a shorter string than the input parse tree $\mathsf{t} : A$.
We express this termination argument as the instantiation of $P$ with $S_1$. We derive denesting again to observe the computational effeciency of the derivation, setting $\Sigma = \{a\}$.
\[\infer{\contains{}{\fix ~f.decomp_{a^* \times (a^*)^*};(id +  id\times  (\dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f}))); recomp_{a^*}}{a^* \times (a^*)^*}{a^*}}{\contains{\dslcom{f} : a^* \times (a^*)^* \leq a^*}{\dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})}{\derive{a}{(a^* \times (a^*)^*)}}{\derive{a}{a^*}}} \]
% Which corresponds to the gallina code
% \begin{minted}{Coq}
% Fixpoint coerce (a : A) : B := 
% let res1 := match decompose A with 
%             | inl tt => inl tt
%             | inr a_sum => match sum with 
%                            | (e,a_sum) => (e,f_e (a_sum))
%                            | (e',a_sym) => (f,f_e' (a_sum))
%                             ...
%                             end 
%             end 
% in recompose res1
% \end{minted}
%We prove denesting $a^* \times (a^*)^* \leq a^*$ in Grabmeyer's axiomatization and consider the computational interpretation.  terms of managable size we set $\Sigma = \{a\}$ 

% Assume $\dslcom{f}: a^* \times \dstar a \leq a^*$. Then we prove the containment as:
% \begin{align}
% a^*\times \dstar a &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a) &&\quad \mathsf{decomp_{a^* \times (a^*)^*}}\\
% & \leq 1 + a \times (((1 \times a^*) \times \dstar a)) &&\quad \text{idempotence}\\
% &\leq 1 + a \times ((1 \times (a^* \times \dstar a)) &&\quad \text{associativity}\\
% & \leq 1 + a \times ((1 \times a^*) &&\quad \dslcom{f}\\
% & \leq a^* && \quad \mathsf{recomp_{a^*}}
% \end{align}
% Which derives 
% \[\contains{\dslcom{f} : a^* \times (a^*)^* \leq a^*}{\mathsf{decomp_{a^* \times (a^*)^*}}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\mathsf{recomp}_{a^*}}{a^* \times (a^* )^*}{a^*}\]
% from which one can apply the fix-rule with side condition $S_1$ to derive
% \[\contains{}{\fix~f. \dslcom{d}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\dslcom{r}}{a^* \times (a^* )^*}{a^*}\] 
This corresponds to the gallina code below
\begin{minted}{Coq}
Definition decomp a := ...
Definition recomp a := ...
Fixpoint grambeyer_denesting (a : (Star a) * (Star (Star a))) : Star a := 
let res := 
 match decomp a with 
 | inl tt => int tt 
 | inr (a,sum) => match (assoc (untag sum)) with 
                   | (tt,p) => (tt, f p)
                  end 
in recomp res
\end{minted}
This yields a linear runtime assuming effecient implementations of $\mathsf{decomp}$ and $\mathsf{recomp}$.
% This is syntactically checked by $S_1$. And important details is that this side condition is satisfied because $\myo{\Sigma_{a \in \Sigma} \derive{a}{A}} = 0$.
\end{example}








% \subsection{Grabmeyer}
% %Brozowski showed that modulus rules of associativity, commutativity and transitivity, a regular expression has a finite set of unique derivatives. 
% Grabmeyer's axiomatization is influenced by the work of Brandt and Henglein (1997) who show that that a coinductive definition of recursive type equality (modulus unfolding) $\tau = \tau'$, can be characterized inductively using a fixpoint rule similar to the one seen for the coercion system. This is due to the regularity of recursive types $\tau$ containing only finitely many distinct subterms, allowing an inductive characterization. Grabmeyer applies this technique to characterize regular expression equivalence inductively with a similar fix rule. The technique is applicable in this context because Brozowski showed that modulus ACI, a regular expression has a finitely many distinct derivatives. Henglein and Nielsen prove completeness for variant of Grabmeyer's axiomatization that contains rules of equational logic, context rules and ACI-rules along with COMP/FIX.
% % The first one is due to Brandt and Henglein who show that 
% % that the method of Brandt and Henglein could be used in this setting as well. He showed that modulus some ACI rewriting\footnote{To minimize the size of the generated relations he allowed more rules than ACI}, regular expression equivalence can be charaterized by a natural deduction system that emulates finitary coinduction via the COMPFix rule. One can see finitary coinduction in action by the accumulation of $(A,B)$ into the context of the second premise.
% \begin{definition}[Grabmeyer's COMP/FIX]
% \[\infer{\Gamma \vdash A =  B}{\myo{A} =  \myo{B} & \Gamma, A = B \vdash \derive{a}{A} = \derive{a}{B},~\forall a \in \Sigma}\] 
% \end{definition}
% A remark about COMP/FIX is its operational nature, refering to derivatives $\derive{a}{A}$ and $\myo{\cdot}$. For translating Grabmeyer proofs into coercion derivations, Henglein and Nielsen rely Salomaas proof of the following equivalence
% \[ A = \myo{A} + \decomp{A}\]
% Which by completeness of their coercion sytem, for all $A$ implies the existence of coercions
% \[\contains{}{decomp_A}{A}{\myo{A} + \Sigma_{a \in \Sigma} \derive{a}{A}} \qquad \contains{}{recomp_A}{\myo{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A}\]
% %We have seen that Kozen proofs leads to slow coercions and a natural question is then which proof rules gives rise to fast coercions? Henglein and Nielsen argue that Grabmeyer rules lead to fast coercions.
% Using these coercions, Grabmeyer proofs can be translated into coercion derivations of the form:
% \[
% \contains{}{\fix ~f.decomp_A;(id + \Sigma_{a \in \Sigma} id\times f_a); recomp_B}{A}{B} \qquad \text{where }\mathsf{f_a} : \derive{a}{A} \leq \derive{a}{B}, \forall ~ a \in \Sigma \]
% % Which corresponds to the gallina code
% % \begin{minted}{Coq}
% % Fixpoint coerce (a : A) : B := 
% % let res1 := match decompose A with 
% %             | inl tt => inl tt
% %             | inr a_sum => match sum with 
% %                            | (e,a_sum) => (e,f_e (a_sum))
% %                            | (e',a_sym) => (f,f_e' (a_sum))
% %                             ...
% %                             end 
% %             end 
% % in recompose res1
% % \end{minted}
% \begin{example}[Effecient Denesting]
% We prove denesting $a^* \times (a^*)^* \leq a^*$ in Grabmeyer's axiomatization and consider the computational interpretation. To keep terms of managable size we set $\Sigma = \{a\}$ 

% Assume $\dslcom{f}: a^* \times \dstar a \leq a^*$. Then we prove the containment as:
% \begin{align}
% a^*\times \dstar a &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a) &&\quad \mathsf{decomp_{a^* \times (a^*)^*}}\\
% & \leq 1 + a \times (((1 \times a^*) \times \dstar a)) &&\quad \text{idempotence}\\
% &\leq 1 + a \times ((1 \times (a^* \times \dstar a)) &&\quad \text{associativity}\\
% & \leq 1 + a \times ((1 \times a^*) &&\quad \dslcom{f}\\
% & \leq a^* && \quad \mathsf{recomp_{a^*}}
% \end{align}
% Which derives 
% \[\contains{\dslcom{f} : a^* \times (a^*)^* \leq a^*}{\mathsf{decomp_{a^* \times (a^*)^*}}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\mathsf{recomp}_{a^*}}{a^* \times (a^* )^*}{a^*}\]
% from which one can apply the fix-rule with side condition $S_1$ to derive
% \[\contains{}{\fix~f. \dslcom{d}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\dslcom{r}}{a^* \times (a^* )^*}{a^*}\] 
% This corresponds to the gallina code below
% \begin{minted}{Coq}
% Fixpoint grambeyer_denesting (a : (Star a) * (Star (Star a))) : Star a := 
% let res := 
%  match decompose a with 
%  | inl tt => int tt 
%  | inr (a,sum) => match (assoc (untag sum)) with 
%                    | (tt,p) => (tt, f p)
%                   end 
% in recompose res
% \end{minted}
% This terminates because the recursive call is applied to a parse tree of a shorter string and yields an effecient program assuming effecient implementations of $\mathsf{decomp}$ and $\mathsf{recomp}$.
% % This is syntactically checked by $S_1$. And important details is that this side condition is satisfied because $\myo{\Sigma_{a \in \Sigma} \derive{a}{A}} = 0$.
% \end{example}
\section{Decomposition, effeciency and polymorphism}
In this section we define quick and slow decomposition coercions. We show that the coercion system presented so far allows effecient implementation of decomposition for a \textit{concrete} regular expression $A$. This is in contract to a generic decomposition that in one go, defines effecient decomposition for all regular expressions. Intuitively this would be a polymorphic coercion $\contains{}{c}{\forall X. X}{\myo{o} + \decomp{X}}$, but since we do not have a $\forall$- introduction rule this is not derivable in the system and only serves as an intuition for what we mean by a generic decomposition. Recomposition is analogous and is therefore not mention in this section.
%Assuming constant time execution of \textsf{decompose} and \textsf{recompose}, this code executes in linear time, applying the recursive call to the parse tree corresponding to the tail of the underlying string. Assuming constant time execution of \textsf{decompose} and \textsf{recompose} is however too strong because decomposition intuitvely traverses the parse \textsf{fold (inr ((fold (inr (a,t)))))} tree to expose the event \textsf{a}.
\subsection{Defining Effeciency}
We express the runtime of a coercion $\contains{}{c}{A}{B}$ applied to a parse tree $\mathsf{t} : \type{A}$ as a function of the string length of $\mathsf{t}$, denoted by $\flatten{\mathsf{t}}$. We postpone the definition of interpretation, $\interp{\contains{}{c}{A}{B}} : \type{A} \rightarrow \type{B}$ to Section \ref{sec:soundinterp} and for now suffice with the intuitive interpretation of coercions as functional programs.

Regular expressions are ambigous as a grammer and two parse trees of the same regular expression, $\mathsf{t} : a^*$, $\mathsf{t'} : a^*$, of the same string, $\flatten{t}=\flatten{t'}$ can differ arbitarily much in the size of the parse trees by repeatedly putting  $() : 1$ in the leaves of the tree. We say a compact parse tree $\mathsf{t} : A$, is the smallest parse tree of string $\flatten{t}$, implying that the parse tree size will differ from the string length by a constant factor. An effecient coercion, executes in time linear with respect to the size of a compact parse tree.
To allow such a runtime, decomposition should be a constant time operation on compact parse trees. If the tree is sparse, the runtime should be linear the length of the path that is a depth-first-search of the parse tree for the first non $(): 1$ leaf.
\subsection{Polymorphic but slow}
Salomaa implicitly defined the polymorphic decomposition $\contains{}{c}{\forall X. X}{\myo{o} + \decomp{X}}$ and recomposition $\contains{}{c}{\forall X.\myo{o} + \decomp{X}}{X}$ when he proved $A = \myo{A} + \decomp{A}$ for his system $F_1$ by structural induction on $A$ \mycomment{is that the right name of system?}. Computationally this corresponds to definition by mutual structural recursion, with the direction the induction hypothesis is used to indicate a call to $\mathsf{decomp}$ or $\mathsf{recomp}$. The ineffeciency of the definition is is caused by the $A^*$ case of the proof, when  $A^*$ is a \textit{problematic} regular expression, ie. if $\mynu{A} = \textsf{true}$. In this case of the proof, one has inductiion hypotheses $A \leq 1 + \Sigma_{a \in \Sigma} \derive{a}{A}$ and  $1 + \Sigma_{a \in \Sigma} \derive{a}{A} \leq A$ and must show $A^* \leq 1  + \Sigma_{a \in \Sigma} ((\derive{a}{A})\times A)^*$ and  $1  + \Sigma_{a \in \Sigma} ((\derive{a}{A})\times A)^* \leq A^*$. A sketch of the proof for the first containment is
\begin{proof} (Sketch)
\begin{align}
A^* &= ( \Sigma_{a \in \Sigma} \derive{a}{A})^* &&\text{by star context with decomp then drop}\\
 &= 1 + (\Sigma_{a \in \Sigma} \derive{a}{A}) \times (\Sigma_{a \in \Sigma} \derive{a}{A})^* &&\text{by unfold}\\
 &= 1 + (\Sigma_{a \in \Sigma} \derive{a}{A}) \times A^* && \text{by drop then star context with recomp } \\
 &= 1 + (\Sigma_{a \in \Sigma} (\derive{a}{A}) \times A^*) && \text{by distributivity }
\end{align}
\end{proof}
%One turns the problematic regular expression $A^*$ into an unproblematic one $B^*$ by using drop and after a few more steps getting the shap $1 + B \times A^*$ where $\mynu{B} = \false$. 
Thinking of $A^*$ as a list, this proof decomposes each element of the list, extracts the head, then recomposes the tail. Moreover, we used the induction hypothesis (recursive call) in both directions to achieve this. This yields an factorial time algorithm 
\subsection{Fast but monomorphic}
We now show how to derive an effecient decomposition for $a^* \times (a^*)^*$ with $\Sigma = \{a\}$. The return type is
\[1 + a \times ((1 \times a^*) \times (\dstar a) +  a \times ((1 \times a^*) \times (\dstar a) )\]
%To be precise, given that a parse tree has leaves \textsf{tt} and \textsf{Event a}, fast decomposition (and recomposition) has a recursion pattern that is a depth-first traversal of the parse tree that halt at the occurence of the first \textsf{Event a} leaf. For example, in the parse tree \textsf{fold (inr ((fold (inr (a,t)))))}, the recursive shall should not be applied to the subterm \textsf{t} in \textsf{(a,t)}. It is important to emphasize that we only are interested in effecient decomposition/recomposition \mycomment{any aspects of recomposition that might differ from decomposition? Yes I think recomposition does not need fixpoints, which could be mentioned, also check code}, since this might be performed multiple times in a Grabmeyer-style coercion derivation. Salomaa proved that $A = \decomp{A}$ and by completeness of the HN system, there must exist decomposition and recomposition coercions, but they are very slow. They take exponential time to compute. In the rest of this section we focus mostly on decomposition as recomposition is analogous.\\\\
%Haven emphasized the importance of effecient decomposition we now show an effecient decomposition of $a^*\times \dstar a$ into $\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a)$ derivable in HN: Given the input type $a^*\times \dstar a$, our input is parse tree $(t_1,t_2)$ and we should only inspect $t_2$ if the leaves of $t_1$ are all \textsf{tt}. 
Intuitively a parse tree $\mathsf{t} : a^* \times (a^*)^*$ is a list $\mathsf{l} : \mathsf{list~a}$ paired with a nested list $\mathsf{ll} : \mathsf{list~(list~a)}$. We want to define a function that tries to find the first $\mathsf{a}$ in $(\mathsf{l},\mathsf{ll}) : \mathsf{list~a} \times \mathsf{list~(list~a)}$
\begin{minted}{Haskell}
decomp ([],[]) = inl ()
decomp ([],l::ll') = decomp (l,ll')
decomp (a::l',ll) = inr (a,(l',ll))
\end{minted}
% We start from the intutive gallina program
% \begin{minted}{Coq}
% Fixpoint decomp (aa : (Star a) * (Star (Star a))) 
%  : 1 + a * ((1 * Star a) * (Star (Star a)) + 
%             (1 * Star a) * (Star (Star a)))  := 
% match aa with 
% | (sa,ssa) => match sa with 
%                | fold (inl tt) => match ssa with 
%                                    | fold (inl tt) => inl tt
%                                    | fold (inr pp) => decomp pp
%                | fold (inr (a,sa)) => inr (a,inl ((tt,sa),ssa))
%               end
% end
% \end{minted}
The program is terminating because we only recurse on subterms of the input. It is effecient it stops at the first event leaf. We now derive the coercion assuming the hypohtesis $\dslcom{f} : \denesting \leq \myo{\denesting} + a \times \derive{a}{(\denesting)}$
% \begin{align*}
% a^* \times \dstar{a} &\leq (1 + a \times a^*) \times \dstar{a}  &&\dslcominv{wrap}  \\
%                      &\leq 1 \times \dstar{a} + (a \times a^* ) \times \dstar{a} &&\dslcom{distR}\\
%                      &\leq 1 \times (1 + a^* \times \dstar{a})  + (a \times a^* ) \times \dstar{a} &&\dslcominv{wrap}\\
%                      &\leq 1 \times 1 + 1 \times (a^* \times \dstar{a})  + (a \times a^* ) \times \dstar{a} &&\dslcom{distL}\\
%                      &\leq 1 \times 1 + \Big(1 + a \times \derive{a}{(\denesting)} \Big) + (a \times a^* ) \times \dstar{a} &&\dslcom{id}+((\dslcom{id} \times \dslcom{f});\dslcom{proj})\\
%                      &\leq 1 \times 1 + \Big(1 +  (a \times a^* ) \times \dstar{a} \Big) + (a \times a^* ) \times \dstar{a} &&\dslcominv{assoc};((\dslcom{id} \times \dslcominv{proj})\times \dslcom{id})\\
%                      &\leq 1 + 1 \times (a^* \times \dstar{a}) +  a \times (a^*  \times \dstar{a})\\
%                      &\leq 1 + 1\times ( 1 + a \times (((1 \times a^*) \times \dstar a)))) +\\
%                      &\quad(1 \times a^*) \times \dstar a))  + a \times (a^*  \times \dstar{a}) \\
%                      &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a)
% \end{align*}
\begin{align}
a^* \times \dstar{a} &\leq (1 + a \times a^*) \times \dstar{a}  &&\dslcominv{wrap}  \\
                     &\leq 1 \times \dstar{a} + (a \times a^* ) \times \dstar{a} &&\dslcom{distR}\\
                     &\leq 1 \times \dstar{a} +  a \times (a^*  \times \dstar{a}) && \dslcom{assoc} \\
                     &\leq 1 \times (1 + a^* \times \dstar{a}) +  a \times (a^*  \times \dstar{a})&& \dslcominv{wrap} \\
                     &\leq 1 + 1 \times (a^* \times \dstar{a}) +  a \times (a^*  \times \dstar{a})&&\dslcom{distL}\\
                     &\leq 1 + 1\times \Big( 1 + a \times ((1 \times a^*) \times \dstar a)~+\\
                     &\quad(1 \times a^*) \times \dstar a\Big)  + a \times (a^*  \times \dstar{a}) && \dslcom{f} \\
                     &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a) && \dslcom{untag}
\end{align}
The coercion we have built is:
\begin{align}
\fix f.~&(\dslcominv{wrap} \times \dslcom{id});\dslcom{distR};\\
 \Big( &(\dslcom{id} \times \dslcominv{wrap});\dslcom{distL};\\
       &\dslcom{untag} + ((\dslcom{id} \times \dslcom{f});\dslcom{proj});\\
       &\dslcominv{assoc};(\dslcom{untag} + \dslcom{untag}) \\
&+\\
 &\dslcom{id} \times \dslcom{projinv} \times \dslcom{id} \Big);\\
 & \Big(\dslcom{id} + \dslcom{assoc} ; \dslcom{untag} \Big)
\end{align}
% \begin{align}
% a^* \times \dstar{a} &\leq (1 + a \times a^*) \times \dstar{a}   \\
%                      &\leq 1 \times \dstar{a} + (a \times a^* ) \times \dstar{a}\\
%                      &\leq \dstar{a} +  a \times (a^*  \times \dstar{a}) \\
%                      &\leq 1 + a^* \times \dstar{a} +  a \times (a^*  \times \dstar{a})\\
%                      &\leq 1 + ( 1 + a \times (((1 \times a^*) \times \dstar a) +\\
%                      &\quad(1 \times a^*) \times \dstar a))  + a \times (a^*  \times \dstar{a}) \\
%                      &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a)
% \end{align}
% \begin{align}
% \fix f.~&(\dslcominv{wrap} \times \dslcom{id});\dslcom{distR};\\
% &\dslcom{proj};\dslcominv{wrap};(\dslcom{id} + \dslcom{f});\dslcominv{associnv};(\dslcom{untag} + \dslcom{id});\dslcom{assoc};(\dslcom{id}+\dslcom{untag})\\
% &+\\
%                                                                                               &(\dslcom{proj} \times \dslcom{id})
% \end{align}
This is derivable with side condition $S_2$ because no use of $\dslcominv{proj}$ happens before the recursive call.
\begin{remark}[A new side condition]
Refering to the decomposition coercion above as $\dslcom{d}$, and the recomposition coercion which we have omitted as $\dslcom{e}$, the Grabmeyer coercion for denesting is
\[ \fix~f. \dslcom{d}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\dslcom{r} \]
This consists of the outer fixpoint $\fix f$ satisfying $S_1$ and a nested fixpoint in $\dslcom{d}$ satisfying $S_2$. We have thus given an example of a fast coercion that in order to be derivable requires a side condition $S_5 = S_1 \lor S_2$ not present in Henglein and Nielsen.

We believe that language containments can always be expressed as Grabmeyer coercions using $S_5$ but proving this is difficult due to the non-compositional nature of $S_2$. This side condition dictates that a recursive call may not happen after $\dslcominv{proj}$ which prevents a polymorphic decomposition by structural recursion on the regular expression.
\end{remark}
\subsection{Fast and polymoprhic}
We would like an effecient and polymorphic decomposition but its natural definition is not admissible in the coercion system. To see why, assume, consider again the $A^*$ case with $\mynu{A} = \true$. We have induction hypothesis $\dslcom{IH} : A \leq 1 + \decomp A$ and add hypothesis $\dslcom{f} : A^* \leq 1 + (\decomp{A}) \times A^*$
\begin{proof} (Sketch)
\begin{align}
A^* &\leq  1 + A \times A^* && \text{by unfold}\\
 &\leq 1 + (1 + \decomp{a}) \times A^* && \text{by } \dslcom{IH}\\
 &\leq 1 + 1 \times A^* + (\Sigma_{a \in \Sigma} \derive{a}{A} \times A^*) && \text{by distibutivity}\\
 &\leq 1 +  1 \times (\decompp{A}{A^*}) +  (\Sigma_{a \in \Sigma} \derive{a}{A} \times A^*)  && \text{by } \dslcom{f}  \\
 &\leq 1 + \decomp{A^*} && \text{idempotence} 
\end{align}
\end{proof}
The dsl program this proof builds is \mycomment{did not check details}
\[ \fix ~f.\dslcom{wrap};(\dslcom{id} + (\dslcom{IH};\dslcom {id});\dslcom{distR};(\dslcom{proj};\dslcom{f})+\dslcom{id});\dslcom{shuffle};(\dslcom{untag} + \dslcom{untag}) \]
Termination is ensured by recursing only on subterms of the input parse tree. Though it is terminating, it does not satisfy and of the decidable side conditions in Henglein and Nielsen. It only satisfies their most general, but undecidable, side condition, hereditary totalilty
% \footnote{All side conditions except for the one used to code Kozen derivations are immediate to verify. The Kozen side condition fails because it disallows use of \dslcom{projinv} before the recursive call and \dslcom{e} uses this command in the event case of the decomposition}.
In the previous example, we satisfied $S_2$ by sequencing $\dslcominv{proj}$ after the recursive call. This is not possible here since the coercoin $\dslcom{IH}$, appearing before the recursive call, might use $\dslcom{projinv}$. Indeed if the regular expression contains an event $a$ and we for illustration take $\Sigma = \{a\}$, derivation of the decomposition $a \leq 0 + a \times 1 $ uses $\dslcominv{proj}$. Using the fixpoint rule would result in a terminating coercion because recursion would be on a subterm of the parse tree but syntactically this is not recognized by any of the side conditions that have been presented.


%\[
%\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \]

% It seems that any decomposition can be proved using the Kozen measure. Kozen measure relies on no projin between fixpoint and recursive call and this has some drawbacks. The decomposition of events requires projiinv and therefore the decomposition of $\dstar a$ cannot just blindly decompose the subterm because it would use projinv in an illegal position. Instead a projinv free version must be used and after finishing the recursive calls, one can fit the bill by applying the appropriate projinvs. This strictness has two drawbacks. It prevents decomposition from being recursively defined on the syntax of regular expressions, making it harder to define the parametric decomposition. Secondly it prevents tail-recursion for which ocaml can optimize code. We now introduce how we have done this...
\mycomment{Write a tail-recursive and non-tailrecursive example in benchmark}
\mycomment{Would deriving recomposition have projinv appear before recursive call, which is not allowed in Kozen measure? Recomposition does not seem to introduce anything new}
\mycomment{They now need disjunct of Kozen and Grabmeyer measure}
\mycomment{Kozen alone is not suffecient because decomposition uses projinv}
\mycomment{If we instead werer to prove nesting (not denesting), would we need projinv before recursive call}
\mycomment{Did we define recomposition effeciently in the code?}
\mycomment{Recomposition is not interesting because tagL can be used to make it work without recursion}
\mycomment{Our approach leads to tailrecursive decomposition I think, which could be faster in a tail recursion optimizing language like ocaml}



\mycomment{This subsection should change structure, we want to land at the introduction of peek rule. We get there from limitation of HN rules due to side conditions. Exemplified by example on paper. Relate to Salomaas proof of decomposition? A possible structure: Grabmeyer uses decomposition, HN show this leads to fast proofs, Salomaas decomposition, HN's decomposition, our decomposition with peek}
\section{A restrictive coercion language}
In this section we present a novel variant of the coercion system that: 1) Allows fast polymorphic decomposition; 2) restrict the variable rule for recursive call such that all derivable coercions become terminating without imposing a side condition on the fixpoint rule. This restriction has some advantages which explain.
% The decomposition operation can be viweed as a polymorphic coercion $\contains{}{c}{\forall X. X}{\myo{o} + \decomp{X}}$. Since we do not have a $\forall$- introduction rule this is not derivable in the Henglein and Nielsen system, but it serves as an intuition for how we might define in one go, a effecient generic decomposition, for all regular expressions. In Coq this would look like:
% \begin{minted}{Coq}
% Fixpoint (A : regex) : A -> (o A) +  
%                             \big[Plus/Empt]_(i <- l) ((Event i)_;_(i \ Event a) :=
% fun a => 
% match A with 
% | ....
% \end{minted}
% In fact, 
%HERE
%  Henglein and Nielsen sketch an approach to build effecient coercions by following the shape of a Grabmeyer derivation. Assuming that $A \subseteq B$,
%  prove that they can code proofs consisting solely of COMP-fix with transitivity, context-rules and ACI. To do this they make use of the the animorov decomposition in building their coercions. The decomposition and its adaptation to containment can be seen in Figure \ref{fig:decomp}
%  \begin{figure}
%    \centering
%    \begin{align}
% &A = \mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}\\
% &\contains{}{d}{A}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}} \label{eq:derive1}\\
% &\contains{}{e}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A} \label{eq:derive2}\\
% &\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \label{eq:coerce}
%    \end{align}
%    \caption{Regular expression decomposition}
%    \label{fig:decomp}
%  \end{figure}
% Salooma show this can derived using the rules of weak equivalence along with $(1 + A)^* = A^*$. Henglein and Nielsen prove that they can code Salomaas derivations; which implies they can derive coercions in (\ref{eq:derive1}, \ref{eq:derive2}). 
% Their claim however that Grabmeyer style coercions, as seen in (\ref{eq:coerce}), are always effecient linear time programs, is unfortunately wrong. In particular, for the synthesis approach given by Henglein and Nielsen it is slower than Kozen. The reason for this is that the effeciency of Grabmeyes style coercions relies on implementing constant time decomposition $\contains{}{d}{A}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}$ and re-composition $\contains{}{e}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A}$.\\
\subsection{A new language construct}
To solve the issue from the last section, rather than defining yet another side condition, we recognize that replacing $A$ with $(1 + \decomp A)$ caused the problematic reappearence of $A^*$ after distribution (\ref{align:problematic}). If we could replace $A$ with $(\decomp A)$, there would be no need for the fixpoint. The derivation would go like this:
\begin{proof} (Sketch)
\begin{align}
A^* &\leq  1 + A \times A^* && \text{by unfold}\\
 &\leq 1 + (\decomp{A}) \times A^* && \text{by } \dslcom{peek}~\text{with }\dslcom{IH}\\
 &\leq 1 + \decomp{A^*} && \text{distributivity} 
\end{align}
\end{proof}
Where we define $\dslcom{peek}$ as.
\[ \infer{\contains{}{peek ~ c}{A^*}{1 + B \times A^*}}{\contains{}{c}{A}{1 + B}} \]
\begin{remark}
It seems likely that a variant of the Kozen measure could be defined that would allow $\dslcom{peek}$ to be derived from the Henglein and Nielsen's fix rule. This is no consequence to us as we are aiming at replacing the general fixpoint construct with a stricter construct that only allows obviously terminating recursion
\end{remark}

\subsection{Restricting the fixpoint}
We have described an algorithm to synthesize effecient decomposition coercions using $\dslcom{peek}$. In particular, the decomposition makes no use of the general fixpoint. For now we however use this decomposition to build Grabmeyer coercions that however do make use of the general fixpoint rule
\[\infer{\containsP{}{\fix ~f.decomp_A;(id + \Sigma_{a \in \Sigma} id\times f_a); recomp_B}{A}{B}{S_1}}{\containsP{\dslcom{f} : A \leq B}{f_a}{\derive{a}{A}}{\derive{a}{B}}{S_1}, \forall a \in \Sigma} \]

% \begin{minted}{Coq}
% Fixpoint decomp (A : regex) : A -> (o A) +  
%                             \big[Plus/Empt]_(i <- l) ((Event i)_;_(i \ Event a)

% fun a =>
% match A with 
% | Star B =>  match drop ((star_ctx decomp) a)
%              | fold (inl ()) => inl () 
%              | fold (inr (a',a_sigma)) =>  distr a' (star_ctx comp (dropinv a_sigma))
% ...
% end 
% with
%          comp (A : regex) : (o A) +  
%                             \big[Plus/Empt]_(i <- l) ((Event i)_;_(i \ Event a) -> A
% \end{minted}
% \footnote{We omit definition of \textsf{distr}. As we saw earlier, \textsf{starctx} and \textsf{drop} are linear time taversals of the parse tree.}
% This program has exponential runtime due to the recursive call \textsf{decomp} in \textsf{starctx}. The ineffeciency is due to the decomposition of the wole list when we only care about the decomposition of the head. 

% \begin{lemma}
% \noindent \\
% (1) Decomposition and recomposition is derivable with the rules of weak equivalence with $\dslcom{peek}$ and $\dslcom{peekinv}$\\
% (2) The runtime is best-case constant and worst-case linear
%\end{lemma}
% \[
% \contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \]
In the work of Henglein and Nielsen it is beneficial to have a general fixpoint rule that can be instantiated with different side conditions, as it makes their proof space so large that they can derive all the rules in each of the three axiomatizations they study (Salomaa,Kozen,Grabmeyer). Their proof space thus contains fast linear-time coercion and slow exponential time-coercions. In this work however, our interests differ, we are interested in: 1) Effecient and simple synthesis of coercions; 2) The interpretation of these coercions must yield linear time programs.
Regarding the second point, we saw that Grabmeyer coercions interpret to fast programs, so we will aim restricting recursion to only allow us such derivations. Let us investigate the recursion pattern in a Grabmeyer coercion:\\
For regular expression $A$, after executing decomposition $\dslcom{d}$ we have 
 \[ \myo{A} + \decomp{A} \]
 The right summand is piped into $\Sigma_{a \in \Sigma} \dslcom{id}\times f_a$, where $\dslcom{id}$ will be applied to the parse tree $\mathsf{a} : a$ for each $a \in \Sigma$. Any recursive call to $\dslcom{f}$ in any of the continuations $\mathsf{f_a}$, will thus happen in the right component $\mathsf{t} : \derive{a}{A}$ of the pair $(\mathsf{a},\mathsf{t}) : a \times \derive{a}{A}$. 
% The general fix rule is more general than we need it to be and for mechanisation purposes it is more convenient to restrict our syntax so that it does not contain exotic terms (those that do not satisfy the side condition in Henglein and Nielsens system).
This recursion pattern tells us that we may freely add our conclusion to our set of assumptions without any sideconditions:
\[\infer{\contains{\Gamma}{\fix ~f.d}{A}{B}}{ \contains{\Gamma, \dslcom{f}: A \leq B}{d}{A}{B}}
\]
So long that we restrict discharge of an assumption till after the consumption of an event
\[\infer{\contains{\Gamma, \mathsf{f} : A \leq B, \Gamma'}{var~f}{a \times A}{a \times B}}{}
\]
Let us call this for event guarded recursion. Notice that event guardedness is built into the syntax; the assumption $\mathsf{f}$ with input type $A$ can be used only to build $\mathsf{var~f}$ which has input type $a \times A$.

We can now replace the general fix rule with event guarded recursion and $\dslcom{peek}$. The effect of restricting the coercion language is that firstly; Only fast proofs are derivable. Secondly, the system has no exotic terms, $\fix f. f$ is not derivable. Put differently, all rules are compositional, and context is not necessary in order to build a subterm in the coercion sytem. 

These changes simplifies the soundness proof which essentially comes down to proving termination of interpretation. Restricting the syntax of coercions simplifies the termination argument for the interpreter. 

Completeness on the other hand is essentially coercion synthesis and the elimination of the side condition removes the computation cost of checking the side conditions during synthesis. This is significant for side condition $S_2$ which takes time linear in the size of the coercion.\\\\
The new definition of the coercion system can be seen in Figure (\ref{fig:system})
\begin{figure}
\caption{The new coercion system is the rules of Henglein and Nielsen, with fixpoint and variable rules replaced by these rules}
\label{fig:system}
\centering
\begin{displaymath}
\begin{array}{lll}
% \myaxiomC{shuffle}{A + B + C}{A + (B + C)} \qquad  
% \myaxiomC{retag}{A + B}{B + A} \qquad 
% \myaxiomC{untagL}{0 + A}{A} 
% \\\\
% \containsG{untag}{A + A}{A}   \qquad \containsG{tagL}{A}{A + B} \qquad

% \myaxiomC{assoc}{A \times B \times C}{A \times (B \times C)}
% \\\\\
% \myaxiomC{swap}{A \times 1}{A} 
% \myaxiomC{proj}{1 \times A}{A} \qquad
% \myaxiomC{abortR}{A \times 0}{0} \qquad
% \myaxiomC{abortL}{0 \times A}{0} 
% \\\\
% \myaxiomC{distL}{A \times (B + C)} {A \times B + A \times C} \qquad
% \myaxiomC{distR}{(A + B)\times C}{A \times C + B \times C} 
% \\\\
% \myaxiomC{wrap}{1 + A \times A^*}{A^* } \qquad \myaxiomC{id}{A}{A}
% \\\\
% \infer{\containsG{c;d}{A}{C}}{\containsG{c}{A}{B} & \containsG{d}{B}{C}} \qquad

% \infer{\containsG{c + d}{A + B}{ C + D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}}  \qquad

% \infer{\containsG{c \times d}{A \times B}{ C \times D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}} 
% \\\\
\infer{\contains{}{\dslcom{peek}~d}{A^*}{1 + B \times A^*}}{\contains{}{d}{A}{1 + B}} \qquad
\\\\
\infer{\contains{\Gamma}{\fix ~f.d}{A}{B}}{ \contains{\Gamma, \dslcom{f}: A \leq B}{d}{A}{B}} \qquad
\infer{\contains{\Gamma, \mathsf{f} : A \leq B, \Gamma'}{var~f}{a \times A}{a \times B}}{}
\end{array}
\end{displaymath}
\end{figure}


\section{Completeness and synthesis}
It is known that language equivalence and containment of reuglar expressions can be expressed coinductively as simulation (rspt. bisimulation) using the brozowski derivative $\derive{a}{A}$ to transition between regular expressions and nullariness operator $\mynu{\cdot}$ for finality. With this extensional view,language containment can be stated coinductively as:
\begin{definition}[Simulation]
\[\infer={A \sim B}{\mynu{A} \implies \mynu{B} & \forall a \in \Sigma, \derive{a}{A} \sim \derive{a}{B} }\]
\end{definition}
\begin{lemma}
Simulation coincides with language contaiment
\end{lemma}
Simulation is a coinductive definition, and has the following coinduction principle (taking $F$ as the generating function for $\sim$):\\
\[ (A,B) \in R \land R \subseteq F(R) \implies A \sim B  \]
That is, to show a simulation it suffices to find a (possibly infinitary) relation $R$ that is a post fixedpoint of $F$. 

% This proof principle can be awkard to use because a post fixed point must be guessed up front. Hur et al. [cite] introduce parameterized coinduction that allows $R$ to be constructed incrementally throughout the proof. They introduce the parameterized greatest fixpoint
% \[ G_f(R) \triangleq gfp(\lambda X. F(X \cup R))
% \]
% Where $G_f(\{\}) = gfp(F)$. The following rule allows the accumulation into $R$
% \[ Y \subseteq G_f(X) \iff Y \subseteq G_f(X \cup Y)
% \]
% Relating this back to simulation, is showing an inclusion:
% \[A \sim B = \{(A,B)\} \subseteq G_f(\{\})  \]
% If we were to prove that $A^*$ is in simulation with $\dstar{A}$, for any $A$, the accumulation rule would add to $R$ the following infinitary relation
% \[ X = \{ A^*  \sim  \dstar{A} | A \in \mathcal{Regex} \} \]
% \[  X \subseteq  G_f(\{\})  \iff X \subseteq  G_f(\{X\})  \]
% We will call this infinitary coinduction.\\
% If on the other hand we were to prove this statement with $A$ fixed to $\event{a}$ for some specic event $a$, we would accumulate $R$ by a single pair:
% \[  \{(\dstar{\event{a}},\event{a}^*)\} \subseteq  G_f(\{\})  \iff X \subseteq  G_f(\{(\dstar{\event{a}},\event{a}^*))  \]
% We will call this finitary coinduction. Brandt and Henglein (1997) showed that some times finitary coinduction can be captured by natural deduction systems. For example, they showed that coinductively defined equivalence of $\mu$-types
% can be captured by a natural deduction judgment $\Gamma \vdash \tau = \tau' $ where $\Gamma$ corresponds to the $R$ that accumulates during parameterized coinduction. As natural deductions are inductive finite derivations, only finitary coinduction can be modeled, which works for $\mu$-types because they encode regular trees which have finitely many distinct sub-trees, putting an upperbound on the reachable pairs one may see during a derivation. A similar result holds for regular expressions.\\

% We recall that the definition of simulation was:
% \[\infer={A \sim B}{\mynu{A} \implies \mynu{B} & \forall a \in \Sigma, \derive{a}{A} \sim \derive{a}{B} }\]
Simulation coincides with language containment so to show completeness it suffices to prove:
\[ A \sim B \implies \exists c, \contains{}{c}{A}{B} \]
That is, we must synthesize coercion $\dslcom{c}$. Using the decomposition from the previous section, intuitively $\dslcom{c}$ will take the shape:
\[\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B}\]
And the continuations $\dslcom{f}_a$ will be derived under the extended assumption: $\contains{(A,B)}{f_a}{\derive{a}{A}}{\derive{a}{B}}$. In general, our context $\Gamma$ in $\containsG{c}{E}{F}$ is finite and we can thus not simulate the infinitary coinduction going on in the simulation relation. We must constrain simulation to an equivalent relation that only does finitary coinduction.\\
This gives rise to a finite characterisation of containment as a terminating decision procedure,
\begin{align}
\simulationb{V}{A}{B} =\begin{cases}
			\true, & \text{if }(A,B) \in V\\
                      \bigwedge_{a \in \Sigma}\simulationb{V \cup \{(A,B)\}}{\derive{a}{A}}{\derive{a}{B}}, & \text{otherwise}
		 \end{cases}
\end{align}
So long that we let $(A,B) \in V$ check membership modulus ACI. To show completeness of the coercion system one could do it as:
\begin{enumerate}
\item Prove $A \sim B$ implies $\simulationb{\{\}}{A}{B}$, by finitiness of distinct derivatives modulus ACI
\item Prove $\simulationb{\{\}}{A}{B}$ implies $\exists c.~\contains{}{c}{A}{B}$ by functional induction
\end{enumerate}
Doing (1.) in a proof assistant like Coq is challenging because the termination argument must be translated into a property that can be characterized by an inductive judgment, which we can structurally recurse on. The Bragga method [cite] explains in detail how one may separate the termination argument from the definition of a recursive function, by instead of reucrsing on the input $x$, recursing on an inductive proof about $x$, called $\mathbb{D}~x$. We take this approach and it is easier to define $\mathbb{D}~x$ if we move from standard brozowski derivatives to partial derivatives. Normally partial derivatives are defined by mapping a regular expression to a set of regular expression. In our mechanisation we use inductive lists instead of sets, and to keep the paper formalization as close as possible to the mechanisation, we will present partial derivatives exactly as they have been defined in the code.\\
\begin{definition}[Partial derivatives]
  \begin{displaymath}
    \begin{array}{lll}
\pd{a}{0} = [0] \qquad \pd{a}{1} = [0] \qquad \pd{a}{a} = [1] \qquad \pd{a}{a'} = [0]\text{if}~a \neq a'\\\\
\pd{a}{A + B} = \cat{\pd{a}{A}}{\pd{a}{B}} \qquad \pd{a}{A \times B} = \cat{(\mathsf{map }(\lambda x.~x \times B)~\pd{a}{B})}{\pd{a}{B}} ~\text{ if }~\mynu{A} = 1~\\\\
\pd{a}{A \times B} = (\mathsf{map }(\lambda x.~x \times B)~\pd{a}{B}) ~\text{ if }~\mynu{A} = 0~ 
\qquad \pd{a}{A^*} = \mathsf{map }(\lambda x.~x \times A^*)~\pd{a}{A}  \\\\
    \end{array}
  \end{displaymath}
Overloading notation, the partial derivative of a list of regular expressions $\tilde{A}$ is:
\[\pd{a}{\tilde{A}} = \mathsf{undup}(\mathsf{flatten}(\mymap{\partial_a}{\tilde{A}}))\]
The nullariness of a list of regular expressions is:
\[\myo{\tilde{A}} = 1 ~\text{if }~\exists A \in \tilde{A}~st. ~ \mynu{A}=1 \text{ otherwise } 0\]
\end{definition}
We can now define Simulation with partial derivatives
\[\infer={\tilde{A} \sim \tilde{B}}{\mynu{\tilde{A}} \implies \mynu{\tilde{B}} & \forall a \in \Sigma, \mypar{a}{\tilde{A}} \sim \mypar{a}{\tilde{B}} }\]

\begin{lemma}
$A \sim B$  implies $[A] \sim [B]$
\end{lemma}
\mycomment{actually the code only uses the partial one, goes directly from language containment I think}
We compute the $\pi$ enumeration from \cite{CZ01} adapted to lists
\begin{definition}[Enumeration]
  \begin{displaymath}
    \begin{array}{lll}
  \pi(0) = [] \qquad \pi(1) = [] \qquad \pi(a) = [1] \qquad \pi(A + B) = \cat{\pi(A)}{\pi(B)} \\\\
  \pi(A \times B) = \cat{\mymap{\lambda x.~x \times B}{\pi(A)}}{\pi(B)} \qquad \pi(A^*) = \mymap{\lambda x.~x \times A^*}{\pi(A)}
    \end{array}
  \end{displaymath}
We extend $\pi$ to lists of regular expressions:
\[\pi([])=[[]] \qquad \pi(A::\tilde{A}) = \mathsf{cartesian}(A,\pi(\tilde{A}))\]
We extend this to a pair of lists of regular expressions:
\[\pi(\tilde{A},\tilde{B})= \mathsf{cartesian}(\pi(\tilde{A}),\pi(\tilde{B}))\]
\end{definition}
The reason we use partial derivatives is so that we can get the following property:
\begin{lemma}[Closure]\label{lem:closure}
 $(\tilde{A},\tilde{B}) \in \pi(\partial_a(\tilde{C},\tilde{D}))$ implies  $(\tilde{A},\tilde{B}) \in \pi((\tilde{C},\tilde{D}))$ 
\end{lemma}
This tells us that for the pair of regular expression lists $\pder{A}{B}$, their enumeration which we compute with$\pi$, contains the enumerations of all partial derivatives which you can find starting from $\pder{A}{B}$. This allows us to define the domain of $\simulationb{}{}$
\begin{definition}[Domain]
  \begin{displaymath}
    \begin{array}{lll}
      \infer[\mathbb{D}_{stop}]{\domm{V}{\pder{A}{B}}}{\pder{A}{B} \in V} \qquad \infer[\mathbb{D}_{step}]{\domm{V}{\pder{A}{B}}}{\pder{A}{B} \notin V & \mathsf{uniq}(\pder{A}{B}) & \forall a.~\domm{(\pder{A}{B}::V)}{\partial_a(\pder{A}{B})}}
    \end{array}
  \end{displaymath}
\end{definition}
And define the decision procedure that follows the course-of-values of $\mathbb{D}$:
\begin{align}
\simulationb{V}{\tilde{A}}{\tilde{B}} =\begin{cases}
			\true, & \text{if }(\pder{A}{B}) \in V\\
                      \bigwedge_{a \in \Sigma}\simulationb{V \cup \{\pder{A}{B}\}}{\mypar{a}{\tilde{A}}}{\mypar{a}{\tilde{B}}}, & \text{otherwise}
		 \end{cases}
\end{align}
The structural recursion of $\simulationb{V}{\pder{A}{B}}$ is defiend on $\domm{V}{\pder{A}{B}}$, which means the function is only defined on $\tilde{A}$ and $\tilde{B}$ when we can derive $\domm{V}{\pder{A}{B}}$. We prove the following:
\begin{lemma}
If uniq $\tilde{A}$ and $\tilde{B}$ , one can derive $\domm{V}{\pder{A}{B}}$ for any $V$.
\end{lemma}
\begin{proof}
Relies on Lemma (\ref{lem:closure}), from which we show the following decreasing measure:\\
Define $M(V,\pder{A}{B} = |\pi(\pder{A}{B}) \backslash \{V\}|  $ \mycomment{using set notation for lists}\\
If $\tilde{A}$ and $\tilde{B}$ are unique and $\pder{A}{B} \notin V$ then $M(\pder{A}{B}::V,\mypar{a}{\pder{A}{B}}) <  M(V,\pder{A}{B}) $
\end{proof}
\mycomment{We use a similar measure in ITP 2023 projection}
We can now state a main lemma:
\begin{lemma}
$A \subseteq B$ iff $\simulationb{nil}{[A]}{[B]}$
\end{lemma}
We proved this property to get a functional induction principle that will help us to prove completeness of the coercion system. As an intermediate result we however have a mechanised decidability of regular expression containment. In the mechanisation we define $\mathsf{bisimulationb}$ analogously to show that the method is the same for deciding regular expression equivalence. Many mechanisations of this result exists in the litterature, but we think this might be the shortest proof of regular expression equivalence in the litterature. It is shorter than [cite Matita], whose main contribution was the compactness of their proof. We believe our concicseness is achieved by defining partial derivatives using lists and computing a closure by extending the standard definition of $\pi(A)$ on a single regular expression to a list of regular expresions $\pi(\tilde{A})$.\\\\
Now with a functional indunction principle in hand we are ready to build the coercions. Since we are working with lists of regular expressions $\tilde{A}$, to avoid clutter we will write $\tilde{A}$ in a position where a regular expression is expected to mean $\mysum{A}{\tilde{A}}$ The main theorem of the completeness proof is:
\begin{theorem}
Assuming for all $\pder{E}{F} \in V$ that $\pder{E}{F} \in \Gamma$, then\\
$\simulationb{V}{\tilde{A}}{\tilde{B}}$ implies that there exists $\dslcom{c}$ s.t. $\contains{\Gamma}{c}{\tilde{E}}{\tilde{F}}$
\end{theorem}
% $\simulationb{V}{\pder{A}{B}}$ is defined on lists of regular expressions, so we define coercions that let us jump from 
\begin{proof}[Sketch]
By induction on $\domm{V}{\pder{E}{F}}$. Derive the following variants of decomposition and recomposition:
\begin{align}
&\forall \tilde{A},a.~\exists \dslcom{c}.~\contains{}{c}{(\tilde{A}}{\myo{\tilde{A}} + \Sigma_{a \in \Sigma} a \times \mypar{a}{\tilde{A}}} \ref{align:pdecomp}\\
&\forall \tilde{A},a.~\exists \dslcom{c}.~\contains{}{c}
{\myo{\tilde{A}} + \Sigma_{a \in \Sigma} a \times \mypar{a}{\tilde{A}}}
{\tilde{A}} \ref{align:precomp}
\end{align}
The proof follows the shape of the Grambeyer coercion:
\[\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B}\]
We first apply $\fix$, then by decomposition and recomposition (\label{align:pdecomp},\label{align:precomp}) it suffices to show for each $a \in \Sigma$ that there exists $\dslcom{c}$ s.t. $\contains{\Gamma,\pder{A}{B}}{c}{\mypar{a}{\tilde{A}}}{\mypar{a}{\tilde{B}}}$, which is our induction hypothesis.
\end{proof}
\begin{corollary}[Completeness and synthesis]
\mycomment{Mention this is compact decision procedure for equivalence, point to file}
(1): If $A \leq B$ then there exists $\dslcom{c}$ s.t. $\contains{}{c}{A}{B}$\\
(2): There exists a procedure such that decides the containment $ A \leq B$ and in the positive case synthesises a coercion $\contains{}{c}{A}{B}$
\end{corollary}


% computing an enumeration of reachable partial derivatives \mycomment{How is this different from equivP in technique and loc}.  


%Completeness and synthesis are related. Completeness does in this setting mean that all language containments \textsf{Contains A B} can be derived in the coercion system of Figure (\ref{fig:system}). In the presence of a decision procedure for language containment \textsf{dec : \{Contains A B\} + \{~ Contains A B\}}, we can use the completeness proof to synthesize a coercion, going from \textsf{Contains A B} to $\contains{}{c}{A}{B}$.
% Recall that we aim at synthesising Grabmeyer style coercions which inductively emulate finitary coinduction proofs of similarity. We recall the definition of simulation

%We follow the completeness proof of Henglein and Nielsen and build $c$ as a Grabmeyer coercion.\\




\section{Soundness and interpretation} \label{sec:soundinterp}
The coercion system is sound if all derivable coercions are language containments:
\[\contains{}{c}{E}{F} \implies E \subseteq F\]
This is closely related to the interpretation of a coercion $\interp{\dslcom{c}} : \type{E} \rightarrow \type{F}$ due to a correspondance between language membership $s \in A$ and type inhabitance $\mathsf{t} : \type{A}$
\begin{lemma}\label{lem:types}
$\mathsf{t} : \type{A}$ implies $\flatten{\mathsf{t}} \in A$\\
$s \in A$ implies $\exists \mathsf{t}.~\mathsf{t} : \type{A}$ and $\flatten{\mathsf{t}} = s$
\end{lemma}
Lemma (\ref{lem:types}) allows us to prove soundness, by defining a terminating interpretation function from coercion derivations $\contains{}{c}{E}{F}$ to coercions in the function space $\type{E} \rightarrow \type{F}$.
% \begin{lemma}
% The existence of an interpretation function $\interp{ \contains{}{c}{E}{F}} : \{f : \type{E} \rightarrow \type{F}~ |~ \pres{f} \}$ implies soundness of the coercion system
% \end{lemma}
We now focus on defining an interpreter and get soundness for free. In Figure (\label{fig:interp}) the equtional rules of the interpreter are given. The last for equalities capture recursion, the first being Grabmeyer recursion with a termination argument known as the $0$-measure $|v|_0$ on parse trees. The last three equalities define the semantics of $\dslcom{peek}$ as a recursive function and it has a termination measure known as $1$-measure $|v|_1$ 
\begin{definition}[Measures on parse trees]
We define the two measures $|\cdot|_0$ and $|\cdot|_1$. For the cases where the two measures coinductive, we simply write $|\cdot|$.
  \begin{displaymath}
    \begin{array}{lll}
\onem{1} = 1 \qquad \zerom{1} = 0
\\\\
 \measure{0} = 0 \qquad \measure{a} = 1 \qquad \measure{inl~ t} = \measure{t} \qquad \measure{inr~ t} = \measure{t} \qquad  \measure{(v,w)}=\measure{v}+\measure{w} \qquad  \measure{fold~v}=\measure{v}\qquad

 %     \measure{(v,w)}=\measure{v}+\measure{w} \\\\ 

 %    \onem{0} = 0 \qquad \onem{1} = 1 \qquad \onem{a} = 1 \qquad \onem{inl~ t} = \onem{t} \qquad \onem{inr~ t} = \onem{t} \qquad
 %     \onem{(v,w)}=\onem{v}+\onem{w} \\\\
 % \onem{fold~v}=\onem{v}
    \end{array}
  \end{displaymath}
\end{definition}
We can verify that the course-of-values for $\dslcom{peek c}$ respects the $\zerom{\cdot}$ measure because in the recursive case \[\irule{(peek c)}{fold (inr (v,x))}{(peek c)(x)}\quad \text{if }\irule{c}{v}{inl ()}\]
We have that 
\[ \zerom{x} < \zerom{fold~ (inr~ v,x)}\]
We can also verify that Grabmeyer recursion satisfies the $\onem{\cdot}$ measure because none of the rules increases the $\onem{\cdot}$ measure, and because of the discharge rule:
\[\infer{\contains{\Gamma, f : A \leq B, \Gamma'}{id \times f}{a \times A}{a \times B}}{}\]
This means that if the application $\dslcom{fix f.c}(t)$ recurses, then reduction will lead to a term $\dslcom{f}(a,t')$ where $\onem{t'} < \onem{(a,t')}\leq \onem{t}$\\
\subsection{Interpretation}
We define the interpretation function $\llbracket \cdot \rrbracket_{\sigma_{\Gamma}}$ that translates a coercion derivation $\containsG{c}{A}{B}$ to a function on parse trees $\interp{\dslcom{c}}_{\sigma_{\Gamma}} : \mathcal{T}\llbracket A \rrbracket  \rightarrow \mathcal{T}\llbracket B \rrbracket$. The subscript, $\sigma_{\Gamma} : \{ \dslcom{f} ~|~\dslcom{f} : A \leq B  \in \Gamma \ \} \rightharpoonup  (\type{A} \rightarrow \type{B})$, is a partial map from assumptions $\dslcom{f} : A \leq B  \in \Gamma$, to functions on parse trees $\type{A} \rightarrow \type{B}$. 

The interpretation rules for all context rules as well as axioms, such as $\dslcom{shuffle}$, are exactly those presented in Henglein and Nielsen. As an example, a rule for shuffle is 
\[\interpsig{shuffle}{inl~v}{inl~(inl~v)} \]
To save space we omit interpretation rules for context rules and axioms, refering to Definition 16. in Henglein and Nielsen. Unlike Henglein and Nielsen who define interpretation of fix as $\interp{\fix ~ \dslcom{f.c}}(\mathsf{v})=\interp{\dslcom{c}[\fix~\dslcom{f.c} \backslash \dslcom{f}]}(\mathsf{v})$, we define the interpretation function with an environment $\interp{\cdot}_{\sigma_{\Gamma}}$ because this allows a precise definition that close to the Coq implementation.

In the fix rule, we write $\mathbf{f} \mapsto f_{\mathsf{v}} \cdot \sigma_{\Gamma}$ for the partial function extension, defined when $\mathbf{f} \notin \mathsf{Dom}(f)$. We label $f$ with subscript $\mathsf{v}$ for termination. This is seen in the rule where the application $\sigma_{\Gamma}~\mathsf{f}(\mathsf{v'})$ is allowed only when the string size has decreased since the declaration of the fixpoint.
\begin{definition}[Interpretation]
Let $\interp{\dslcom{c}}_{\sigma_{\Gamma}} : \type{A} \rightarrow \type{B}$ be the interpretation of $\containsG{c}{A}{B}$. We define it as:   
%For $\dslcom{f} \notin \mathsf{Dom}(\sigma_{\Gamma})$ and $f : \type{A} \rightarrow \type{B}$ we write the extension of $\sigma_{\Gamma}$ with $\dslcom{f}$ as $ \dslcom{f} \cdot \sigma_{\Gamma}$.
%We now give the definition of $\llbracket \containsG{c}{A}{B} \rrbracket : \mathcal{T}\llbracket A \rrbracket  \rightarrow \mathcal{T}\llbracket B \rrbracket$, assuming $\sigma_{\Gamma}$.\\
  \begin{displaymath}
    \begin{array}{l}
% \irule{shuffle}{inl v}{inl (inl v)}\\
% \irule{shuffle}{inr v}{inl (inr v)}\\
% \irule{shuffle}{(inr (inr v))}{inr v}\\
% \iruleinv{shuffle}{(inl (inl v))}{inl v}\\
% \iruleinv{shuffle}{(inl (inr v))}{inr (inl v)}\\
% \iruleinv{shuffle}{(inr v)}{inr (inr v)}\\
% \irule{retag}{(inl v)}{inr v}\\
% \irule{retag}{(inr v)}{inl v}\\
% \iruleinv{retag}{}{retag}\\
% \irule{untagL}{(inr v)}{v}\\
% \irule{untag}{(inl v)}{v}\\
% \irule{untag}{(inr v)}{v}\\
% \irule{tagL}{(v)}{inl v}\\ 
% \irule{assoc}{(v,(w, x))}{((v, w), x)}\\
% \iruleinv{assoc}{((v, w), x)}{(v,(w, x))}\\
% \irule{swap}{(v,())}{((), v)}\\
% \iruleinv{swap}{((), v)}{(v,())}\\
% \irule{proj}{((), w)}{w}\\
% \iruleinv{proj}{(w)}{((), w)}\\
% \irule{distL}{(v, inl w)}{inl (v, w)}\\
% \irule{distL}{(v, inr x)}{inr (v, x)}\\
% \iruleinv{distL}{(inl (v, w))}{(v, inl w)}\\
% \iruleinv{distL}{(inr (v, x))}{(v, inr x)}\\
% \irule{distR}{(inl v, w)}{inl (v, w)}\\
% \irule{distR}{(inr v, x)}{inr (v, x)}\\
% \iruleinv{distR}{(inl (v, w))}{(inl v, w)}\\
% \iruleinv{distR}{(inr (v, x))}{(inr v, x)}\\
% \irule{wrap}{(v)}{fold v}\\
% \dslcominv{wrap}(\textsf{(v)})=\dslcominv{fold}\\
% \irule{id}{v}{v}\\
% \iruleinv{id}{}{id}\\
% \irule{(c; d)}{(v)}{d(c(v))}\\
% \irule{(c + d)}{(inl v)}{inl (c(v))}\\
% \irule{(c + d)}{(inr w)}{inr (d(w))}\\
% \irule{(c  d)}{(v, w)}{(c(v), d(w))}\\
% \irule{shuffle}{inl v}{inl (inl v)}\\
% \irule{shuffle}{inr v}{inl (inr v)}\\
% \irule{shuffle}{(inr (inr v))}{inr v}\\
% \iruleinv{shuffle}{(inl (inl v))}{inl v}\\
% \iruleinv{shuffle}{(inl (inr v))}{inr (inl v)}\\
% \iruleinv{shuffle}{(inr v)}{inr (inr v)}\\
\infer{\interp{\fix~\dslcom{f}.\dslcom{c}}_{\sigma_{\Gamma}}(v)=\rec{f}{x}{\interp{\dslcom{c}}(x)_{\mathsf{var~f} \mapsto f \cdot \sigma_{\Gamma}}}{f}(v)}{} \qquad
\infer{\interp{\mathsf{f}}_{\sigma_{\Gamma}}(a,v)=(a,(\sigma_{\Gamma}~\mathsf{f})(v))}{f \in \mathbf{Dom}(\sigma_{\Gamma})}
\\\\
\infer{\interpsig{peek c}{fold (inl ())}{inl ()}}{} \qquad
\infer{\interpsig{peek c}{fold (inr (v,x))}{(\dslcom{peek c})(x)}}{\interpsig{c}{v}{inl ()}}
\\\\
\infer{\interpsig{peek c}{fold (inr (v,x))}{inr (w,x)}}{\interpsig{c}{v}{inr~w}}
    \end{array}
  \end{displaymath}
\end{definition}
\begin{lemma}[Termination]
For any coercion derivation $\contains{}{c}{A}{B}$ and $\mathsf{t} : \type{A}$,\\ 
$\interp{c}_\emptyset(\mathsf{t})$ is defined  %and preserves the string of $\mathsf{t}$
\end{lemma}
\begin{proof}[Sketch]
Show $\containsG{c}{A}{B}$ implies $\interp{c}_{\sigma_{\Gamma}}(\mathsf{t})$ is defined, assuming that forall $\mathsf{f} : C \leq D \in \Gamma$ and $\mathsf{v} : \type{C}$, where $|\mathsf{v}|_0 < |\mathsf{t}|_0$, that $(\sigma_{\Gamma}~\mathsf{f})(\mathsf{v})$ is defined.\\
Proof by induction on $\containsG{c}{A}{B}$. By the IH it suffices to show $f_{\mathsf{t}}(\mathsf{v})$ is defined, assuming $|\mathsf{v}|_0 < |\mathsf{t}|_0$. One can show $f_{\mathsf{t}}(\mathsf{v})$ is defined with $|\cdot|_0$ as termination measure.\\
The variable rule and non-recursive rule of $\dslcom{peek}$ are immediate. The latter two rules of $\dslcom{peek}$ can be shown terminating using $|\cdot|_1$ as termination measure.
\end{proof}
\begin{lemma}[Soundness of interpretation]
 For any coercion derivation $\contains{}{c}{A}{B}$\\ 
$\interp{c}_\emptyset :\type{A} \rightarrow \type{B}$ is a coercion
\end{lemma}
\begin{corollary}[Soundness]
$\contains{}{c}{A}{B}$ implies $A \subseteq B$
\end{corollary}
%  \caption{Interpretation semantics:  $\Gamma \vdash \dslcom{c}(v)=v'$: We present only the new rules for $\dslcom{peek}$ and changed rule for ($\fix f$). All other rules are identical to the presentation by Henglein and Nielsen and to see these definitions we refer to their work.}
\subsection{Representation}
In Coq we represent a derivation of $\containsG{c}{A}{B}$ as the term  \textsf{c : dsl Gamma A B}.
\mycomment{change drop to peek in code}
\begin{minted}{Coq}
Inductive dsl (Gamma: seq (regex * regex)) : regex -> regex -> Type := 
| shuffle A B C : dsl Gamma ((A _+_ B) _+_ C) (A _+_ (B _+_ C))
| dfix A B : dsl ((A,B):: Gamma) A B -> dsl Gamma A B.
| var a A B :   (A,B) \in Gamma -> dsl Gamma (Event a _;_ A) (Event a _;_  B) 
...
\end{minted}
One builds a variable using two regular expressions \textsf{A} and \textsf{B} and a proof that their pair is in the environment. 
\begin{minted}{Coq}
Lemma pf_in : (A,B) \in [(A,B)]. Proof. ... Qed.
Definition var_example : dsl [(A,B)] (a * A) (a * B)  := var a A B pf_in
\end{minted}
Applying \textsf{dfix} extends the environment. In the derivation below, we \textsf{dfix} extends the empty environment \textsf{nil} with \textsf{((1 + Star a),(Star a))} and \textsf{var a (1 + Star a) (Star a) pf\_in2} refers to this entry.
\begin{minted}{Coq}
Lemma pf_in2 : ((1 + Star a),(Star a)) \in [(1 + Star a) (Star a)].
Definition example : dsl nil (1 + Star a) (Star a) := 
dfix (ctrans (peek cid)
     (ctrans (cplus cid (var a (1+(Star a)) (Star a) pf_in2)) wrap))
\end{minted}

% The type of the interpretation function is seen below
% \begin{minted}{Coq}
% Definition post {A : eqType} (r0 r1 : @regex A) (T : pTree r0) := 
%   { T' : pTree r1 | pTree_0size T' <= pTree_0size  T /\ pflatten T = pflatten T' }. 


% Fixpoint interp l r0 r1 (p : dsl l r0 r1) (T : pTree r0) 
%          (Gamma_f : forall x y,  (x,y) \in l -> 
%                     forall (T0 : pTree x), pRel0 T0 T -> post y T0) {struct p}:
%          post r1 T. 
% \end{minted}
% \mycomment{change semantic rules to include environment}
% \textsf{Gammaf} is used to interpret the variables, and its type ensures that it can only be used on parse trees of smaller $0$-size..

\newcommand{\dentry}[2]{\begin{tabular}{l} #1 \\ #2 \end{tabular}}
\newcommand{\tabline}[6]{#1 & #2 & #3 & #4 & #5 & #6}
%\newcommand{\tabline}[11]{#1 & \dentry{#2}{#7} & \dentry{#3}{#8} & \dentry{#4}{#9} & \dentry{#5}{#10} & \dentry{#6}{#11}}
\section{Benchmarking}
The benchmarks have been run on a machine with a 2,6 GHz processor with six cores and 16GB of memory. Results can be seen in Figure (\ref{fig:bench}). The figure lists 8 equivalences and one containment for regular expressions with $\Sigma= \{a,b\}$. For each of the equivalences $A = B$ we synthesize coercions $\contains{}{c}{A}{B}$ and $\contains{}{\dslcominv{c}}{B}{A}$. In total we generate 17 coercions where we record the following:
\begin{itemize}
\item Synthesis time (in seconds)
\item Size of the synthesised program as the number of constructors 
\item Interpretation time for parse trees with string sizes of respectively $50$,$500$ and $5000$.
\end{itemize}
We see that synthesis is fast, completing in under 10 mili-seconds for all containments. The fastest synthesis, $(1 + a)^* \leq a^*$, being about one order of magnitude faster than the slowest, $(a + b)^* \leq a^* + (b^* \times a^*)^*$. As expected, synthesis time correlates with program size yielding a program of size $1032$ for the fastest synthesis and $7281$ for the slowest. For all containments, we observe a growth in interpretation time with respect to the string size of the input parse tree.
\begin{figure}
  \centering
  \begin{tabular}{l | l | l | l | l | l | l}
Containment & Synthesis & Program size & $|s|=50$ &  $|s|=500$ &  $|s|=5000$ \\
\hline
\tabline
{$a ^* = a^* \times  (a^*)^*$}
{\dentry{.001537}{.001130}}
{\dentry{1428}{1534}}
{\dentry{.000919}{.000760}}
{\dentry{.007399}{.005309}}
{\dentry{.082023}{.089442}}\\
\hline
\tabline
{$(a^*)^* = a^*$}
{\dentry{.000679}{.000995}}
{\dentry{1234}{1342}}
{\dentry{.000757}{.000847}}
{\dentry{.008137}{.005861}}
{\dentry{.077686}{.078434}}\\
\hline
\tabline
{$(1 + a)^* = a^*$}
{\dentry{.000509}{.001133}}
{\dentry{1032}{1254}}
{\dentry{.001710}{.000782}}
{\dentry{.005517}{.005085}}
{\dentry{.070280}{.073624}}\\ 
\hline
\tabline
{$(a+b)^* = a^* + (b^* \times a^*)^*$}
{\dentry{.007009}{.003012}}
{\dentry{7281}{2793}}
{\dentry{.000643}{.000956}}
{\dentry{.004882}{.005989}}
{\dentry{.073752}{.079130}}\\
\hline
\tabline
{$ a^* \times (1 + a) = a^*$}
{\dentry{.000988}{.001238}}
{\dentry{1640}{1626}}
{\dentry{.000541}{.000588}}
{\dentry{.005414}{.005429}}
{\dentry{.076216}{.079477}}\\
\hline
\tabline
{$(a + b)^* = (a^* + b^*)^*$}
{\dentry{.004171}{.004643}}
{\dentry{5169}{4226}}
{\dentry{.000826}{.000923}}
{\dentry{.006507}{.005989}}
{\dentry{.087849}{.092896}}\\
\hline
\tabline
{$(a^* + b^*)^* = (a^* \times b^*)^*$}
{\dentry{.005818}{.006281}}
{\dentry{5990}{5913}}
{\dentry{.000956}{.000898}}
{\dentry{.007027}{.009935}}
{\dentry{.098359}{.095056}}\\
\hline
\tabline
{$a^* \times b^* < ((1 + a) \times (1 + b))^*$}
{~~.005293}
{~~7304}
{~~.001099}
{~~.007267}
{~~.101217}\\
\hline
\end{tabular}
  \caption{Benchmark results for containments with $\Sigma=\{a,b\}$}
  \label{fig:bench}
\end{figure}
\mycomment{Check rules are correct}
\mycomment{mention fold/unfold isorecursion 1.3 notation and terminology}
\mycomment{Mention the final containment is slow }
\section{discussion and related work}
Computational interpretations of linear logic \cite{A93}\\
Regular expression containment as a proof search problem Vladimir Komendantsky[Misc url].\\
Also mention Marco and Carstens result-

Greedy parse trees and preservatino of greediness by coercions. Proof relevance computational properties.\\
white board and mechanisation leads to interesting results.
\\
Though the set of derivatives quotiented by ACI is finite for any regular expression, in a proof assistant it can be easier to formulate such finiteness arguments constructively as the existence of an inductive list that contains all the derivatives. This can be done by considering the partial derivatives $\partial_a$ of regular expressions. Convenient finitness arguments have been well studied by others in the litterature to show termination for regular expression equivalence decision procedures. Doing this through partial derivatives was introduced by Almeida et al. \cite{AMR09} and in \cite{MPS12}, they mechanize in Coq the \textsf{equivP} procedure of Almeida et al. \mycomment{How does their termination argument work?}. There is also \cite{A12} who take a different approach, turning regular expression into point-automata, making it straigtforward to compute the enumeration of reachable derivatives. We combine their approaches, which we show in a moment
\mycomment{Two files for this, extensional and extensionalpartial which one is correct I think extensional partial, clean up code later}\\
\begin{itemize}
\item Proof search approch
\item Inductive dsl with environment as list. Does expensive decomposition
\item Inductive dsl with fast decomposition
\item Coinductive dsl with environment as function (that is build in a list-like way) (expensive decompositoin)
\item Missing Coinductive dsl fast
\item With or without implicit variables
\end{itemize}
\bibliography{ref}  

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
