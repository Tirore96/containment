
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}
%This is a template for producing LIPIcs articles. 
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Computationally interpreting regular expression containment proofs to effecient procedures } %TODO Please add

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Jane {Open Access}}{Dummy University Computing Laboratory, [optional: Address], Country \and My second affiliation, Country \and \url{http://www.myhomepage.edu} }{johnqpublic@dummyuni.org}{https://orcid.org/0000-0002-1825-0097}{(Optional) author-specific funding acknowledgements}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional. Use additional curly braces to indicate the correct name splitting when the last name consists of multiple name parts.

\author{Joan R. Public\footnote{Optional footnote, e.g. to mark corresponding author}}{Department of Informatics, Dummy College, [optional: Address], Country}{joanrpublic@dummycollege.org}{[orcid]}{[funding]}

\authorrunning{J. Open Access and J.\,R. Public} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Jane Open Access and Joan R. Public} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  +http://creativecommons.org/licenses/by/3.0/

\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Dummy keyword} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{proof}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{stmaryrd}

%\newcommand\mycomment[1]{\textcolor{red}{#1}}
\newcommand\mycomment[1]{}

\begin{document}
\include{macros}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
% Regular expressions are ubiqutious. They are theoretically relevant to automata theory for being syntactic representations of finite state automata and essential in practice for text processing. Theory and practice share an extensional view of regular expressions, caring more about the binary property of whether a string matches an expression, and less about the more informative property of how it matches the expression as a parse tree. Regular expressions can be viewed as types that have parse trees as their terms and Henglein and Nielsen demonstrate how these parse trees can represent compressed text. In most typed programming languages one expects type checking to work up to subtyping. Incorporating \textit{regular expressions as types} into such a language requires coercive subtyping to preserve that the only terms of regular expression types are their parse trees. Henglein and Nielsen show how such coercions can be derived by a computational interpretation of intuitionistic proofs of regular expression containments. They prove their system sound and complete with respect to language containment. We extend on their work by proving that effecient coercions can be synthesized effeciently and mechanise this result, along with soundness and completeness, in the Coq proof assistant. As part of the solution to synthesis, we also introduce a technique for mechanising decidability of regular expression containment and equivalence in under 900 loc. Finally we extract the synthesis procedure and benchmark the performance of the coercions.
% \\\\
 Regular expressions are typically studied in automata and formal language theory as a formalism for denoting the class of regular languages, where the only observable property is whether a string belongs to the denoted set of strings or not, and any two regular expressions are observably equivalent if they satisfy the same membership tests.  In programming practice, however, regular expressions are typically used to extract more detailed information: not only whether but also how a string matches a regular expression, ranging from extracting a particular substring to constructing a full parse tree.  Many automata theory techniques are not applicable in this setting; for example, a deterministic finite automaton constructed from a regular expression provides no way of constructing a parse tree for the given regular expression.  

It is known that proofs of membership of strings in (the language denoted by) a regular expression are in one-to-one correspondence to parse trees; and that a formal proof of a regular expression containment $A \leq B$ in an axiomatization of regular expression containment can be operationally interpreted as a coercion, a function that maps any proof of containment of a string in $A$ into a proof of containment of the same string in $B$.  Different containment proofs may yield extensionally different functions, however, since regular expressions, considered as grammars, can be ambiguous. Additionally, even if extensionally the same, one proof of containment may yield a slow implementation, another an efficient one.
%This raises the questions: How do we synthesize not only any proof of containment, but a provably efficient one, if one exists at all?  How can we perform this synthesis itself efficiently?  More fundamentally, how do we even know that the containment axiomatization at hand contains not only inefficient coercions?

In this paper we prove that efficient coercions can be synthesized efficiently and mechanize this result in Coq, along with soundness (only terminating coercions are inferred) and completeness (for each valid containment at least one  coercion is derivable). As part of the solution to synthesis, we present a technique for mechanizing the decidability of regular expression containment and equivalence in under 900 lines of code, which to the best of our knowledge is the shortest mechanised proof in the litterature.
Finally we extract our efficient coercion synthesis procedure and benchmark the performance of the synthesized coercions.  %More speculatively, we propose that the interaction between domain-specific language design, functional programming, and computational complexity analysis on the one hand and mechanizated axiomatization on the other hand may provide a productive methodology for efficient domain-specific software design and synthesis--even in the case where mechanized formal correctness is not the key objective.

\end{abstract}

% \section{Introduction}
% Regular expressions is inductive syntax that represents finite state automaton. Regular expressions equivalence and inequalities are well understood and studied. Often these proof systems are considered in a proof-irrelevant way, not caring what the inference tree is, only that it exists. Brandt and Henglein [cite] show how regular expression containment proofs can be interpreted to functional programs on parse trees. Such interpretation has applications in string compression, which we shall explain more later. The rules of their containment axiomatization corresponds to an intrinscially typed domain specific language. They derive the constructs of this language by combining the rules of idempotent semirings, unfolding $A`* \leq 1 +  A \times A^*$, with a powerful fix rule that is a of coinductive nature. 

% Our goal is effecient coercions on parse trees with applications for string compression. 
% Existing work by Brandt and Henglein formalize this using a powerful fix rule, whose soundness they ensure by instantiating it with a proper side condition. This side condition makes the coercion proofs less compositional and thus harder (as well as more expensive) to synthesize. This side condition checks for contractiveness. We derive a similar coercion system with a much weaker (but compositional) fixpoint rule that both simplifies (and makes more effecient) the synthesis of coercions, and implies contractiveness. The system is inspired by parameterized coinduction and allows us simple intepretation (our definition is good for simple termination arguments) and synthesis. These simple approaches would not have worked in Brandt and Henglein's origina system [detail why].\\\\
% We derive this inductive coercion system in the following steps: 
% \begin{enumerate}
% \item The coercion system is an axiomatization of language containment, which can be obtained by adding a few rules to axiomatizations of language equivalence. Starting with Grabmeyer's coinductive characterization, we prove it equivalent to another coinductive characterization that is free of operational notions. 
% \item We then prove this equivalent to an inductive version of this (why is it important to go inductive? Depends on the performance between the two dsls). 
% \item The inductive system for language equivalence is the altered to create one for language containment, and we go from Prop to Type.
% \end{enumerate}
\section{Preliminaries}
\mycomment{double check it is 900 lines it takes to prove this}
Cardelli and Fresch introduce regular expressions as types.
\begin{definition}[Regular expression and semantics]
Regular expressions over a finite base set $a$ are given by the syntax:\\
$A ::= A + B~| A \times B ~|~A ^* ~|~ 1 ~|~ 0 ~|~ a$\\
Matching is defined as:\\
\begin{displaymath} 
\infer{\match{\epsilon}{1}}{}\qquad
\infer{\match{s s'}{A \times B}}{\match{s}{A} & \match{s'}{B}} \qquad
\infer{\match{s} {A + B}}{\match{s}{A}} \qquad
\infer{\match{s} {A + B}}{\match{s}{B}} \qquad
\infer{\match{\epsilon}{A^*}}{}\qquad
\infer{\match{s s'}{A^*}}{\match{s}{A} & \match{s'}{A^*}}\qquad
\end{displaymath}
Intrinsically typed parse trees:
\begin{displaymath}
\begin{array}{l}
\infer{\oft{()}{1}}{} \qquad 
\infer{\oft{a}{a}}{} \qquad \infer{\oft{\pair{t}{t'}}{A \times B}}{\oft{t}{A} & \oft{t'}{B}}
\qquad \infer{\oft{\inl {t}}{A + B}}{\oft{t}{A}} \qquad
\infer{\oft{\inr {t}}{A + B}}{\oft{t}{B}}  \qquad
\infer{\oft{\fold{t}{A^*}}}{\oft{t}{1 + A \times A^*}}
\end{array}
\end{displaymath}
\end{definition}
\begin{definition}[Equivalence] \noindent \\
Language equivalence: $\forall s, \match s\in A \iff s \in B$\\
Language containment: $\forall s, s \in A \implies s \in B$
\end{definition}
\begin{definition}[Derivative (standard/partial) and nullariness]
\begin{displaymath}
\begin{array}{l}
\mynu{a} = \false \qquad
\mynu{\epsilon} = \true \qquad
\mynu{0} = \false
\\\\
\mynu{A + B} = \mynu A  \lor \mynu B \qquad
\mynu{A \times B} = \mynu A  \land \mynu B
\end{array}
\end{displaymath}
\begin{displaymath}
\begin{array}{l}
\derive {a}{1} = \qquad
\derive{a}{0} = 0 \qquad
\derive{a}{b} = \epsilon if a = b\qquad
\derive{a}{b} = 0 if a \neq b\qquad
\derive {a}{(A + B)} = \derive{a}{A} +  \derive{a}{B}\qquad
\\\\
\derive {a}{(A \times B)} = \derive{a}{A} \times B +  \derive{a}{B} ~if \mynu{A} \qquad
\derive {a}{(A \times B)} = \derive{a}{A} \times B ~if not \nu{A} \qquad
\derive {a}{(A^*)} = \derive{a}{A} \times A^*
\end{array}
\end{displaymath}
\end{definition}j
\begin{definition}[Parameterized coinduction]
  fill out....
\end{definition}
\section{Axiomatizations:  Prior ones and a new one}
The first axiomatization of equivalence was given by Salomaa [cite]. We shall do as Brandt and Henglein, refering jointly to the laws of semiring, equality and the unfold rule $A^* = 1 + A \times A^*$ as \textit{weak equivalence}. Salomaa provec soundness and completeness of the system $F_1$ which consists of the rules of weak equivalence with the rules in Figure (\ref{fig:salomaa}).
\begin{definition}[Laws of idempotent semi-ring]
\label{definition:ring}
\begin{displaymath}
\begin{array}{lll}
\myaxiom{A + B + C}{A + (B + C)} \qquad  
\myaxiom{A + B}{B + A} \qquad 
\myaxiom{A + 0}{A} \qquad
\myaxiom{A + A}{A} \qquad
\\\\
\myaxiom{A \times B \times C}{A \times (B \times C)} \qquad
\myaxiom{1 \times A}{A} \qquad
\myaxiom{A \times 1}{A} \qquad 
\myaxiom{0 \times A}{0} \qquad
\\\\
\myaxiom{A \times 0}{0} \qquad
\myaxiom{A \times (B + C)} {A \times B + A \times C} \qquad
\myaxiom{(A + B)\times C}{A \times C + B \times C}
\end{array}
\end{displaymath}
\end{definition}

\begin{definition}[Laws of equality]
\label{definition:equality}
\begin{displaymath}
\begin{array}{lll}
\infer{A = A}{}\qquad
\infer{A = B}{B = A}\qquad
\infer{A = C}{A = B & B = C}
\end{array}
\end{displaymath}
\end{definition}
\begin{figure}
\caption{Rules of Salomaa}
\label{fig:salomaa}
\begin{displaymath}
\infer[A_{11}]{\eqBodyE{A^*}{B^*}}
  {\eqBodyE{A}{B}} \qquad
\myaxiomN{(1 + A)^*}{A^*} \qquad \infer[\mynu{F}=\false]{\eqBodyE{E}{F^* \times G}}{\eqBodyE{E}{F \times E + G}}
\end{displaymath}
\end{figure}
Salomaa proved that from the rules of weak equivalence with the unfold-rule and $A_{11}$, one can derive the decmosition:
\begin{lemma} \label{lem:salomaa}
\[A = \mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}\]
\end{lemma}
We will return to this decomposition again later.
\subsection{Why axiomatizations matter}
\mycomment{Mention regular expressions as types}
Regular expressions denote an important class of automata deserving of its study on equivalences and containments. From an automata theoretic view, axiomatizations improve our understanding on the properties of finite state automaton. The shape of the proof within an axiomatization is of no importance, one only cares about soundness and completeness. Brand and Henglein showed why it is worth to consider not only \textit{what} an axiomatization lets you prove, but also \textit{how} the proof rule lets build your derivation. To see the benefit of taking a proof-relevant approach to regular expression axiomatizations we start by presenting the problem we investigate in this paper.
[Example of simple coercion]
[Explain why parse trees is equal to proof-relevance]
[Hint at comleteteness here]
[Containment is more essential than equivalence because it is more than bijection] 
\begin{center}
\textit{Given regular expressions $A$ and $B$ and language inclusion $A \subseteq  B$, how does one define a procedure that outputs an effecient function mapping parse trees of $A$ to parse trees of $B$}.\\
\end{center}
Such a function, $f : A \rightarrow B$, must exist, otherwise the language inclusion would not hold. This is especially the case in the dependent type theory of Coq which follows Curry-Howard proof as programs philosophy. Let us consider a definition of language containment in Coq:
\begin{minted}{Coq}
Definition Contains A B := forall s, Match s A -> Match s B
\end{minted}
\textsf{Contains} is a map on \textsf{Match} derivations, which are essentially parse trees. 
They are defined as 
\begin{minted}{Coq}
Inductive Match : trace -> regex -> Prop :=
  | MEps : Match [::]  Eps
  | MEvent x : Match [::x] (Event x)
  | MSeq s1 c1 s2 c2 : Match s1 c1 ->  Match s2 c2 -> Match (s1 ++ s2) (c1 _;_ c2)
  | MPlusL s1 c1 c2:  Match s1 c1 -> Match s1 (c1 _+_ c2)
  | MPlusR c1 s2 c2:  Match s2 c2 ->  Match s2 (c1 _+_ c2)
  | MStar0 c  : Match [::] (Star c)
  | MStarSeq c s1 s2:  Match s1 c -> Match s2 (Star c) -> Match (s1 ++ s2) (Star c).
\end{minted}
The definition of parse trees is
\begin{minted}{Coq}
Inductive pTree : @regex A -> Type := 
| p_tt : pTree Eps 
| p_singl a : pTree (Event a)
| p_inl r0 r1 : pTree r0 -> pTree (r0 _+_ r1) 
| p_inr r0 r1 : pTree r1 -> pTree (r0 _+_ r1) 
| p_pair r0 r1 : pTree r0 -> pTree r1 -> pTree (r0 _;_ r1)
| p_fold r : pTree (Eps _+_ (r _;_ (Star r))) -> pTree (Star r).
\end{minted}
One crucial difference between the two is \textsf{Match} derivations live in \prop, making the shape of the derivation inaccesible for case distinction during computatioin in \myset \footnote{To be precise pTree lives in Type because it is parameterized by A which lives in Type. This is due to parameterizing the entire development over the ssreflect finite type which lives in Type }. Though we can not use \textsf{Contains} directly in constructing $f : A \rightarrow B$, knowing that the containment \textsf{Contains A B} holds, can be used as a termination argument for proof search. A mapping $f : A \rightarrow B$ can be obtained by lifting a parse tree \textsf{t} of regular expression $A$, \textsf{t : pTree A}, to a Match derivation \textsf{Match (flatten t) A}, where \textsf{flatten} returns the underlying string of the parse tree. Translating to a match derivation takes linear time in the size of the parse tree. From here we can apply the \textsf{Contains A B} assumption as a function returning \textsf{Match (flatten t) B}, from which me must construct a \textsf{t' : pTree B}, preserving the underlying string, that is,\textsf{flatten t' = flatten t}. Building \textsf{t' : pTree B} is where need proof search. With \textsf{Match (flatten t) B} living in Prop, we may not case distinct on the derivation to learn the shape \textsf{t'} should take. But the presence of \textsf{Match (flatten t) B} means that there exists a natural number $n$, such that the set of parse trees of at most $n$ constructor applications, will contain our desired parse tree. We show this in file \textsf{Constructive.v}, following the technique of the Constructive Epsilon Coq libarary [cite].\\
Now with a procedure in hand to transform a language containment \textsf{Contains A B} into a map on parse trees \textsf{f : pTree A -> pTree B} all we need is a decision procedure for language containment, which there exists several of in the litteratue[cite]. We now have a way to synthesize mapping on parse trees (which we from now on will call coercions). This is of course too slow to be of any real use and only serves to show why we might wish to limit the function space of coercions to include only linear time programs that process the underlying string of the parse tree in a streaming fashion rather than brute forcing the problem. How should the function space be constrained? The coercion is not allowed to alter the underlying string (soundness). What ever constructs we do decide on must let us build any mapping that can be proved with \textsf{Contains} (completeness). As we shall see in a moment, Brandt and Henglein found that axiomatizing regular expression containment, is the same as defining the primitives and combinators of a set of rules for building coercions.
\subsection{Grabmeyer}
It is known that language equivalence and containment of reuglar expressions can be expressed coinductively as simulation (rspt. bisimulation) using the brozowski derivative $\derive{a}{A}$ to transition between regular expressions and nullariness operator $\mynu{\cdot}$ for finality. With this extensional view,language containment can be stated coinductively as:
\begin{definition}[Simulation]
\[\infer={A \sim B}{\mynu{A} \implies \mynu{B} & \forall a \in \Sigma, \derive{a}{A} \sim \derive{a}{B} }\]
\end{definition}
\begin{lemma}
Simulation coincides with language contaiment
\end{lemma}
Simulation is a coinductive definition, and has the following coinduction principle (taking $F$ as the generating function for $\sim$):\\
\[ (A,B) \in R \land R \subseteq F(R) \implies A \sim B  \]
That is, to show a simulation it suffices to find a (possibly infinitary) relation $R$ that is a post fixedpoint of $F$. This proof principle can be awkard to use because a post fixed point must be guessed up front. Hur et al. [cite] introduce parameterized coinduction that allows $R$ to be constructed incrementally throughout the proof. They introduce the parameterized greatest fixpoint
\[ G_f(R) \triangleq gfp(\lambda X. F(X \cup R))
\]
Where $G_f(\{\}) = gfp(F)$. The following rule allows the accumulation into $R$
\[ Y \subseteq G_f(X) \iff Y \subseteq G_f(X \cup Y)
\]
Relating this back to simulation, is showing an inclusion:
\[A \sim B = \{(A,B)\} \subseteq G_f(\{\})  \]
If we were to prove that $A^*$ is in simulation with $\dstar{A}$, for any $A$, the accumulation rule would add to $R$ the following infinitary relation
\[ X = \{ A^*  \sim  \dstar{A} | A \in \mathcal{Regex} \} \]
\[  X \subseteq  G_f(\{\})  \iff X \subseteq  G_f(\{X\})  \]
We will call this infinitary coinduction.\\
If on the other hand we were to prove this statement with $A$ fixed to $\event{a}$ for some specic event $a$, we would accumulate $R$ by a single pair:
\[  \{(\dstar{\event{a}},\event{a}^*)\} \subseteq  G_f(\{\})  \iff X \subseteq  G_f(\{(\dstar{\event{a}},\event{a}^*))  \]
We will call this finitary coinduction. Brandt and Henglein (1997) showed that some times finitary coinduction can be captured by natural deduction systems. For example, they showed that coinductively defined equivalence of $\mu$-types
can be captured by a natural deduction judgment $\Gamma \vdash \tau = \tau' $ where $\Gamma$ corresponds to the $R$ that accumulates during parameterized coinduction. As natural deductions are inductive finite derivations, only finitary coinduction can be modeled, which works for $\mu$-types because they encode regular trees which have finitely many distinct sub-trees, putting an upperbound on the reachable pairs one may see during a derivation. A similar result holds for regular expressions.\\
 Brozowski showed that modulus rules of associativity, commutativity and transitivity, a regular expression has a finite set of unique derivatives. Grabmeyer showed that the method of Brandt and Henglein could be used in this setting as well. He showed that modulus some ACI rewriting\footnote{To minimize the size of the generated relations he allowed more rules than ACI}, regular expression equivalence can be charaterized by a natural deduction system that emulates finitary coinduction via the COMPFix rule. One can see finitary coinduction in action by the accumulation of $(A,B)$ into the context of the second premise.
\begin{definition}[Grabmeyer's COMPFix]
\[\infer{\Gamma \vdash A =  B}{\mynu{A} =  \mynu{B} & \Gamma, A = B \vdash \derive{a}{A} = \derive{a}{B},~\forall a \in \Sigma}\] 
\end{definition}
%Characterising (in)-equalties of regular expressions by a set of compositional rules is what we mean by an axiomatization. Many such axiomatizations exist. Brandt and Henglein presents an axiomatization of containment: $\Gamma \vdash A \leq B$, which is related to equivalence since $A = B \iff A \subseteq B \land B \subseteq A$. They take a proof-relevant view of containment proofs, defining the judgment $\Gamma \vdash c : A \leq B$, where $c$ is an inductive term that records the shape of the contaiment proof. This proof-relevant view allows an interpretation to be given to $c$ as $[c]$ that yields a functional program mapping parse trees of $A$ to parse trees of $B$. Proving the existance of such interpretation function $[\cdot]$ corresponds to a soundness proof the containment axiomatization. On the other hand, completeness means the language containment $A \subseteq B$ implies the existence of a derivation $\vdash c : A \leq  B$, that is, we must be able to synthesize $c$ from a language containment.
\subsection{Henglein and Nielsen}
Henglein and Nielsen show that multiple distinct axiomatizations of regular expression containment, such as the one proposed by GBrabmeyer for example, all can be captured coinductively motivated rule, which in the context of proof-relevance translates nicely to a fixpoint construction. Unrestricted fixpoint definitions allows non-termination, which is unsound, and to avoid this they apply a side condition to restrict it to safe use. The rules for declaring and calling fixpoints are seen below:
\begin{displaymath}
\infer[P(\fix f.c)]{ \containsG{\fix f.c}{A}{B}}{\contains{\Gamma,f : A \leq B}{c}{A}{B}} \qquad
 \infer{\contains{\Gamma ,f : A \leq B, \Gamma'}{ f}{A}{B}}{}
\end{displaymath}
The side condition in the fix rule checks syntactically that the coercion is terminating and is reminicset of Coq's fixpoint \textsf{fix} which must be defined by structural recursion. Henglein and Nielsen introduce the following predicates for ensuring termination (defining more than, to show they can code multiple axiomatizations). We present two of their side conditions which will be relevant for our context:
\begin{definition}[Syntactic side conditions $S_i$]
 Define predicates $S_1$,$S_2$,$S_3$,$S_4$ on coercion judgments of the form $\containsG{\fix f. c}{E}{F}$ as follows:
 \begin{itemize}
 \item $S_1(\containsG{\fix f. c}{E}{F})$ if and only if each occurence of $f$ in $c$ is left-guarded by a $d$ where, $\contains{\Gamma ,...}{d}{E'}{F'}$ is the coercion judgment for $d$ occuring in the derivation of $\containsG{\fix f. c}{E}{F}$ and $\myo{(E')}=0$\\
 \item $S_2(\containsG{\fix f. c}{E}{F})$ if and only if each occurence of $f$ in $c$ is eft-guarded and for each subterm of the form $c_1;c_2$ in $c$ at least one of the following conditions is satisfied:
   \begin{itemize}
   \item $c_1$ is closed and $\dslcominv{proj}$ free
     \item $c_2$ is closed
\item $S_3(\containsG{\fix f. c}{E}{F}) \textit{if c is of the form } \dslcominv{wrap};(\dslcom{id}+\dslcom{e} \times \dslcom{f});d \textit{ where d and e are closed}$ \footnote{In Henglein and Nielsen, the $S_3$ sidecondition has a minor mistake because it instead expects the shape $\dslcominv{wrap};(\dslcom{id}+\dslcom{id} \times \dslcom{f});d \textit{ where d is closed}$. This is insuffecient to derive the example above, used in their proof of completeness. It is however minor as replacing \dslcom{id} with \dslcom{e} does not affect the underlying termination measure}
\item $S_4 = S_1 \lor S_3$
   \end{itemize}
%\item  $S_3(\containsG{\fix f. c}{E}{F})$ if $c$ is of the form $\dslcominv{wrap};(\dslcom{id} + \dslcom{d} \times f) ; \dslcom{e}$ where $d$ and $e$ are closed \footnote{This is slightly different from Henglein and Nielsen because their version is not suffecient for their completeness proof via encoding to salomaa}
%\item 
 \end{itemize}
\end{definition}
We follow their presentation, using $\myaxiomC{c}{A}{B}$ for simultanously defining the two rules  $\containsG{c}{A}{B}$ and  $\containsG{c^{-1}}{A}{B}$
\begin{displaymath}
\begin{array}{lll}
\myaxiomC{shuffle}{A + B + C}{A + (B + C)} \qquad  
\myaxiomC{retag}{A + B}{B + A} \qquad 
\myaxiomC{untagL}{0 + A}{A} 
\\\\
\containsG{untag}{A + A}{A}   \qquad \containsG{tagL}{A}{A + B} \qquad

\myaxiomC{assoc}{A \times B \times C}{A \times (B \times C)}
\\\\
\myaxiomC{swap}{A \times 1}{A} 
\myaxiomC{proj}{1 \times A}{A} \qquad
\myaxiomC{abortR}{A \times 0}{0} \qquad
\myaxiomC{abortL}{0 \times A}{0} 
\\\\
\myaxiomC{distL}{A \times (B + C)} {A \times B + A \times C} \qquad
\myaxiomC{distR}{(A + B)\times C}{A \times C + B \times C} 
\\\\
\myaxiomC{wrap}{1 + A \times A^*}{A^* } \qquad \myaxiomC{id}{A}{A}
\\\\
\infer{\containsG{c;d}{A}{C}}{\containsG{c}{A}{B} & \containsG{d}{B}{C}} \qquad

\infer{\containsG{c + d}{A + B}{ C + D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}}  \qquad

\infer{\containsG{c \times d}{A \times B}{ C \times D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}} 
\end{array}
\end{displaymath}
Returning to the fix-rule, the side condition prevents deriving ill-fixpoints like $\contains{}{\fix ~ f.f}{A}{B}$. Henglein and Nielsen show that different predicates can be used to ensure soundness and that the particular way they restrict the use of recursive calls corresponds to distinct termination measures.\\\\


The two rules in Figure (\ref{fig:salomaa}) are axioms in Salomaas axiomatization of equivalence and they are not present in Henglein and Nielsen's system.
They can however be derived using the fixpoint rule. Reusing their example we show to derive the first rule.
\begin{example}
From $\contains{}{c}{A}{B}$, we can construct a coercion $d$, s.t $\contains{}{d}{A^*}{B^*}$.\\
Assume $A^* \leq B^*$ and call assumption $f$.
\begin{align}
  A^* &\leq (1 + A \times A^*) \textsf{by} ~\dslcominv{wrap}\\
&\leq  (1 + B \times B^*) \textsf{by} ~\dslcom{id}+ \dslcom{c} \times \dslcom{f}\\
&\leq B^*
\end{align}
The explicit derivation is:
$\contains{f : A^* \leq B^*}{\dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}$\\
Applying the fix rule, the full proof is:
$\contains{}{\fix~f. \dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}}{A^*}{B^*}$\\
To ensure the fix rule has been used in a sound way, we verify that indeed satisfies side condition $S_3$.\\
\end{example}
The rules of Henglein and Nielsen can be thought of as an intrinsically typed domain specific language for defining coercions on parse trees.
The example above showed how to construct the coercion $\fix~f. \dslcominv{wrap} ; \dslcom{id} + \dslcom{c} \times \dslcom{f};  \dslcom{wrap}$ of type $A^* \leq B^*$. The interpretation of this coercion should yield us the following gallina code
\begin{minted}{Coq}
Definition map := 
 fix f (a : Star) := let v := match fold a with 
                       | inl () => inl ()
                       | inr (a',b) => inr (c a',f b)
                      in fold v
\end{minted}
This is just the map function on lists. The $S_3$ condition checks that $\dslcom{f}$ is only applied on the tail of the list. For the completeess proof by Henglein and Nielsen, $S_3$ is too strict by itself, and they present two distinct side conditions, that each yield a sound and complete characterization of containment. The other rule, $(1 + A)^* = A^*$ is in the right dirction interpreted as the filter function that removes \textsf{None}. Both rules apply a linear time operation.
 This design choice of the system to have a permissive fixpoint rule that then is constrained by a side condition is motivated by a desire to compare different axiomatizations of equivalence in order to understand the computational interpretation of the proofs. Henglein and Nielsen compare Salooma, Kozen and Grabmeyer, showing that using one side condition, $S_4$, they can code the proofs of Salooma and Grabmeyer, and with another side condition $S_2$ they can code Kozen. As all the axiomatizations are sound and complete, we only care which proof rules that admit proofs that interpret to effecient programs. The proof rules translates to the primitives of our coercion language, and the wrong choice leads to slow coercions
\subsection{Kozen}
Along with the rules of weak equivalence, and the unfold rule: $1 + (E^* \times E) \leq E$ Kozen axiomatizes containment with the rules
\begin{displaymath}
\infer{A^* \times B \leq B}{A \times B \leq B} \qquad \infer{A \times B^* \leq A}{A \times B \leq A}
\end{displaymath}
Henglein and Nielsen show the  computational interpretation of these rules by deriving them in the coercion system, assuming $d : A \times B \leq B $ and $e : A \times B \leq A $ the coercions of the two rules above are:
\begin{align}
&\fix~f.(\dslcominv{wrap}\times \dslcom{id}); \dslcom{distR};(\dslcom{proj} + (\dslcominv{assoc};(\dslcom{id} \times \dslcom{f}));\dslcom{d});\dslcom{untag}\label{align:kozen1}\\
&\fix~f.( \dslcom{id} \times \dslcominv{wrap}); \dslcom{distL};((\dslcom{swap};\dslcom{proj}) + (\dslcom{assoc};(\dslcom{e} \times \dslcom{id}));\dslcom{f});\dslcom{untag}\label{align:kozen2}
\end{align}
This corresponds to the  \textsf{foldright} (\ref{align:kozen1}) and \textsf{foldleft} (\ref{align:kozen2}) functions in functional programmming, shown in Figure \ref{figure:kozen}. Just as the previous example of the fixpoint rule, termination of this program is guaranteed by only working on the tail of the list. This is syntactically check by side condition $S_2$.
\begin{figure}\label{figure:kozen}
\caption{Computational interpretation of Kozen rules}
  \centering
  \begin{minted}{Coq}
Definition d : A \times B -> B := ...
Definition fold_right : (Star A) * B -> B := fix f (a : Star A * B) := 
                match a with 
                  | (fold (inl ()),b) => b 
                  | (fold (inr (a',a_star)),b) => d (a',f (a_star,b))
                end

Definition e : A \times B -> A := ...
Definition fold_left A * Star B -> A := fix f (a : A * Star B) := 
                match a with 
                  | (a,fold (inl ())) => a
                  | (a,fold (inr (b,b_star))) => f (e (a,b),b_star)
                end
\end{minted}
\end{figure}
With the interpretation of the two inference rules in place, we can now consider the interpretation for the proof of
\begin{align}
a ^* \times (a^*)^* \leq a^*
\end{align}
Initially we have: 
\begin{align}
\contains{}{\dslcom{retag};\dslcom{tagL};\dslcom{wrap}}{a \times a^*}{a^*}
\end{align}
Now we apply \textsf{foldright}, writing $\dslcom{foldright}(c)$ for the instantiation of $\dslcom{d}$ in (\ref{align:kozen1}) with $\dslcom{c}$
\begin{align}
\contains{}{\dslcom{foldright}(\dslcom{retag};\dslcom{tagL};\dslcom{wrap})}{a^* \times a^*}{a^*}
\end{align}
Now apply \textsf{foldleft}
\begin{align}
\contains{}{\dslcom{foldright}(\dslcom{foldleft}(\dslcom{retag};\dslcom{tagL};\dslcom{wrap}))}{a^* \times (a^*)^*}{a^*}
\end{align}
We have created a nested loop which is ineffecient, morever the three presented rules for are the only ways of manipulating kleene star so it seems that Kozens system only contains proofs with this nested shape.
\subsection{Effeciency of coercions}
We have seen that Kozen proofs leads to slow coercions and a natural question is then which proof rules gives rise to fast coercions? Henglein and Nielsen argue that Grabmeyer rules lead to fast coercions. The shape of such coercions is
\[
\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \]
Which corresponds to the gallina code
\begin{minted}{Coq}
Fixpoint coerce (a : A) : B := 
let res1 := 
match decompose A with 
| inl tt => inl tt
| inr a_sum => match sum with 
              | (e,a_sum) => (e,f_e (a_sum))
              | (e',a_sym) => (f,f_e' (a_sum))
              ...
              end 
end 
in recompose res1
\end{minted}
We now redo the denesting example in the Grabmeyer system. For now we omit the definitions of:\\
Decomposition: $\contains{}{d_A}{A}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}$\\
Recomposition: $\contains{}{r_A}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A}$\\
We assume to have $\dslcomm{d_A}$ and $\dslcomm{r_A}$ available for all regular expressions $A$.\\
We now build a coercion $c$ that satisfies $\contains{}{c}{a^* \times \dstar a}{\star a}$.\\
Assume $\contains{}{f}{a^* \times \dstar a}{a^*}$.\\
To keep terms of managable size we do this for $\Sigma = \{a\}$ when decomposing/recomposing.
\begin{align}
a^*\times \dstar a &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a) &&\quad \text{decomposition}\\
& \leq 1 + a \times (((1 \times a^*) \times \dstar a)) &&\quad \text{idempotence}\\
&\leq 1 + a \times ((1 \times (a^* \times \dstar a)) &&\quad \text{associativity}\\
& \leq 1 + a \times ((1 \times a^*) &&\quad \text{by f}\\
& \leq a^* && \quad \text{recomposition}
\end{align}
The coercion we have built is:\\
\[ \fix~f. \dslcom{d}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\dslcom{r} \]
It corresponds to the following gallina code: 
\begin{minted}{Coq}
Fixpoint f (a : (Star a) * (Star (Star a))) : Star a := 
let res := 
 match decompose a with 
 | inl tt => int tt 
 | inr (a,sum) => match (assoc (untag sum)) with 
                   | (tt,p) => (tt, f p)
                  end 
in recompose res
\end{minted}
This terminates because the recursive call is applied to a parse tree of a shorter string. This is syntactically checked by $S_1$. And important details is that this side condition is satisfied because $\myo{\Sigma_{a \in \Sigma} \derive{a}{A}} = 0$.
\subsection{Decomposition and recomposition}
Assuming constant time execution of \textsf{decompose} and \textsf{recompose}, this code executes in linear time, applying the recursive call to the parse tree corresponding to the tail of the underlying string. Assuming constant time execution of \textsf{decompose} and \textsf{recompose} is however too strong because decomposition intuitvely traverses the parse \textsf{fold (inr ((fold (inr (a,t)))))} tree to expose the event \textsf{a}. So to be more precise, given that a parse tree has leaves \textsf{tt} and \textsf{Event a}, fast decomposition (and recomposition) has a recursion pattern that is a depth-first traversal of the parse tree that halt at the occurence of the first \textsf{Event a} leaf. For example, in the parse tree \textsf{fold (inr ((fold (inr (a,t)))))}, the recursive shall should not be applied to the subterm \textsf{t} in \textsf{(a,t)}. It is important to emphasize that we only are interested in effecient decomposition/recomposition \mycomment{any aspects of recomposition that might differ from decomposition? Yes I think recomposition does not need fixpoints, which could be mentioned, also check code}, since this might be performed multiple times in a Grabmeyer-style coercion derivation. Salomaa proved that $A = \decomp{A}$ and by completeness of the HN system, there must exist decomposition and recomposition coercions, but they are very slow. They take exponential time to compute. In the rest of this section we focus mostly on decomposition as recomposition is analogous.\\\\
Haven emphasized the importance of effecient decomposition we now show an effecient decomposition of $a^*\times \dstar a$ into $\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a)$ derivable in HN: Given the input type $a^*\times \dstar a$, our input is parse tree $(t_1,t_2)$ and we should only inspect $t_2$ if the leaves of $t_1$ are all \textsf{tt}. We start from the intutive gallina program and later derive the coercion:\\
\begin{minted}{Coq}
Fixpoint decomp (aa : (Star a) * (Star (Star a))) 
 : 1 + a * ((1 * Star a) * (Star (Star a)) + 
            (1 * Star a) * (Star (Star a)))  := 
match aa with 
| (sa,ssa) => match sa with 
               | fold (inl tt) => match ssa with 
                                   | fold (inl tt) => inl tt
                                   | fold (inr pp) => decomp pp
               | fold (inr (a,sa)) => inr (a,inl ((tt,sa),ssa))
              end
end
\end{minted}
The program is terminating because we only recurse on subterms of the input. We now derive the coercion:
\begin{align}
a^* \times \dstar{a} &\leq (1 + a \times a^*) \times \dstar{a}   \\
                     &\leq 1 \times \dstar{a} + (a \times a^* ) \times \dstar{a}\\
                     &\leq 1 \times \dstar{a} +  a \times (a^*  \times \dstar{a}) \\
                     &\leq 1 \times (1 + a \times \dstar{a}) +  a \times (a^*  \times \dstar{a}) \\
                     &\leq 1 + 1 \times (a^* \times \dstar{a}) +  a \times (a^*  \times \dstar{a})\\
                     &\leq 1 + 1\times ( 1 + a \times (((1 \times a^*) \times \dstar a)))) +\\
                     &\quad(1 \times a^*) \times \dstar a))  + a \times (a^*  \times \dstar{a}) \\
                     &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a)
\end{align}
The coercion we have built is:
\begin{align}
\fix f.~&(\dslcominv{wrap} \times \dslcom{id});\dslcom{distR};\\
 \Big( &(\dslcom{id} \times \dslcominv{wrap});\dslcom{distL};\\
       &\dslcom{untag} + ((\dslcom{id} \times \dslcom{f});\dslcom{proj});\\
       &\dslcominv{assoc};(\dslcom{untag} + \dslcom{untag}) \\
&+\\
 &\dslcom{id} \times \dslcom{projinv} \times \dslcom{id} \Big);\\
 &\dslcom{id} + (\dslcom{assoc} ; \dslcom{untag})
\end{align}
% \begin{align}
% a^* \times \dstar{a} &\leq (1 + a \times a^*) \times \dstar{a}   \\
%                      &\leq 1 \times \dstar{a} + (a \times a^* ) \times \dstar{a}\\
%                      &\leq \dstar{a} +  a \times (a^*  \times \dstar{a}) \\
%                      &\leq 1 + a^* \times \dstar{a} +  a \times (a^*  \times \dstar{a})\\
%                      &\leq 1 + ( 1 + a \times (((1 \times a^*) \times \dstar a) +\\
%                      &\quad(1 \times a^*) \times \dstar a))  + a \times (a^*  \times \dstar{a}) \\
%                      &\leq 1 + a \times (((1 \times a^*) \times \dstar a) + (1 \times a^*) \times \dstar a)
% \end{align}
% \begin{align}
% \fix f.~&(\dslcominv{wrap} \times \dslcom{id});\dslcom{distR};\\
% &\dslcom{proj};\dslcominv{wrap};(\dslcom{id} + \dslcom{f});\dslcominv{associnv};(\dslcom{untag} + \dslcom{id});\dslcom{assoc};(\dslcom{id}+\dslcom{untag})\\
% &+\\
%                                                                                               &(\dslcom{proj} \times \dslcom{id})
% \end{align}
This is derivable Henglein and Nielsen if we take the Kozen side condition. This is because no use of $\dslcominv{proj}$ happens before the recursive call.\\\\
We have now built the decomposition $\dslcom{d}$ in the Grabmeyer coercion:
\[ \fix~f. \dslcom{d}; (\dslcom{id} + \dslcom{untag} ; \dslcom{assoc} ; (\dslcom{id} \times \dslcom{f})) ;\dslcom{r} \]
This consists of the outer fixpoint $\fix f$ satisfying $S_1$ and a nested fixpoint in $\dslcom{d}$ satisfying $S_2$. We have thus given an example of a fast coercion that requires side condition $S_5 = S_1 \lor S_2$ in order to be derivable. We believe that language containments can always be expressed as Grabmeyer coercions using $S_5$ but proving this is difficult due to the non-compositional nature of $S_2$. This side condition dictates that a recursive call may not happen after $\dslcominv{proj}$ which prevents a natural and parametric definition of decomposition by structural recursion on the regular expression. We will explain this in detail now.
%\[
%\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \]

% It seems that any decomposition can be proved using the Kozen measure. Kozen measure relies on no projin between fixpoint and recursive call and this has some drawbacks. The decomposition of events requires projiinv and therefore the decomposition of $\dstar a$ cannot just blindly decompose the subterm because it would use projinv in an illegal position. Instead a projinv free version must be used and after finishing the recursive calls, one can fit the bill by applying the appropriate projinvs. This strictness has two drawbacks. It prevents decomposition from being recursively defined on the syntax of regular expressions, making it harder to define the parametric decomposition. Secondly it prevents tail-recursion for which ocaml can optimize code. We now introduce how we have done this...
\mycomment{Write a tail-recursive and non-tailrecursive example in benchmark}
\mycomment{Would deriving recomposition have projinv appear before recursive call, which is not allowed in Kozen measure? Recomposition does not seem to introduce anything new}
\mycomment{They now need disjunct of Kozen and Grabmeyer measure}
\mycomment{Kozen alone is not suffecient because decomposition uses projinv}
\mycomment{If we instead werer to prove nesting (not denesting), would we need projinv before recursive call}
\mycomment{Did we define recomposition effeciently in the code?}
\mycomment{Recomposition is not interesting because tagL can be used to make it work without recursion}
\mycomment{Our approach leads to tailrecursive decomposition I think, which could be faster in a tail recursion optimizing language like ocaml}



\mycomment{This subsection should change structure, we want to land at the introduction of peek rule. We get there from limitation of HN rules due to side conditions. Exemplified by example on paper. Relate to Salomaas proof of decomposition? A possible structure: Grabmeyer uses decomposition, HN show this leads to fast proofs, Salomaas decomposition, HN's decomposition, our decomposition with peek}
\subsection{Decomposition as polymorphic coercion}
The decomposition operation can be viweed as a polymorphic coercion $\contains{}{c}{\forall X. X}{\myo{o} + \decomp{X}}$. Since we do not have a $\forall$- introduction rule this is not derivable in the Henglein and Nielsen system, but it serves as an intuition for how we might define in one go, a effecient generic decomposition, for all regular expressions. In Coq this would look like:
\begin{minted}{Coq}
Fixpoint (A : regex) : A -> (o A) +  
                            \big[Plus/Empt]_(i <- l) ((Event i)_;_(i \ Event a) :=
fun a => 
match A with 
| ....
\end{minted}
In fact, Salomaa already implicitly defined such a function in 1966 when he proved $A = \myo{A} + \decomp{A}$ for his system $F_1$ by structural induction on $A$ \mycomment{is that the right name of system?}. Computationally this corresponds to definition by structural recursion, with the applicatoin of the induction hypothesis as the recursive call. The decomposition is ineffecient because of the way the  induction hypothesis is used. The ineffeciency of the Salomaa coding of decomposition occurs in the $A^*$ case of the proof, caused by what is known as \textit{problematic} regular expressions, ie. $A^ *$ where $\mynu{A} = \textsf{true}$. In this case of the proof, one has inductiion hypothesis $A = 1 + \Sigma_{a \in \Sigma} \derive{a}{A}$, and must show $A^* = 1  + \Sigma_{a \in \Sigma} ((\derive{a}{A})\times A)^*$. A sketch of how this is done is:
\begin{proof} (Sketch)
\begin{align}
A^* &= ( \Sigma_{a \in \Sigma} \derive{a}{A})^* &&\text{by star context with IH -> then drop}\\
 &= 1 + (\Sigma_{a \in \Sigma} \derive{a}{A}) \times (\Sigma_{a \in \Sigma} \derive{a}{A})^* &&\text{by unfold}\\
 &= 1 + (\Sigma_{a \in \Sigma} \derive{a}{A}) \times A^* && \text{by drop then star context with IH <- } \\
 &= 1 + (\Sigma_{a \in \Sigma} (\derive{a}{A}) \times A^*) && \text{by distributivity }
\end{align}
\end{proof}
One turns the problematic regular expression $A^*$ into an unproblematic one $B^*$ by using drop and after a few more steps getting the shap $1 + B \times A^*$ where $\mynu{B} = \false$. Thinking of $A^*$ as a list, operationally this proof decomposes each element, extracts the head, then re-composes the tail. Moreover, we used the induction hypothesis (recursive call) in both directions to achieve this. This yields an exponential time algorithm \footnote{In fact, proving this as an equalityo pperatially corresponds to definition by mutual recursion, defining \textsf{decomp} and \textsf{comp} at the same time \mycomment{Is this true?}}
%HERE
%  Henglein and Nielsen sketch an approach to build effecient coercions by following the shape of a Grabmeyer derivation. Assuming that $A \subseteq B$,
%  prove that they can code proofs consisting solely of COMP-fix with transitivity, context-rules and ACI. To do this they make use of the the animorov decomposition in building their coercions. The decomposition and its adaptation to containment can be seen in Figure \ref{fig:decomp}
%  \begin{figure}
%    \centering
%    \begin{align}
% &A = \mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}\\
% &\contains{}{d}{A}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}} \label{eq:derive1}\\
% &\contains{}{e}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A} \label{eq:derive2}\\
% &\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \label{eq:coerce}
%    \end{align}
%    \caption{Regular expression decomposition}
%    \label{fig:decomp}
%  \end{figure}
% Salooma show this can derived using the rules of weak equivalence along with $(1 + A)^* = A^*$. Henglein and Nielsen prove that they can code Salomaas derivations; which implies they can derive coercions in (\ref{eq:derive1}, \ref{eq:derive2}). 
% Their claim however that Grabmeyer style coercions, as seen in (\ref{eq:coerce}), are always effecient linear time programs, is unfortunately wrong. In particular, for the synthesis approach given by Henglein and Nielsen it is slower than Kozen. The reason for this is that the effeciency of Grabmeyes style coercions relies on implementing constant time decomposition $\contains{}{d}{A}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}$ and re-composition $\contains{}{e}{\mynu{A} + \Sigma_{a \in \Sigma} \derive{a}{A}}{A}$.\\
What we would prefer is a similar but effecient way of deriving $A \leq \myo{A} + \decomp{A}$. The ineffeciency was due to applying the IH with context rule for star (mapping over the list). Let us try applying the IH only to exposed head after unfolding.\\\\
Assume access to:
\begin{itemize}
\item The inuction hypothesis $ \dslcom{IH} : A \leq 1 + \decomp A$
\item The conclusion itself $\dslcom{f} : A^* \leq 1 + (\decomp{A}) \times A^*$,
\end{itemize}
\begin{proof} (Sketch)
\begin{align}
A^* &\leq  1 + A \times A^* && \text{by unfold}\\
 &\leq 1 + (1 + \decomp{a}) \times A^* && \text{by } \dslcom{IH}\\
 &\leq 1 + 1 \times A^* + (\Sigma_{a \in \Sigma} \derive{a}{A} \times A^*) && \text{by distibutivity}\\
 &\leq 1 +  1 \times (\decompp{A}{A^*}) +  (\Sigma_{a \in \Sigma} \derive{a}{A} \times A^*)  && \text{by } \dslcom{f}  \\
 &\leq 1 + \decomp{A^*} && \text{idempotence} 
\end{align}
\end{proof}
The dsl program this proof builds is \mycomment{did not check details}
\[ \fix ~f.\dslcom{wrap};(\dslcom{id} + (\dslcom{IH};\dslcom {id});\dslcom{distR};(\dslcom{proj};\dslcom{f})+\dslcom{id});\dslcom{shuffle};(\dslcom{untag} + \dslcom{untag}) \]
Termination is ensured by recursing only on subterms of the input parse tree. Though it is terminating, it does not satisfy and of the decidable side conditions in Henglein and Nielsen. It only satisfies their most general, but undecidable, side condition, hereditarly total \footnote{All side conditions except for the one used to code Kozen derivations are immediate to verify. The Kozen side condition fails because it disallows use of \dslcom{projinv} before the recursive call and \dslcom{e} uses this command in the event case of the decomposition}.
In past examples, for this scenario with recursion on $A$ in $1 \times A$ we would satisfy the $S_2$ condition, but this is not the case here since the coercoin $\dslcom{IH}$, appearing before the recursive call, might use $\dslcom{projinv}$. Indeed if the regular expression contains an event $a$ and we for illustration take $\Sigma = \{a\}$, derivation of the decomposition $a \leq 0 + a \times 1 $ uses $\dslcominv{proj}$. Using the fixpoint rule would result in a terminating coercion because recursion would be on a subterm of the parse tree but syntactically this is not recognized by any of the side conditions that have been presented.\\\\
To solve this issue, rather than defining yet another side condition, we recognize that replacing $A$ with $(1 + \decomp A)$ caused the problematic reappearence of $A^*$ after distribution (\ref{align:problematic}). If we could replace $A$ with $(\decomp A)$, there would be no need for the fixpoint. The derivation would go like this:
\begin{proof} (Sketch)
\begin{align}
A^* &\leq  1 + A \times A^* && \text{by unfold}\\
 &\leq 1 + (\decomp{A}) \times A^* && \text{by } \dslcom{peek}~\text{with }\dslcom{IH}\\
 &\leq 1 + \decomp{A^*} && \text{distributivity} 
\end{align}
\end{proof}
Where we define $\dslcom{peek}$ as.
\[ \infer{\containsG{peek ~ c}{A^*}{1 + B \times A^*}}{\containsG{c}{A}{1 + B}} \]
It seems likely that a variant of the Kozen measure could be defined that would allow $\dslcom{peek}$ to be derived from the Henglein and Nielsen's fix rule. For now we will however promote it to its own rule. 
\subsection{A coercion system without a general fixpoint}
So far we have described an algorithm to synthesize effecient coercions that decompose \mycomment{how about recompose} regular expressions. This is done using the $\dslcom{peek}$ rule and decomposition thus makes no use of the general fixpoint rule. We use this decomposition in building Grabmeyer coercions that however do make use of the fixpoitn rule, seen below:
% \begin{minted}{Coq}
% Fixpoint decomp (A : regex) : A -> (o A) +  
%                             \big[Plus/Empt]_(i <- l) ((Event i)_;_(i \ Event a)

% fun a =>
% match A with 
% | Star B =>  match drop ((star_ctx decomp) a)
%              | fold (inl ()) => inl () 
%              | fold (inr (a',a_sigma)) =>  distr a' (star_ctx comp (dropinv a_sigma))
% ...
% end 
% with
%          comp (A : regex) : (o A) +  
%                             \big[Plus/Empt]_(i <- l) ((Event i)_;_(i \ Event a) -> A
% \end{minted}
% \footnote{We omit definition of \textsf{distr}. As we saw earlier, \textsf{starctx} and \textsf{drop} are linear time taversals of the parse tree.}
% This program has exponential runtime due to the recursive call \textsf{decomp} in \textsf{starctx}. The ineffeciency is due to the decomposition of the wole list when we only care about the decomposition of the head. 

% \begin{lemma}
% \noindent \\
% (1) Decomposition and recomposition is derivable with the rules of weak equivalence with $\dslcom{peek}$ and $\dslcom{peekinv}$\\
% (2) The runtime is best-case constant and worst-case linear
%\end{lemma}
\[
\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B} \qquad \text{assuming }f_a : \derive{a}{A} \rightarrow \derive{a}{B}, \forall ~ a \in \Sigma \]
In the work of Henglein and Nielsen it is beneficial to have a general fixpoint rule that can be instantiated with different side conditions, as it makes their proof space so large that they can derive all the rules in each of the three axiomatizations they study (Salomaa,Kozen,Grabmeyer). Their proof space thus contains fast linear-time coercion and slow exponential time-coercions. In this work however, our interests differ, we are interested in:
\begin{itemize}
\item Effecient and simple synthesis of coercions
\item The interpretation of these coercions must yield linear time programs \mycomment{sparse parsetrees might have slower runtime?}
\end{itemize}
Regarding the second point, Grabmeyer coercions interpret to fast programs, so we will be fine with restricting recursion to only allow us such derivations. Let us investigate the recursion pattern in a Grabmeyer coercion:\\
For regular expression $A$, after executing decomposition $\dslcom{d}$ we have 
 \[ \myo{A} + \decomp{A} \]
 The right summand is piped into $\Sigma_{a \in \Sigma} \dslcom{id}\times f_a$, where $\dslcom{id}$ will be applied to the parse tree $\event{a}$ for each event in the alpabet. Any recursive call to $\dslcom{f}$ in any of the $f_a$, will thus happen in the right component $t$ of the pair $(\event{a},t)$. 
% The general fix rule is more general than we need it to be and for mechanisation purposes it is more convenient to restrict our syntax so that it does not contain exotic terms (those that do not satisfy the side condition in Henglein and Nielsens system).
This recursion pattern tells us that we may add our conclusion to our set of assumptions:
\[\infer{\contains{\Gamma}{\fix ~f.d}{A}{B}}{ \contains{\Gamma, \dslcom{f}: A \leq B}{d}{A}{B}}
\]
But we can only discharge an assumption after consuming an event
\[\infer{\contains{\Gamma, f : A \leq B, \Gamma'}{f}{a \times A}{a \times B}}{}
\]
Let us call this for event guarded recursion.
We can now replace the general fix rule with event guarded recursion and $\dslcom{peek}$. The effect of restricting the coercion language is that firstly; Only fast proofs are derivable. Secondly, the system has no exotic terms, $\fix f. f$ is not derivable. Put differently, all rules are compositional, and context is not necessary in order to build a subterm in the coercion sytem. The proof of soundeness, that is,  $\contains{}{c}{E}{F}$ is only derivable if $E \subseteq F$. It is not relevant for completeness, essentially comes down to proving termination and string preservation of interpretation. Restricting the syntax of coercions simplifies the termination argument for the interpreter. Completeness on the other hand is essentially coercion synthesis and the elimination of the side conditions eliminates the need for repeatedly executing side conditions to check the well-formedness during synthesis. In particular the Kozen side condition takes time linear in the size of the coercion syntax.\\\\
The full coercion axiomatization we present in this work is the coercions for weak equivalence with $\dslcom{peek}$ along with the new fix and variable rules. The full rule set is presented in Figure (\ref{fig:system})
\begin{figure}
\caption{The new coercion system without the general fix rule}
\label{fig:system}
\centering
\begin{displaymath}
\begin{array}{lll}
\myaxiomC{shuffle}{A + B + C}{A + (B + C)} \qquad  
\myaxiomC{retag}{A + B}{B + A} \qquad 
\myaxiomC{untagL}{0 + A}{A} 
\\\\
\containsG{untag}{A + A}{A}   \qquad \containsG{tagL}{A}{A + B} \qquad

\myaxiomC{assoc}{A \times B \times C}{A \times (B \times C)}
\\\\\
\myaxiomC{swap}{A \times 1}{A} 
\myaxiomC{proj}{1 \times A}{A} \qquad
\myaxiomC{abortR}{A \times 0}{0} \qquad
\myaxiomC{abortL}{0 \times A}{0} 
\\\\
\myaxiomC{distL}{A \times (B + C)} {A \times B + A \times C} \qquad
\myaxiomC{distR}{(A + B)\times C}{A \times C + B \times C} 
\\\\
\myaxiomC{wrap}{1 + A \times A^*}{A^* } \qquad \myaxiomC{id}{A}{A}
\\\\
\infer{\containsG{c;d}{A}{C}}{\containsG{c}{A}{B} & \containsG{d}{B}{C}} \qquad

\infer{\containsG{c + d}{A + B}{ C + D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}}  \qquad

\infer{\containsG{c \times d}{A \times B}{ C \times D}}{\containsG{c}{A}{C} & \containsG{d}{B}{D}} 
\\\\
\infer{\containsG{\dslcom{peek}~d}{A^*}{1 + B \times A^*}}{\containsG{d}{A}{1 + B}} \qquad
\\\\
\infer{\contains{\Gamma}{\fix ~f.d}{A}{B}}{ \contains{\Gamma, \dslcom{f}: A \leq B}{d}{A}{B}} \qquad
\infer{\contains{\Gamma, f : A \leq B, \Gamma'}{id \times f}{a \times A}{a \times B}}{}
\end{array}
\end{displaymath}
\end{figure}


\section{Completeness and synthesis}
We recall that the definition of simulation was:
\[\infer={A \sim B}{\mynu{A} \implies \mynu{B} & \forall a \in \Sigma, \derive{a}{A} \sim \derive{a}{B} }\]
Simulation coincides with language containment so to show completeness it suffices to prove:
\[ A \sim B \implies \exists c, \contains{}{c}{A}{B} \]
That is, we must synthesize coercion $\dslcom{c}$. Using the decomposition from the previous section, intuitively $\dslcom{c}$ will take the shape:
\[\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B}\]
And the continuations $\dslcom{f}_a$ will be derived under the extended assumption: $\contains{(A,B)}{f_a}{\derive{a}{A}}{\derive{a}{B}}$. In general, our context $\Gamma$ in $\containsG{c}{E}{F}$ is finite and we can thus not simulate the infinitary coinduction going on in the simulation relation. We must constrain simulation to an equivalent relation that only does finitary coinduction.\\
Grabmeyer investigated this problem for regular expression equivalence in \cite{G05}.
Modulus ACI equivalence, the set of reachable regular expression derivatives is finite and Grabmeyer uses this to give an inductive characterisation of regular expression bisimulation up to ACI equivalence. His system consists of ACI equivalence, rules of equational logic, context rules and COMP/FIX:
\[\infer[\mathsf{COMP/FIX}]{\Gamma \vdash A =  B}{\mynu{A} =  \mynu{B} & \Gamma, A = B \vdash \derive{a}{A} = \derive{a}{B},~\forall a \in \Sigma}\] 
This gives rise to a finite characterisation of containment as a terminating decision procedure,
\begin{align}
\simulationb{V}{A}{B} =\begin{cases}
			\true, & \text{if }(A,B) \in V\\
                      \bigwedge_{a \in \Sigma}\simulationb{V \cup \{(A,B)\}}{\derive{a}{A}}{\derive{a}{B}}, & \text{otherwise}
		 \end{cases}
\end{align}
So long that we let $(A,B) \in V$ check membership modulus ACI. To show completeness of the coercion system one could do it as:
\begin{enumerate}
\item Prove $A \sim B$ implies $\simulationb{\{\}}{A}{B}$, by finitiness of distinct derivatives modulus ACI
\item Prove $\simulationb{\{\}}{A}{B}$ implies $\exists c.~\contains{}{c}{A}{B}$ by functional induction
\end{enumerate}
Doing (1.) in a proof assistant like Coq is challenging because the termination argument must be translated into a property that can be characterized by an inductive judgment, which we can structurally recurse on. The Bragga method [cite] explains in detail how one may separate the termination argument from the definition of a recursive function, by instead of reucrsing on the input $x$, recursing on an inductive proof about $x$, called $\mathbb{D}~x$. We take this approach and it is easier to define $\mathbb{D}~x$ if we move from standard brozowski derivatives to partial derivatives. Normally partial derivatives are defined by mapping a regular expression to a set of regular expression. In our mechanisation we use inductive lists instead of sets, and to keep the paper formalization as close as possible to the mechanisation, we will present partial derivatives exactly as they have been defined in the code.\\
\begin{definition}[Partial derivatives]
  \begin{displaymath}
    \begin{array}{lll}
\pd{a}{0} = [0] \qquad \pd{a}{1} = [0] \qquad \pd{a}{a} = [1] \qquad \pd{a}{a'} = [0]\text{if}~a \neq a'\\\\
\pd{a}{A + B} = \cat{\pd{a}{A}}{\pd{a}{B}} \qquad \pd{a}{A \times B} = \cat{(\mathsf{map }(\lambda x.~x \times B)~\pd{a}{B})}{\pd{a}{B}} ~\text{ if }~\mynu{A} = 1~\\\\
\pd{a}{A \times B} = (\mathsf{map }(\lambda x.~x \times B)~\pd{a}{B}) ~\text{ if }~\mynu{A} = 0~ 
\qquad \pd{a}{A^*} = \mathsf{map }(\lambda x.~x \times A^*)~\pd{a}{A}  \\\\
    \end{array}
  \end{displaymath}
Overloading notation, the partial derivative of a list of regular expressions $\tilde{A}$ is:
\[\pd{a}{\tilde{A}} = \mathsf{undup}(\mathsf{flatten}(\mymap{\partial_a}{\tilde{A}}))\]
The nullariness of a list of regular expressions is:
\[\myo{\tilde{A}} = 1 ~\text{if }~\exists A \in \tilde{A}~st. ~ \mynu{A}=1 \text{ otherwise } 0\]
\end{definition}
We can now define Simulation with partial derivatives
\[\infer={\tilde{A} \sim \tilde{B}}{\mynu{\tilde{A}} \implies \mynu{\tilde{B}} & \forall a \in \Sigma, \mypar{a}{\tilde{A}} \sim \mypar{a}{\tilde{B}} }\]

\begin{lemma}
$A \sim B$  implies $[A] \sim [B]$
\end{lemma}
\mycomment{actually the code only uses the partial one, goes directly from language containment I think}
We compute the $\pi$ enumeration from \cite{CZ01} adapted to lists
\begin{definition}[Enumeration]
  \begin{displaymath}
    \begin{array}{lll}
  \pi(0) = [] \qquad \pi(1) = [] \qquad \pi(a) = [1] \qquad \pi(A + B) = \cat{\pi(A)}{\pi(B)} \\\\
  \pi(A \times B) = \cat{\mymap{\lambda x.~x \times B}{\pi(A)}}{\pi(B)} \qquad \pi(A^*) = \mymap{\lambda x.~x \times A^*}{\pi(A)}
    \end{array}
  \end{displaymath}
We extend $\pi$ to lists of regular expressions:
\[\pi([])=[] \qquad \pi(A::\tilde{A}) = \mathsf{cartesian}(A,\pi(\tilde{A}))\]
We extend this to a pair of lists of regular expressions:
\[\pi(\tilde{A},\tilde{B})= \mathsf{cartesian}(\pi(\tilde{A}),\pi(\tilde{B}))\]
\end{definition}
The reason we use partial derivatives is so that we can get the following property:
\begin{lemma}[Closure]\label{lem:closure}
 $(\tilde{A},\tilde{B}) \in \pi(\partial_a(\tilde{C},\tilde{D}))$ implies  $(\tilde{A},\tilde{B}) \in \pi((\tilde{C},\tilde{D}))$ 
\end{lemma}
This tells us that for the pair of regular expression lists $\pder{A}{B}$, their enumeration which we compute with$\pi$, contains the enumerations of all partial derivatives which you can find starting from $\pder{A}{B}$. This allows us to define the domain of $\simulationb{}{}$
\begin{definition}[Domain]
  \begin{displaymath}
    \begin{array}{lll}
      \infer[\mathbb{D}_{stop}]{\domm{V}{\pder{A}{B}}}{\pder{A}{B} \in V} \qquad \infer[\mathbb{D}_{step}]{\domm{V}{\pder{A}{B}}}{\pder{A}{B} \notin V & \mathsf{uniq}(\pder{A}{B}) & \forall a.~\domm{(\pder{A}{B}::V)}{\partial_a(\pder{A}{B})}}
    \end{array}
  \end{displaymath}
\end{definition}
And define the decision procedure that follows the course-of-values of $\mathbb{D}$:
\begin{align}
\simulationb{V}{\tilde{A}}{\tilde{B}} =\begin{cases}
			\true, & \text{if }(\pder{A}{B}) \in V\\
                      \bigwedge_{a \in \Sigma}\simulationb{V \cup \{\pder{A}{B}\}}{\mypar{a}{\tilde{A}}}{\mypar{a}{\tilde{B}}}, & \text{otherwise}
		 \end{cases}
\end{align}
The structural recursion of $\simulationb{V}{\pder{A}{B}}$ is defiend on $\domm{V}{\pder{A}{B}}$, which means the function is only defined on $\tilde{A}$ and $\tilde{B}$ when we can derive $\domm{V}{\pder{A}{B}}$. We prove the following:
\begin{lemma}
If uniq $\tilde{A}$ and $\tilde{B}$ , one can derive $\domm{V}{\pder{A}{B}}$ for any $V$.
\end{lemma}
\begin{proof}
Relies on Lemma (\ref{lem:closure}), from which we show the following decreasing measure:\\
Define $M(V,\pder{A}{B} = |\pi(\pder{A}{B}) \backslash \{V\}|  $ \mycomment{using set notation for lists}\\
If $\tilde{A}$ and $\tilde{B}$ are unique and $\pder{A}{B} \notin V$ then $M(\pder{A}{B}::V,\mypar{a}{\pder{A}{B}}) <  M(V,\pder{A}{B}) $
\end{proof}
\mycomment{We use a similar measure in ITP 2023 projection}
We can now state a main lemma:
\begin{lemma}
$A \subseteq B$ iff $\simulationb{nil}{[A]}{[B]}$
\end{lemma}
We proved this property to get a functional induction principle that will help us to prove completeness of the coercion system. As an intermediate result we however have a mechanised decidability of regular expression containment. In the mechanisation we define $\mathsf{bisimulationb}$ analogously to show that the method is the same for deciding regular expression equivalence. Many mechanisations of this result exists in the litterature, but we think this might be the shortest proof of regular expression equivalence in the litterature. It is shorter than [cite Matita], whose main contribution was the compactness of their proof. We believe our concicseness is achieved by defining partial derivatives using lists and computing a closure by extending the standard definition of $\pi(A)$ on a single regular expression to a list of regular expresions $\pi(\tilde{A})$.\\\\
Now with a functional indunction principle in hand we are ready to build the coercions. Since we are working with lists of regular expressions $\tilde{A}$, to avoid clutter we will write $\tilde{A}$ in a position where a regular expression is expected to mean $\mysum{A}{\tilde{A}}$ The main theorem of the completeness proof is:
\begin{theorem}
Assuming for all $\pder{E}{F} \in V$ that $\pder{E}{F} \in \Gamma$, then\\
$\simulationb{V}{\tilde{A}}{\tilde{B}}$ implies that there exists $\dslcom{c}$ s.t. $\contains{\Gamma}{c}{\tilde{E}}{\tilde{F}}$
\end{theorem}
% $\simulationb{V}{\pder{A}{B}}$ is defined on lists of regular expressions, so we define coercions that let us jump from 
\begin{proof}[Sketch]
By induction on $\domm{V}{\pder{E}{F}}$. Derive the following variants of decomposition and recomposition:
\begin{align}
&\forall \tilde{A},a.~\exists \dslcom{c}.~\contains{}{c}{(\tilde{A}}{\myo{\tilde{A}} + \Sigma_{a \in \Sigma} a \times \mypar{a}{\tilde{A}}} \ref{align:pdecomp}\\
&\forall \tilde{A},a.~\exists \dslcom{c}.~\contains{}{c}
{\myo{\tilde{A}} + \Sigma_{a \in \Sigma} a \times \mypar{a}{\tilde{A}}}
{\tilde{A}} \ref{align:precomp}
\end{align}
The proof follows the shape of the Grambeyer coercion:
\[\contains{}{\fix ~f.d;(id + \Sigma_{a \in \Sigma} id\times f_a); e}{A}{B}\]
We first apply $\fix$, then by decomposition and recomposition (\label{align:pdecomp},\label{align:precomp}) it suffices to show for each $a \in \Sigma$ that there exists $\dslcom{c}$ s.t. $\contains{\Gamma,\pder{A}{B}}{c}{\mypar{a}{\tilde{A}}}{\mypar{a}{\tilde{B}}}$, which is our induction hypothesis.
\end{proof}
\begin{corollary}[Completeness and synthesis]
(1): If $A \leq B$ then there exists $\dslcom{c}$ s.t. $\contains{}{c}{A}{B}$\\
(2): There exists a procedure such that for any $A$ and $B$, it either synthesises a coercion $\contains{}{c}{A}{B}$ or outputs a proof of $\lnot A \leq B $ \mycomment{Point to this in code}
\end{corollary}


% computing an enumeration of reachable partial derivatives \mycomment{How is this different from equivP in technique and loc}.  


%Completeness and synthesis are related. Completeness does in this setting mean that all language containments \textsf{Contains A B} can be derived in the coercion system of Figure (\ref{fig:system}). In the presence of a decision procedure for language containment \textsf{dec : \{Contains A B\} + \{~ Contains A B\}}, we can use the completeness proof to synthesize a coercion, going from \textsf{Contains A B} to $\contains{}{c}{A}{B}$.
% Recall that we aim at synthesising Grabmeyer style coercions which inductively emulate finitary coinduction proofs of similarity. We recall the definition of simulation

%We follow the completeness proof of Henglein and Nielsen and build $c$ as a Grabmeyer coercion.\\




\section{Soundness and interpretation}
Soundness is:
\[\contains{}{c}{E}{F} \implies E \subseteq F\]
An interpretation function is:
\[\interp{ \contains{}{c}{E}{F}} : \type{E} \rightarrow \type{F}\]
\begin{lemma}\label{lem:types}
$t : \type{A}$ implies $\flatten{t} \in A$\\
$s \in A$ implies $\exists t.~t : \type{A}$ and $\flatten{t} = s$
\end{lemma}
By Lemma (\ref{lem:types}), to prove soundness, it suffices to show that there exists an interpretation function that maps a coercion derivation $\contains{}{c}{E}{F}$ to the susbet of the function space $\type{E} \rightarrow \type{F}$ that only contains string preserving maps.
\begin{definition}[String preservation]
A function $f :\type{E} \rightarrow \type{F}$ is preserving $\pres{f}$ iff $\forall t. \flatten{f~t} = \flatten{t}$.\\
\end{definition}
\begin{lemma}
The existence of an interpretation function $\interp{ \contains{}{c}{E}{F}} : \{f : \type{E} \rightarrow \type{F}~ |~ \pres{f} \}$ implies soundness of the coercion system
\end{lemma}
We now focus on defining an interpreter and get interpretation for free. In Figure (\label{fig:interp}) the equtional simplification rules of the interpreter are given. The last for equalities capture recursion, the first being Grabmeyer recursion with a termination argument known as the $0$-measure $|v|_0$ on parse trees. The last three equalities define the semantics of $\dslcom{peek}$ as a recursive function and it has a termination measure known as $1$-measure $|v|_1$ 
\begin{definition}[Measures on parse trees]
  \begin{displaymath}
    \begin{array}{lll}
      \zerom{0} = 0 \qquad \zerom{1} = 0 \qquad \zerom{a} = 1 \qquad \zerom{inl~ t} = \zerom{t} \qquad \zerom{inr~ t} = \zerom{t} \qquad
     \zerom{(v,w)}=\zerom{v}+\zerom{w} \\\\ \zerom{fold~v}=\zerom{v}\\\\
    \onem{0} = 0 \qquad \onem{1} = 1 \qquad \onem{a} = 1 \qquad \onem{inl~ t} = \onem{t} \qquad \onem{inr~ t} = \onem{t} \qquad
     \onem{(v,w)}=\onem{v}+\onem{w} \\\\ \onem{fold~v}=\onem{v}
    \end{array}
  \end{displaymath}
\end{definition}
We can verify that the course-of-values for $\dslcom{peek c}$ respects the $\zerom{\cdot}$ measure because in the recursive case \[\irule{(peek c)}{fold (inr (v,x))}{(peek c)(x)}\quad \text{if }\irule{c}{v}{inl ()}\]
We have that 
\[ \zerom{x} < \zerom{fold~ (inr~ v,x)}\]
We can also verify that Grabmeyer recursion satisfies the $\onem{\cdot}$ measure because none of the rules increases the $\onem{\cdot}$ measure, and because of the discharge rule:
\[\infer{\contains{\Gamma, f : A \leq B, \Gamma'}{id \times f}{a \times A}{a \times B}}{}\]
This means that if the application $\dslcom{fix f.c}(t)$ recurses, then reduction will lead to a term $\dslcom{f}(a,t')$ where $\onem{t'} < \onem{(a,t')}\leq \onem{t}$
\begin{figure} \ref{fig:interp}
\mycomment{Check rules are correct}
\mycomment{mention fold/unfold isorecursion 1.3 notation and terminology}
  \centering
  \begin{displaymath}
    \begin{array}{l}
\irule{shuffle}{inl v}{inl (inl v)}\\
\irule{shuffle}{inr v}{inl (inr v)}\\
\irule{shuffle}{(inr (inr v))}{inr v}\\
\iruleinv{shuffle}{(inl (inl v))}{inl v}\\
\iruleinv{shuffle}{(inl (inr v))}{inr (inl v)}\\
\iruleinv{shuffle}{(inr v)}{inr (inr v)}\\
\irule{retag}{(inl v)}{inr v}\\
\irule{retag}{(inr v)}{inl v}\\
\iruleinv{retag}{}{retag}\\
\irule{untagL}{(inr v)}{v}\\
\irule{untag}{(inl v)}{v}\\
\irule{untag}{(inr v)}{v}\\
\irule{tagL}{(v)}{inl v}\\ 
\irule{assoc}{(v,(w, x))}{((v, w), x)}\\
\iruleinv{assoc}{((v, w), x)}{(v,(w, x))}\\
\irule{swap}{(v,())}{((), v)}\\
\iruleinv{swap}{((), v)}{(v,())}\\
\irule{proj}{((), w)}{w}\\
\iruleinv{proj}{(w)}{((), w)}\\
\irule{distL}{(v, inl w)}{inl (v, w)}\\
\irule{distL}{(v, inr x)}{inr (v, x)}\\
\iruleinv{distL}{(inl (v, w))}{(v, inl w)}\\
\iruleinv{distL}{(inr (v, x))}{(v, inr x)}\\
\irule{distR}{(inl v, w)}{inl (v, w)}\\
\irule{distR}{(inr v, x)}{inr (v, x)}\\
\iruleinv{distR}{(inl (v, w))}{(inl v, w)}\\
\iruleinv{distR}{(inr (v, x))}{(inr v, x)}\\
\irule{wrap}{(v)}{fold v}\\
\dslcominv{wrap}(\textsf{(v)})=\dslcominv{fold}\\
\irule{id}{v}{v}\\
\iruleinv{id}{}{id}\\
\irule{(c; d)}{(v)}{d(c(v))}\\
\irule{(c + d)}{(inl v)}{inl (c(v))}\\
\irule{(c + d)}{(inr w)}{inr (d(w))}\\
\irule{(c × d)}{(v, w)}{(c(v), d(w))}\\
\irule{(fix f.c)}{(v)}{c[fixf.c/f](v)}\\
\irule{(peek c)}{fold (inl ())}{inl ()}\\
\irule{(peek c)}{fold (inr (v,x))}{(peek c)(x)}\quad \text{if }\irule{c}{v}{inl ()}\\
\irule{(peek c)}{fold (inr (v,x))}{inr (w,x)}\quad \text{if }\irule{c}{v}{inr w}
    \end{array}
  \end{displaymath}
  \caption{Interpretation semantics}
\end{figure}
Before we present the interpreter, we show how one represents the coercion system in Coq.
In Coq the coercion system we represent a derivation of $\containsG{c}{A}{B}$ as the term  \textsf{c : dsl Gamma A B}.
\mycomment{change drop to peek in code}
\begin{minted}{Coq}
Inductive dsl (Gamma: seq (regex * regex)) : regex -> regex -> Type := 
| shuffle A B C : dsl Gamma ((A _+_ B) _+_ C) (A _+_ (B _+_ C))
| dfix A B : dsl ((A,B):: Gamma) A B -> dsl Gamma A B.
| var a A B :   (A,B) \in Gamma -> dsl Gamma (Event a _;_ A) (Event a _;_  B) 
...

Lemma pf_in : ((1 + Star a),(Star a)) \in [(1 + Star a) (Star a)].
Proof. ... Qed.

Definition example : dsl nil (1 + Star a) (Star a) := 
dfix (ctrans (peek cid)
     (ctrans (cplus cid (dsl_var pf_in)) wrap))
\end{minted}
The type of the interpretation function is seen below
\begin{minted}{Coq}
Definition post {A : eqType} (r0 r1 : @regex A) (T : pTree r0) := 
  { T' : pTree r1 | pTree_0size T' <= pTree_0size  T /\ pflatten T = pflatten T' }. 


Fixpoint interp l r0 r1 (p : dsl l r0 r1) (T : pTree r0) 
         (Gamma_f : forall x y,  (x,y) \in l -> 
                    forall (T0 : pTree x), pRel0 T0 T -> post y T0) {struct p}:
         post r1 T. 
\end{minted}
\mycomment{change semantic rules to include environment}
\textsf{Gammaf} is used to interpret the variables, and its type ensures that it can only be used on parse trees of smaller $0$-size..
\section{Benchmarking}
Implementations we compare\\
\begin{figure}
  \centering
  \begin{tabular}{l | l | l | l | l | l}
Containment & Synthesis time & $|p|$=10 &  $|p|$=100 &  $|p|$=1000 \\
\hline
$a^* \times \dstar a \leq a^*$ & ? & & &\\
\hline
$\dstar a \leq a^*$ & ? & & &\\
\hline
$(1 + a)^* \leq a^*$ & ? & & &\\
\hline
$(a+b)^* \leq a^* + (b \times a^*)^*$ & ? & & &\\
\hline
$a^* \times b^* \leq ((1 + a) \times (1+ B))^*$ & ? & & &
\end{tabular}
  \caption{Synthesis and execution time for coercion with $\Sigma=\{0...4\}$ and resp. $\Sigma=\{0..99\}$}
  \label{fig:bench}
\end{figure}

\begin{itemize}
\item Proof search approch
\item Inductive dsl with environment as list. Does expensive decomposition
\item Inductive dsl with fast decomposition
\item Coinductive dsl with environment as function (that is build in a list-like way) (expensive decompositoin)
\item Missing Coinductive dsl fast
\item With or without implicit variables
\end{itemize}

We are interested in the time it takes to synthesize and execute coercions. How do we test this?\\
\subsection{examples}
Examples from the paper:\\
\begin{align}
a ^* \times (a^*)^* \leq a^*
\end{align}
Also try a simpler example that highligts the peek rule. 
\subsection{Approach}
Syntheses: Build $f$.\\
Execution: Run $f a$\\
We separate the two because you can synthesize once and use many times.\\
How does this relate to code extraction?
\begin{itemize}
\item Extract synthesis and interpretation procedures and call them both in OCaml. 
\end{itemize}

\section{discussion and related work}
Computational interpretations of linear logic \cite{A93}\\
Regular expression containment as a proof search problem Vladimir Komendantsky[Misc url].\\
Also mention Marco and Carstens result-

Greedy parse trees and preservatino of greediness by coercions. Proof relevance computational properties.\\
white board and mechanisation leads to interesting results.
\\
Though the set of derivatives quotiented by ACI is finite for any regular expression, in a proof assistant it can be easier to formulate such finiteness arguments constructively as the existence of an inductive list that contains all the derivatives. This can be done by considering the partial derivatives $\partial_a$ of regular expressions. Convenient finitness arguments have been well studied by others in the litterature to show termination for regular expression equivalence decision procedures. Doing this through partial derivatives was introduced by Almeida et al. \cite{AMR09} and in \cite{MPS12}, they mechanize in Coq the \textsf{equivP} procedure of Almeida et al. \mycomment{How does their termination argument work?}. There is also \cite{A12} who take a different approach, turning regular expression into point-automata, making it straigtforward to compute the enumeration of reachable derivatives. We combine their approaches, which we show in a moment
\mycomment{Two files for this, extensional and extensionalpartial which one is correct I think extensional partial, clean up code later}\\
\bibliography{ref}  

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
