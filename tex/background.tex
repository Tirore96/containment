% Intro text.
% \subsection{Regular expressions as types}
%
Regular expressions are typically studied in automata and formal language theory as a formalism for denoting the class of regular languages, where the only observable property is whether a string belongs to the denoted set of strings or not, and any two regular expressions are observably equivalent if they satisfy the same membership tests.  In programming practice, however, regular expressions are typically used to extract not only whether but also how a string matches a regular expression, ranging from extracting a particular substring to constructing a full parse tree. Many common automata theory techniques are not applicable in this setting; for example, a deterministic finite automaton constructed from a regular expression provides no general way of constructing a parse tree for the given regular expression. 

Here we provide background concepts and results on interpreting 
regular expressions as types \cite{frca2004,HN11}. Note that we are
informal here.  Mechanizations are provided later.  
\begin{definition}[Regular expression]
\emph{Regular expressions} $A, B$ over a finite alphabet $\Sigma$ are given by the abstract syntax
$$A, B ::= 0 \mid 1 \mid a \mid A + B \mid A \times B \mid A ^*$$
where $a \in \Sigma$. 
\end{definition}


\begin{figure}
\caption{Matching relation and type inhabitation, as presented in \cite{HN11}}
\label{fig:matchtype}
      \begin{displaymath} 
\begin{array}{ll}
\infer{\match{\epsilon}{1}}{} & \infer{\oft{()}{1}}{} \\\\
\infer{\match{a}{a}}{} &  \infer{\oft{\inj{a}}{a}}{} \\\\
\infer{\match{s s'}{A \times B}}{\match{s}{A} \match{s'}{B}}  &
                                                                \infer{\oft{\pair{t}{t'}}{A
                                                                \times
                                                                B}}{\oft{t}{A}
                       & \oft{t'}{B}}  \\\\   
\infer{\match{s} {A + B}}{\match{s}{A}} &  \infer{\oft{\inl {t}}{A + B}}{\oft{t}{A}} \\\\
\infer{\match{s} {A + B}}{\match{s}{B}} &  \infer{\oft{\inr {t}}{A +
                                          B}}{\oft{t}{B}} \\\\
\infer{\match{s s'}{A^*}}{\match{s}{A} & \match{s'}{A^*}} &
                                                            \infer{\oft{\fold{t}}{A^*}}{\oft{t}{1
                                                            + A \times
                                                            A^*}}\\\\   
\infer{\match{\epsilon}{A^*}}{}         & \\\\
\mathbf{a)}~ \text{Regular expression matching} &  ~~~~~~~
\mathbf{b)}~ \text{Type inhabitation}
\end{array}
% \begin{array}{l}
% \infer{\oft{()}{1}}{} \qquad 
% \infer{\oft{\inj{a}}{a}}{} \qquad
% \infer{\oft{\pair{t}{t'}}{A \times B}}{\oft{t}{A} & \oft{t'}{B}} \qquad 
% \infer{\oft{\inl {t}}{A + B}}{\oft{t}{A}} \qquad
% \infer{\oft{\inr {t}}{A + B}}{\oft{t}{B}}  \qquad
% \infer{\oft{\fold{t}}{A^*}}{\oft{t}{1 + A \times A^*}}
% \end{array}
    \end{displaymath}
\end{figure} 

We represent this in Coq by an inductive datatype.
\begin{samepage}
\begin{minted}{Coq}
Section Regex.
Variable (A : eqType).
Inductive regex  : Type :=
| One : regex
| Zero : regex
| Event : A -> regex
| Plus : regex -> regex -> regex
| Times : regex -> regex -> regex
| Star : regex -> regex.    
\end{minted}
\end{samepage}
We use the \coq{Section} mechanism to implicitly parameterizes the type
\coq{regex} and constructors by an \coq{eqType}, which is a type
equipped with a decidable equality. Later in the benchmarks, this type
will be instantiated as a finite subset of the natural numbers. 
\dawit{mention why regex is typed Type}
\dawit{from henglein and nielsen, mention that we follow their presentation}
\begin{definition}[Constant part]
We denote by $\myo{A}$, the constant part of $A$, defined such that
$\myo{A}=1$ if $\matchv{\epsilon}{A}$ holds and $\myo{A}=0$
otherwise.
\end{definition}
The constant part of a regular expression $\myo{A}$ captures whether
$\epsilon$ is part of the language denoted by $A$. A computable
definition of $\myo{\cdot}$ is straightforward to implement by
structural recursion on the regular expression.
\begin{definition}[Matching]
The \emph{matching} relation between strings $s \in \Sigma^*$ and regular expressions $A, B$ is defined inductively by
the rule in Figure \ref{fig:matchtype}a where string juxtaposition denotes concatenation and $\epsilon$ the
empty string.

We write $\vdash s \in A$ if there exists a derivation of $s \in A$ and say that \emph{$s$ matches $A$}.
We define $\lang{A} = \{ s \mid \vdash s \in A \}$ and call it the \emph{language} denoted by $A$.
\end{definition}

We can, as usual, make derivations of matchings explicit as proof terms, which yields a type system with judgements of the form 
$t : s \in A$.  Since $s$ is a function of $t$, it can be left out, which yields judgements of the form $t : A$.  
By inspection of the resulting inference system it can be seen that each proof term corresponds to a unique element of the type corresponding to the regular expression.
\begin{definition}[Inhabitation]
Regular expressions as types $A, B$ over a finite alphabet $\Sigma$ are the types constructible from singleton types of $\Sigma$-elements,
product, sum, unit, zero and (finite) list type constructors.
The binary relation between terms $t$ and regular expressions as types $A$ is defined inductively by
the rule in Figure \ref{fig:matchtype}.

We write $\vdash t: A$ and say $t$ is an \emph{element} of $A$ if $t: A$ is derivable.
We define $\type{A} = \{ t \mid \,\, \vdash t : A \}$ and call it the \emph{type} denoted by $A$.
\end{definition}
Intuitively, a term $t : A$ represents a parse tree for a particular
string $s$ in the language denoted by $A$. 
We represent parse trees in Coq as the intrinsically typed \coq{pTree
R}
We represent this in Coq by the \coq{Match} judgment
\begin{samepage}
\begin{minted}{Coq}
Inductive Match : trace -> regex -> Prop :=
| M_One            : Match [::] One
| M_Event          : Match [::x] (Event x)
| M_Times s s' A B : Match s A ->  
                     Match s' B -> Match (s ++ s') (Times A B)
  ...

Inductive pTree : regex S -> Type := 
| p_tt             : pTree One 
| p_in a           : pTree (Event a)
| p_pair A B       : pTree A -> pTree B -> pTree (Times A B)
....
\end{minted}
\end{samepage}
 The underlying string of
the parse tree can be obtained by flattening.
\begin{definition}[Flattening terms to strings]
The function $\flatten{\cdot}$ is defined by
\[ \begin{array}{rclrcl}
\flatten{()} & = & \epsilon &
\flatten{\inj{a}} & = & a \\
\flatten{\pair{t}{u}} & = & \flatten{t} \, \flatten{u} &
\flatten{\inl{t}} & = & \flatten{t} \\
\flatten{\inr{u}} & = & \flatten{u} &
\flatten{\fold{t}} & = & \flatten{t}
\end{array} \]
\end{definition}
Informally, $\flatten{\cdot}$ unparses a term $t$ into a string by
listing its characters in the order they occur in $t$. This function
is straightforward to define in Coq and we omit its definition. 


%Recall regular expressions interpreted as types whose elements represent parse trees of strings that can be flattened (unparsed) to their underlying strings again.  

\begin{definition}[Coercion]
A function $f :\type{A} \rightarrow \type{B}$ is a \emph{coercion} if $\flatten{f(t)} = \flatten{t}$ for all $t \in \type{A}$.
We write $f : A \leq B$ if $f$ is a coercion.
\end{definition}
In other words, a function is a coercion if it transforms any parse
tree $t$ of a string under regular expression $A$ into a parse tree
under regular expression $B$ for the same string.  Note that coercions
must be total, that is defined on each input term, but nothing is said
in the definition about how efficient a coercion must be -- or that it
even be computable. This makes the function space $:\type{A}
\rightarrow \type{B}$ large and we care only about a subset of these
functions, namely the effecient ones. 



\begin{example}[Effecient hand-coded coercion]
\label{C11-example}
Consider the following regular expression equivalence \cite[Rule C11, p.~25]{conway71}:
$$(A + B)^* \equiv (A^* \times B)^* \times A^*.$$
We can prove it by constructing functions $c : (A + B)^* \leq (A* \times B)^* \times A^*$ and $d : (A* \times B)^* \times A^* \leq (A + B)^*$
and checking that these are coercions.
\begin{verbatim}
c []        = ([], [])
c (x :: xs) = 
	let (l, r) = c xs
	in case (x, l) of
	     (inl a, []) -> ([], a :: r)
	   | (inl a, (as, b) :: l') -> (((a :: as, b) :: l'), r)
	   | (inr b, l) -> (([], b) :: l, r)
\end{verbatim}
Similarly for $d$.  In this case, the pair $c, d$ forms an isomorphism, that is $c \circ d = \id$ and $d \circ c = \id$.  In general, equivalent regular expressions are \emph{not} isomorphic, however. For example, we have $A \equiv A + A$; that is $A$ and $A + A$ denote the same language, but they are not isomorphic as types. Conversely, this equivalence is the only axiomatic regular expression equivalence without an isomorphic interpretation. 
\end{example}

The example above shows how an effecient coercion might be coded by
hand. For the applications of string compression, which we will
motivate later, we need to be able to synthesize such effecient
coercions automatically. In essence, the problem we investigate in this paper is; Given regular
expressions $A$ and $B$, how does one synthesize an effecient coercion $c :\type{A}
\rightarrow \type{B}$. 

The related problem of deriving, by hand,
coercions that are correct by construction has been studied by
Henglein and Nielsen in terms of a domain specific language for coercions \cite{HN11} and we will comment on their work in the next section.


% The complimentary operation to flattening is parsing, and we
% will see how combining flattening and parsing allows us to prove an equivalence
% between match derivations and parse trees. In general, parsing of an
% input according to some grammer, seen for example in the parsing stage
% of a compiler, is usually a partial operation, returning the abstract
% syntax tree if the input satisfies the grammer, and producing an error
% otherwise. In our setting we usually know that the input $s$ belongs to a language denoted by a
% regular expression $\match{s}{A}$. In such a setting, a total parser
% can be defined. Of course, if the definition of the parser inspects
% the derivation of $\match{s}{A}$, then its implementation becomes
% trivial, simply map each matching rule to the corresponding typing
% rule for parse trees. This would make the behaviour of the parser,
% depend on the shape of the proof, e.g. for an ambigous regular
% expression with multiple parse trees of the same string, the parser
% would disambiguate the grammer by following the structure of the proof
% of $\match{s}{A}$. This would make the matching relation
% \textit{proof-relevant} which defeats the purpose of introducing parse
% trees.
\subsection{Synthesizing ineffecient coercions}
We end this section by proving a correspondance between matching and
type inhabitation, and use this correspondance to present a
proof-of-concept for how coercions might, naievly, be synthesised in a
proof-irrelevant way, generating highly ineffecient  coercions.
 
The technique that will be presented relies on  
Brozowski derivatives [cite], which we define in Figure \ref{fig:derive}.
\begin{figure}
\caption{Brozowksi derivative}
\label{fig:derive}
  \begin{displaymath}
    \begin{array}{ll}
% \mynueq 0 {\false} \\
% \mynueq 1 {\true} \\
% \mynueq{A + B}{\mynu A \lor \mynu B} \\
% \mynueq{A + B}{\mynu A \land \mynu B} \\
% \mynueq{\mystar A}{\true}
% \\\\
      \bdereq 0 a i 0 & \\
      \bdereq 1 a i 0 & \\
      \bdereq {(a_i)} a i 1 & \\
      \bdereq {(a_j)} a i 0 & (i \neq j) \\
      \bdereq {A + B} a i {\bder A a i + \bder B a i} & \\ 
      \bdereq {A \times B} a i {\bder A a i \times B + \bder B a i}
      ~~~  & \mynu{A} = \true\\
      \bdereq {A \times B} a i {\bder A a i \times B} ~~~ & \mynu{A} =
                                                            \false\\
      \bdereq {\mystar A} a i {\bder A a i \times \mystar A}
    \end{array}
    \end{displaymath}
\end{figure}



% the result of which is a term $t$ of the same $A$, but read as a type,
%flattening corresponds to unparsing the term to the parsed string $s$
%again.  

% \begin{lemma}[Matching and derivatives]
% \label{lem:matchder}
% %\item $\myoeq A 1$ iff $\matchv{\epsilon}{A}$
% $\matchv{s}{\bder A a {}}$ iff
% $\matchv{a\,s}{A}$
% \end{lemma}
\begin{lemma}[Parsing]
\label{lem:parse}
\noindent
\begin{enumerate}
\item If $\myoeq A 1$ then there exists a term $t$ such that
$\oftv{t}{A}$ and $\flatten{A}=\epsilon$.
\item If $\oft{t}{\bder A a {}}$ is derivable then there exists a parse
tree $t'$ for which $\oft{t'}{A}$ is derivable and $\flatten{t'}=a
\,\flatten{t}$
\item For any string $s$ and regular expression $A$ then either 
a term $t$ exists such that $\oftv{t}{A}$ and $\flatten{s}$, otherwise
$\matchv{s}{A}$ is not derivable. 
\end{enumerate}
\end{lemma}
\begin{proof}
1) and 2) are both shown by induction on $A$, constructively building
the term $t$.

3) is shown by induction on $s$ where the base case $s = \epsilon$ is
handled by 1) and inductive step handled by 2). 
\end{proof}
Note that Lemma \ref{lem:parse} essentially defines a parser, with
\ref{lem:parse}.1 producing an initial parse tree for the empty string and
\ref{lem:parse}.2 transforms an existing parse tree of a string $s$ to
a parse tree of the longer string $a \, s$ and these parts are glued
together by \ref{lem:parse}.3 which recursively builds a parse tree of
a longer and longer string, until either the full string is parsed or
it becomes clear that the string does not belong to the language
denoted by the regular expression.

We use this lemma to prove a correspondance between matching and parse trees.
\begin{lemma} 
\label{lem:matchparse}
$\vdash s \in A$ if and only if there exists a term $t$ such that $\vdash \oft{t}{A}$ and $\flatten{t} = s$.
\end{lemma}
\begin{proof}
The if direction follows from \ref{lem:parse}.3, where one concludes
that the term $t$ must exist by contradiction of $\matchv{s}{A}$ being
both derivable and not derivable.

The only-if direction follows from a simple induction on
$\oftv{t}{A}$, using the shape of the typing derivation to construct $\matchv{s}{A}$.
\end{proof}


% Regular expressions interpreted as types provide a convenient
% theoretical and practical conceptual framework, for studying the
% intensional structure of regular expressions beyond their common
% extensional interpretation as sets of strings \dawit{nice, refers back
% to introduction}. 
% \dawit{Cut out ambiguity example of intentional property of matching}



%  For example,
% \begin{definition}[Ambiguity] 
% \dawit{we do not mechanise ambiguity, should be cut}
% A regular expression $A$ is \emph{ambiguous} if there exist $t, t': A$ such that $t \neq t'$ and $\flatten{t} = \flatten{t'}$.  
% \end{definition}
% Note that the same regular language can be denoted by both ambiguous and unambiguous regular expressions.

%We usually use $c$ and variants as metavariables for coercions.

From the this lemma it follwos that coercions are witnesses of language containment:
\begin{lemma}
\label{lem:coerceequiv}
$\lang{A} \subseteq \lang{B}$ if and only if there exists a coercion $c : A \leq B$.
\end{lemma}
\begin{proof}
If direction: From the assumption $\lang{A} \subseteq \lang{B}$, one
must build a coercion from $\type{A}$ to $\type{B}$, that is, an
arbitrary term $t$ such that $\oftv{t}{A}$, must be mapped to a term
$t'$ such that $\oftv{t'}{B}$ and $\flatten{t}=\flatten{t'}$. 
By assumption we know $\matchv{\flatten{t}}{A}$ which by the language
containment implies $\matchv{\flatten{t}}{B}$. By Lemma
\ref{lem:matchparse} (if direction) there must then exist the desired
term $t'$.

Only-if direction: From the assumption $c : A \leq B$, one must show
for any $s$ such that $\matchv{s}{A}$, then it also holds that
$\matchv{s}{B}$. Again by Lemma \ref{lem:matchparse} (if direction),
there exists a term $t$ such that $\oftv{t}{A}$ and
$\flatten{t}=s$, which by the coercion assumption implies
$\oftv{c(t)}{B}$ and $\flatten{c(t)}=s$. Now by Lemma
\ref{lem:matchparse} (only-if direction) we have $\matchv{s}{B}$.

% the assumption $\matchv{\flatten{t}}{A}$ and $\matchv{s}{B}$ we must
% construct a coercion from $\type{A}$ to $\type{B}$
% For the if direction, we construct the coercion $\parse{B}
% \circ \flattenn : A \leq B$, where $\parse{B}$ is the partial
% application of the parse function. The coercion is a total function 
% because we for any $t : A$ have
% $\match{\flatten{t}}{B}$.

% For the only-if direction, we show the containment $\lang{A} \subseteq
% \lang{B}$ by defining $\flattenn \circ c \circ \parse{A} : \lang{A}
% \rightarrow \lang{B}$.
%Assume $\lang{A} \subseteq \lang{B}$, that is $A$ is contained in $B$.  A coercion $c$ can be constructed canonically as follows.
% Given a term $t \in \type{A}$, flatten it to a string, then parse the string under regular expression $B$.  One might be tempted to construct an automaton from $B$ and then run the automaton on the string, which is guaranteed to succeed by the assumption.  But that does not yield a parse tree.  Generating a full parse tree, and doing so efficiently, is possible, but a much harder task \cite{frca2004,ghrst2016}.  
% Conversely, assume a coercion $c : A \leq B$ exists. Let $s \in \lang{A}$.  
% Then there exists $t \in \type{A}$ such that $\flatten{t} = s$.  
% We now have that $c(t) \in \type{B}$ and thus $\flatten{c(t)} \in \lang{B}$.  
% Since coercions preserve strings we can conclude that 
% $\flatten{c(t)} = \flatten{t} = s \in \lang{B}$, which proves the containment.
\end{proof}

% \subsection{Coding an effecient coercion by hand}


% As previously observed \cite{ghrst2016}, the only-if direction
% suggests a useful technique for proving language containments.


% On the other hand, the if direction of Lemma \ref{lem:coerceequiv},
% and more precisely the constructive proof of this direction, gives us
% a general procedure for deriving coercions. 

% \begin{verbatim}
% coerce A B (a : A) : B = parseflatten(a) 
% \end{verbatim}



% Note that the coercion $c$ in the example above transforms a parse
% tree for $(A + B)^*$ directly to a parse tree for $(A^* \times B)^*
% \times A^*$, without the detour of unparsing and then parsing
% again. Intuitively, it reuses the parsing effort that has gone into
% parsing under $A$ to directly contstruct a parse tree for $B$.

% This is unlike the proof of ($\implies$) in Lemma
% \ref{lem:coerceequiv}, which relies on flattening the input
% parsetree and parsing the resulting string to a parsetree of the
% output type. 

%  all language containments imply
% the existence of a coercion. 


% example \ref{C11-example} shows an ad hoc
% way to implement the containment 
% $$\lang{(A + B)^*} \subseteq \lang{(A^* \times B)^* \times A^*}.$$

% But how can we synthesize such a function automatically, given only
% input and output type, and how can we be sure that it is a coercion?





% \section{Containment axiomatizations and their operational interpretation}

% Recall that coercions are string preserving functions between the type interpretations of regular expressions, which in turn represent the parse trees of strings under the regular expression.  

% \dawit{why mention categories?}
% It turns out that regular expressions, interpreted as types, form a category with products and coproducts where the coercions as the morphisms.  
% This naturally leads to the idea of building a categorical domain-specific language (DSL) of coercions and seeding it with enough primitive coercions  such that there exists at least one coercion $c : A \leq B$ for every language containment $\lang{A} \subseteq \lang{B}$.

% One way of getting there is equipping axiomatizations of regular expression containment via a Curry-Howard-style functional interpretation, where the proof terms constitute the sought-after DSL for coercions.
% \subsubsection{Defining Effeciency}
% \dawit{This text does not work, how should effeciency be explained? It
% is not mechanised, but benchmarked which might allow us to be informal.}
% We express the runtime of a coercion $\contains{}{c}{A}{B}$ applied to a parse tree $\mathsf{t} : \type{A}$ as a function of the string length of $\mathsf{t}$, denoted by $\flatten{\mathsf{t}}$. We postpone the definition of interpretation, $\interp{\contains{}{c}{A}{B}} : \type{A} \rightarrow \type{B}$ to Section \ref{sec:soundinterp} and for now suffice with the intuitive interpretation of coercions as functional programs.
% \dawit{Check font of pare tree examples}
% Regular expressions are ambigous as a grammer and two parse trees of the same regular expression, $\mathsf{t} : a^*$, $\mathsf{t'} : a^*$, of the same string, $\flatten{t}=\flatten{t'}$ can differ arbitarily much in the size of the parse trees by repeatedly putting  $() : 1$ in the leaves of the tree. We say a compact parse tree $\mathsf{t} : A$, is the smallest parse tree of string $\flatten{t}$, implying that the parse tree size will differ from the string length by a constant factor. An efficient coercion, executes in time linear with respect to the size of a compact parse tree.
% To allow such a runtime, decomposition should be a constant time operation on compact parse trees. If the tree is sparse, the runtime should be linear the length of the path that is a depth-first-search of the parse tree for the first non $(): 1$ leaf \dawit{Define leaf}.
\dawit{Show parse tree of input type first: In example of fast hand
coded coercion}
\dawit{Commented out effeciency}
\dawit{Comment in benchmark that using this lemma for synthesis
results in stack overflow}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End:
